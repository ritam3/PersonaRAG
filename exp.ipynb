{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from rag_core.index_builder import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_web_docs(urls: List[str]):\n",
    "    \"\"\"\n",
    "    Load documents from a list of URLs.\n",
    "\n",
    "    - HTML URLs are loaded with WebBaseLoader.\n",
    "    - PDF URLs (including Google Drive file links) are loaded with OnlinePDFLoader.\n",
    "    \"\"\"\n",
    "    html_urls: List[str] = []\n",
    "    pdf_urls: List[str] = []\n",
    "\n",
    "    for url in urls:\n",
    "        u = url.strip()\n",
    "        if not u:\n",
    "            continue\n",
    "        \n",
    "        html_urls.append(u)\n",
    "\n",
    "    docs = []\n",
    "\n",
    "    # --- HTML pages via WebBaseLoader ---\n",
    "    if html_urls:\n",
    "        html_loader = WebBaseLoader(\n",
    "            web_paths=html_urls,\n",
    "            bs_kwargs=dict(\n",
    "                parse_only=bs4.SoupStrainer(\n",
    "                    [\"article\", \"main\", \"div\", \"section\", \"body\"]\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "        html_docs = html_loader.load()\n",
    "        for d in html_docs:\n",
    "            d.metadata[\"source\"] = d.metadata.get(\"source\", d.metadata.get(\"url\"))\n",
    "        docs.extend(html_docs)\n",
    "\n",
    "    # --- PDF files (including Drive) via OnlinePDFLoader ---\n",
    "    for pdf_url in pdf_urls:\n",
    "        try:\n",
    "            pdf_loader = OnlinePDFLoader(pdf_url)\n",
    "            pdf_docs = pdf_loader.load()\n",
    "            for d in pdf_docs:\n",
    "                d.metadata[\"source\"] = d.metadata.get(\"source\", pdf_url)\n",
    "            docs.extend(pdf_docs)\n",
    "        except Exception as e:\n",
    "            print(f\"[index_builder] Failed to load PDF from {pdf_url}: {e}\")\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[index_builder] Crawled 12 URLs under https://fearless-writers-028990.framer.app/project/\n",
      "[index_builder] Total URLs to load: 17\n",
      "[index_builder] Loaded 17 raw documents\n",
      "[index_builder] Split into 51 chunks\n",
      "[index_builder] Saved FAISS index to /Users/ritamupadhyay/Documents/Ritam_QA/data/vectorstore/career_faiss_index (chunks=51)\n"
     ]
    }
   ],
   "source": [
    "n, chunks = build_and_save_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://fearless-writers-028990.framer.app/old-home', 'start_index': 4}, page_content=\"Ritam UpadhyayData ScientistHomeAboutProjectsStacksSearchMenuAvailable for workLet's Transform Ideas into RealityRitam UpadhyayData ScientistProblem SolverData ScientistProblem SolverData ScientistProblem SolverData ScientistProblem SolverRitam UpadhyayData ScientistProblem SolverData ScientistProblem SolverData ScientistProblem SolverData ScientistProblem SolverRitam UpadhyayData ScientistProblem SolverData ScientistProblem SolverData ScientistProblem SolverData ScientistProblem SolverWelcome to my portfolio! I am Ritam Upadhyay, passionate in the field of Data Science and Artificial Intelligence. I have 2 years of work experience in the field of Data Science at Paytm and I am currently pursuing my Master's in Data Science from Arizona State University.More about meMore about meMore about meContactContactContactMy Latest WorksI present my top-tier projects, meticulously crafted with unwavering passion, simplicity, boundless creativity, and unparalleled attention to detail.RAG-PDF QA\"),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/old-home', 'start_index': 805}, page_content='meContactContactContactMy Latest WorksI present my top-tier projects, meticulously crafted with unwavering passion, simplicity, boundless creativity, and unparalleled attention to detail.RAG-PDF QA MethodImplementation of RAG method to perform question answering on PDF documents. HuggingFace embeddings in addition to the Groq API were used for efficient and accurate information retrieval. Session management was done to ensure embeddings are computed once per document upload to optimize performance by avoiding redundant computations. January 2025RAG-PDF QA MethodImplementation of RAG method to perform question answering on PDF documents. HuggingFace embeddings in addition to the Groq API were used for efficient and accurate information retrieval. Session management was done to ensure embeddings are computed once per document upload to optimize performance by avoiding redundant computations. January 2025RAG-PDF QA MethodImplementation of RAG method to perform question answering on PDF'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/old-home', 'start_index': 1610}, page_content='are computed once per document upload to optimize performance by avoiding redundant computations. January 2025RAG-PDF QA MethodImplementation of RAG method to perform question answering on PDF documents. HuggingFace embeddings in addition to the Groq API were used for efficient and accurate information retrieval. Session management was done to ensure embeddings are computed once per document upload to optimize performance by avoiding redundant computations. January 2025LLM Based Q/A on Web Context Development of Web App that uses open source LLMs to answer user queries based on its own knowledge as well as any reference provided in the form of external webpages.December 2024LLM Based Q/A on Web Context Development of Web App that uses open source LLMs to answer user queries based on its own knowledge as well as any reference provided in the form of external webpages.December 2024LLM Based Q/A on Web Context Development of Web App that uses open source LLMs to answer user queries based'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/old-home', 'start_index': 2412}, page_content='knowledge as well as any reference provided in the form of external webpages.December 2024LLM Based Q/A on Web Context Development of Web App that uses open source LLMs to answer user queries based on its own knowledge as well as any reference provided in the form of external webpages.December 2024Next Word\"Forecasting the Future: Utilizing LSTM for Next-Word Prediction in Natural Language Processing.\" December 2024Next Word\"Forecasting the Future: Utilizing LSTM for Next-Word Prediction in Natural Language Processing.\" December 2024Next Word\"Forecasting the Future: Utilizing LSTM for Next-Word Prediction in Natural Language Processing.\" December 2024All projectAll projectAll projectStackTech Stack I am familiar withPythonPythonPythonSQLSQLSQLStreamlitStreamlitStreamlitTensorflowTensorflowTensorflowMoreMoreMoreIndexMain HomeAboutResourcesProjectStackContactrupadh17@asu.eduLinkedinIndexMain HomeAboutResourcesProjectStackContactrupadh17@asu.eduLinkedinIndexMain'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/old-home', 'start_index': 3244}, page_content='HomeAboutResourcesProjectStackContactrupadh17@asu.eduLinkedinIndexMain HomeAboutResourcesProjectStackContactrupadh17@asu.eduLinkedinIndexMain HomeAboutResourcesProjectStackContactrupadh17@asu.eduLinkedin'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/old-home', 'start_index': 3449}, page_content='Create a free website with Framer, the website builder loved by startups, designers and agencies.'),\n",
       " Document(metadata={'source': 'https://github.com/ritam3', 'title': 'LinkedIn', 'start_index': 3}, page_content='Skip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNavigation Menu\\n\\nToggle navigation\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Sign in\\n          \\n\\n\\n \\n\\n\\nAppearance settings\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Platform\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          GitHub Copilot\\n\\n        \\n\\n        Write better code with AI\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n          GitHub Spark\\n\\n            \\n              New\\n            \\n\\n\\n        Build and deploy intelligent apps\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n          GitHub Models\\n\\n            \\n              New\\n            \\n\\n\\n        Manage and compare prompts\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n          GitHub Advanced Security\\n\\n        \\n\\n        Find and fix vulnerabilities\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n          Actions\\n\\n        \\n\\n        Automate any workflow\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Codespaces\\n\\n        \\n\\n        Instant dev environments\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n          Issues\\n\\n        \\n\\n        Plan and track work\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n          Code Review\\n\\n        \\n\\n        Manage code changes\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n          Discussions'),\n",
       " Document(metadata={'source': 'https://github.com/ritam3', 'title': 'LinkedIn', 'start_index': 796}, page_content='Issues\\n\\n        \\n\\n        Plan and track work\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n          Code Review\\n\\n        \\n\\n        Manage code changes\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n          Discussions\\n\\n        \\n\\n        Collaborate outside of code\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n          Code Search\\n\\n        \\n\\n        Find more, search less\\n      \\n\\n\\n\\n\\n\\n\\nExplore\\n\\n\\n\\n      Why GitHub\\n\\n    \\n\\n\\n\\n      Documentation\\n\\n    \\n\\n\\n\\n\\n\\n      GitHub Skills\\n\\n    \\n\\n\\n\\n\\n\\n      Blog\\n\\n    \\n\\n\\n\\n\\n\\n\\nIntegrations\\n\\n\\n\\n      GitHub Marketplace\\n\\n    \\n\\n\\n\\n      MCP Registry\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n              View all features\\n              \\n\\n\\n \\n\\n\\n\\n\\n        Solutions\\n        \\n\\n\\n\\n\\n\\n\\n\\nBy company size\\n\\n\\n\\n      Enterprises\\n\\n    \\n\\n\\n\\n      Small and medium teams\\n\\n    \\n\\n\\n\\n      Startups\\n\\n    \\n\\n\\n\\n      Nonprofits\\n\\n    \\n\\n\\n\\n\\n\\n\\nBy use case\\n\\n\\n\\n      App Modernization\\n\\n    \\n\\n\\n\\n      DevSecOps\\n\\n    \\n\\n\\n\\n      DevOps\\n\\n    \\n\\n\\n\\n      CI/CD\\n\\n    \\n\\n\\n\\n      View all use cases\\n\\n    \\n\\n\\n\\n\\n\\n\\nBy industry\\n\\n\\n\\n      Healthcare\\n\\n    \\n\\n\\n\\n      Financial services\\n\\n    \\n\\n\\n\\n      Manufacturing'),\n",
       " Document(metadata={'source': 'https://github.com/ritam3', 'title': 'LinkedIn', 'start_index': 1603}, page_content='DevOps\\n\\n    \\n\\n\\n\\n      CI/CD\\n\\n    \\n\\n\\n\\n      View all use cases\\n\\n    \\n\\n\\n\\n\\n\\n\\nBy industry\\n\\n\\n\\n      Healthcare\\n\\n    \\n\\n\\n\\n      Financial services\\n\\n    \\n\\n\\n\\n      Manufacturing\\n\\n    \\n\\n\\n\\n      Government\\n\\n    \\n\\n\\n\\n      View all industries\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n              View all solutions\\n              \\n\\n\\n \\n\\n\\n\\n\\n        Resources\\n        \\n\\n\\n\\n\\n\\n\\n\\nTopics\\n\\n\\n\\n      AI\\n\\n    \\n\\n\\n\\n      DevOps\\n\\n    \\n\\n\\n\\n      Security\\n\\n    \\n\\n\\n\\n      Software Development\\n\\n    \\n\\n\\n\\n      View all\\n\\n    \\n\\n\\n\\n\\n\\n\\nExplore\\n\\n\\n\\n      Learning Pathways\\n\\n    \\n\\n\\n\\n\\n\\n      Events & Webinars\\n\\n    \\n\\n\\n\\n      Ebooks & Whitepapers\\n\\n    \\n\\n\\n\\n      Customer Stories\\n\\n    \\n\\n\\n\\n      Partners\\n\\n    \\n\\n\\n\\n      Executive Insights\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Open Source\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          GitHub Sponsors\\n\\n        \\n\\n        Fund open source developers\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          The ReadME Project\\n\\n        \\n\\n        GitHub community articles\\n      \\n\\n\\n\\n\\nRepositories\\n\\n\\n\\n      Topics\\n\\n    \\n\\n\\n\\n      Trending\\n\\n    \\n\\n\\n\\n      Collections'),\n",
       " Document(metadata={'source': 'https://github.com/ritam3', 'title': 'LinkedIn', 'start_index': 2424}, page_content='The ReadME Project\\n\\n        \\n\\n        GitHub community articles\\n      \\n\\n\\n\\n\\nRepositories\\n\\n\\n\\n      Topics\\n\\n    \\n\\n\\n\\n      Trending\\n\\n    \\n\\n\\n\\n      Collections\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Enterprise\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Enterprise platform\\n\\n        \\n\\n        AI-powered developer platform\\n      \\n\\n\\n\\n\\nAvailable add-ons\\n\\n\\n\\n\\n\\n\\n\\n\\n          GitHub Advanced Security\\n\\n        \\n\\n        Enterprise-grade security features\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n          Copilot for business\\n\\n        \\n\\n        Enterprise-grade AI features\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n          Premium Support\\n\\n        \\n\\n        Enterprise-grade 24/7 support\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\nPricing\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch or jump to...\\n\\n\\n\\n\\n\\n\\n\\nSearch code, repositories, users, issues, pull requests...\\n\\n \\n\\n\\n\\n\\n        Search\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nClear\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\nSearch syntax tips \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Provide feedback\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\nWe read every piece of feedback, and take your input very seriously.'),\n",
       " Document(metadata={'source': 'https://github.com/ritam3', 'title': 'LinkedIn', 'start_index': 3251}, page_content='Search syntax tips \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Provide feedback\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\nWe read every piece of feedback, and take your input very seriously.\\n\\n\\nInclude my email address so I can be contacted\\n\\n\\n     Cancel\\n\\n    Submit feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Saved searches\\n      \\nUse saved searches to filter your results more quickly\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\nName\\n\\n\\n\\n\\n\\n\\nQuery\\n\\n\\n\\n            To see all available qualifiers, see our documentation.\\n          \\n \\n\\n\\n\\n\\n\\n     Cancel\\n\\n    Create saved search\\n\\n\\n\\n\\n\\n\\n\\n\\n                Sign in\\n              \\n\\n\\n                Sign up\\n              \\n\\n\\n \\n\\n\\nAppearance settings\\n\\n\\n\\nResetting focus\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nYou signed in with another tab or window. Reload to refresh your session.\\nYou signed out in another tab or window. Reload to refresh your session.\\nYou switched accounts on another tab or window. Reload to refresh your session.\\n \\n\\n\\nDismiss alert\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\nritam3\\n\\n\\nFollow\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n    Overview'),\n",
       " Document(metadata={'source': 'https://github.com/ritam3', 'title': 'LinkedIn', 'start_index': 4108}, page_content='Dismiss alert\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\nritam3\\n\\n\\nFollow\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n    Overview\\n\\n\\n\\n\\n\\n    Repositories\\n    11\\n\\n\\n\\n\\n\\n    Projects\\n    0\\n\\n\\n\\n\\n\\n      Packages\\n      0\\n\\n\\n\\n\\n\\n    Stars\\n    0\\n\\n\\n\\n\\n\\n \\n\\n\\n\\nMore\\n\\n\\n \\n\\n\\nOverview\\n\\n\\nRepositories\\n\\n\\nProjects\\n\\n\\nPackages\\n\\n\\nStars\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\nritam3\\n \\n\\n\\n\\nFollow\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Ritam Upadhyay\\n        \\n\\n          ritam3\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFollow\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTempe\\n\\n\\n\\n\\nhttps://fearless-writers-028990.framer.app/\\n\\nLinkedIn\\n\\n\\n\\n\\nin/ritam-upadhyay-51ba81192\\n\\n\\n\\n\\n \\nBlock or Report\\n\\n\\n\\n\\n\\n\\n\\n\\n        Block or report ritam3\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\nBlock user\\n\\n            Prevent this user from interacting with your repositories and sending you notifications.\\n          Learn more about blocking users.\\n        \\n\\n              You must be logged in to block users.\\n            \\n\\n\\n\\n        Add an optional note'),\n",
       " Document(metadata={'source': 'https://github.com/ritam3', 'title': 'LinkedIn', 'start_index': 4871}, page_content=\"You must be logged in to block users.\\n            \\n\\n\\n\\n        Add an optional note\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMaximum 250 characters. Please don't include any personal information such as legal names or email addresses. Markdown supported. This note will be visible to only you.\\n\\n\\n\\n\\n          Block user\\n        \\n \\n\\nReport abuse\\n\\n        Contact GitHub support about this user’s behavior.\\n        Learn more about reporting abuse.\\n      \\nReport abuse\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n    Overview\\n\\n\\n\\n\\n\\n    Repositories\\n    11\\n\\n\\n\\n\\n\\n    Projects\\n    0\\n\\n\\n\\n\\n\\n      Packages\\n      0\\n\\n\\n\\n\\n\\n    Stars\\n    0\\n\\n\\n\\n\\n\\n \\n\\n\\n\\nMore\\n\\n\\n \\n\\n\\nOverview\\n\\n\\nRepositories\\n\\n\\nProjects\\n\\n\\nPackages\\n\\n\\nStars\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Popular repositories\\n    \\n\\n\\n\\n Loading\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Adafruit-GFX-Library\\n               Adafruit-GFX-Library \\nPublic\\n\\n\\n\\n              Forked from adafruit/Adafruit-GFX-Library\\n\\n\\n            Adafruit GFX graphics core library, this is the 'core' class that all our other graphics libraries derive from\"),\n",
       " Document(metadata={'source': 'https://github.com/ritam3', 'title': 'LinkedIn', 'start_index': 5676}, page_content=\"Forked from adafruit/Adafruit-GFX-Library\\n\\n\\n            Adafruit GFX graphics core library, this is the 'core' class that all our other graphics libraries derive from\\n          \\n\\n\\n\\nC\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Adafruit_TouchScreen\\n               Adafruit_TouchScreen \\nPublic\\n\\n\\n\\n              Forked from adafruit/Adafruit_TouchScreen\\n\\n\\n            Arduino library for 4-wire resistive touchscreens\\n          \\n\\n\\n\\nC++\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Robolution\\n               Robolution \\nPublic\\n\\n\\n\\n            Self balancing bot\\n          \\n\\n\\n\\nC++\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                IEEE-Megaproject\\n               IEEE-Megaproject \\nPublic\\n\\n\\n\\n\\n\\n\\n\\nC\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                USB_Host_Shield_2.0\\n               USB_Host_Shield_2.0 \\nPublic\\n\\n\\n\\n              Forked from felis/USB_Host_Shield_2.0\\n\\n\\n            Revision 2.0 of USB Host Library for Arduino.\\n          \\n\\n\\n\\nC++\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Leafdetect\\n               Leafdetect \\nPublic\\n\\n\\n\\n\\n\\n\\n\\nPython\"),\n",
       " Document(metadata={'source': 'https://github.com/ritam3', 'title': 'LinkedIn', 'start_index': 6464}, page_content='Revision 2.0 of USB Host Library for Arduino.\\n          \\n\\n\\n\\nC++\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Leafdetect\\n               Leafdetect \\nPublic\\n\\n\\n\\n\\n\\n\\n\\nPython\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Something went wrong, please refresh the page to try again.\\n          If the problem persists, check the GitHub status page\\n          or contact support.\\n        \\n\\n\\n\\n\\n        Uh oh!\\n\\n There was an error while loading. Please reload this page.\\n\\n \\n \\n\\n\\n\\n\\n\\n\\n\\n\\nFooter\\n\\n\\n\\n\\n\\n\\n\\n\\n        © 2025 GitHub,\\xa0Inc.\\n      \\n\\n\\nFooter navigation\\n\\n\\nTerms\\n\\n\\nPrivacy\\n\\n\\nSecurity\\n\\n\\nStatus\\n\\n\\nCommunity\\n\\n\\nDocs\\n\\n\\nContact\\n\\n\\n\\n\\n       Manage cookies\\n    \\n\\n\\n\\n\\n\\n      Do not share my personal information\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    You can’t perform that action at this time.'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/', 'start_index': 4}, page_content=\"Ritam UpadhyayData ScientistHomeAboutProjectsStacksSearchMenuAvailable for workBack to homepageBack to homepageBack to homepageRitam UpadhyayData ScientistProblem SolverData ScientistProblem SolverData ScientistProblem SolverData ScientistProblem SolverRitam UpadhyayData ScientistProblem SolverData ScientistProblem SolverData ScientistProblem SolverData ScientistProblem SolverRitam UpadhyayData ScientistProblem SolverData ScientistProblem SolverData ScientistProblem SolverData ScientistProblem SolverAboutWelcome to my portfolio! I am an ASU Master's student ('26) with experience at Juniper Networks and Paytm Money. I leverage ML/AI to solve complex business challenges, specializing in NLP, computer vision, and predictive analytics. Seeking roles in Data Science, AI/ML, and NLP to drive impact at scale.My projectsMy projectsMy\"),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/', 'start_index': 644}, page_content=\"to solve complex business challenges, specializing in NLP, computer vision, and predictive analytics. Seeking roles in Data Science, AI/ML, and NLP to drive impact at scale.My projectsMy projectsMy projectsContactContactContactContactLinkedinritam.upadhyayritam.upadhyayritam.upadhyayGithubritam3ritam3ritam3Emailrupadh17@asu.edurupadh17@asu.edurupadh17@asu.eduEducation2018-2022Bachelor of Technology in Electronics and Communication EngineeringBachelor of Technology in Electronics and Communication EngineeringBachelor of Technology in Electronics and Communication EngineeringBirla Institute of Technology, Mesra2024-2026Master's in Data Science, Analytics and EngineeringMaster's in Data Science, Analytics and EngineeringMaster's in Data Science, Analytics and EngineeringArizona State UniversityWork experienceJune 2025 - August 2025Product Management Intern : Data ScienceProduct Management Intern : Data ScienceProduct Management Intern : Data ScienceJuniper Networks / Mist● Engineered an\"),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/', 'start_index': 1451}, page_content='experienceJune 2025 - August 2025Product Management Intern : Data ScienceProduct Management Intern : Data ScienceProduct Management Intern : Data ScienceJuniper Networks / Mist● Engineered an end-to-end classification pipeline that processed over 13,000 customer support tickets, achieving a 100% automated classification rate and providing proactive insights into emerging issues.'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/', 'start_index': 1833}, page_content='● Developed a multi-stage data processing module using LLMs to automatically extract and normalize critical information, including site, MAC address, and issue description, from raw Salesforce ticket data.\\n● Implemented an advanced LLM-based analysis to parse support agent-customer conversations, automatically identifying root causes and agent actions, which helped streamline case resolution and improve the agent knowledge base.\\n● Leveraged BERTopic on a dataset of over 13,000 \"complete issue\" data points to uncover 56 distinct, actionable topics and 728 subtopics, enabling the model to adapt to dynamic support trends without manual retraining.'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/', 'start_index': 2486}, page_content=\"● Built and deployed a final classification model using Facebook BART to automatically route each ticket to the most relevant subtopic, ensuring every new ticket is assigned a precise classification from the unsupervised model's established categories.August 2022 - July 2024Software Engineer : Data ScienceSoftware Engineer : Data ScienceSoftware Engineer : Data SciencePaytm Money● Developed machine learning models that ran 6000+ customer images per day in real-time to alert users regarding improper face uploads at the front end, increasing photo acceptance rate from 80% to 90%. \\n● Created a machine learning model using XGBoost to predict and identify churned users in advance having 93% recall, and led marketing plan campaigns. \\n● Devised CNN classifier for document recognition across 11 classes, achieving 97% precision and 92% accuracy.\"),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/', 'start_index': 3224}, page_content='● Devised CNN classifier for document recognition across 11 classes, achieving 97% precision and 92% accuracy. \\n● Engineered scalable ML models that handle 20000+ calls per day from the backend and frontend, automating the KYC verification process and streamlining the onboarding journey for users. \\n● Designed a K-Means clustering approach to segment 1.1M users according to investment habits in Mutual Funds. \\n● Initiated statistics-based verification of account statements for Income and Bank Proof, automating 85% of account statement pdf uploads.January 2022 - July 2022Software InternSoftware InternSoftware InternPaytm Money● Designed ML models to extract Period and Names from Account Statements with 95% precision.\\n● Built ML model to perform similarity analysis using Random Forest Classifier with 98% precision.'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/', 'start_index': 3948}, page_content='● Built ML model to perform similarity analysis using Random Forest Classifier with 98% precision. \\n● Deployed models as Rest API using FastAPI for scaling.May 2020 - July 2021Research InternResearch InternResearch InternCentre of Excellence in Advanced Manufacturing Technology, IIT Kharagpur● Performed literature survey to identify gripper selection and pose estimation for robotic grippers in assembly lines.\\n● Created an AI-based system based on computer vision for job recognition to estimate object pose and coordinates with 100% accuracy to grasp on the test set.'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/', 'start_index': 4520}, page_content='● Published Real Time Deep Learning-Based Image Processing for Pose Estimation and Object Localization in Autonomous Robot Applications, The International Journal of Advanced Manufacturing Technology, Springer.ProjectsJanuary 2025RAG-PDF QA Method - RAG-PDF QA Method - RAG-PDF QA Method - Implementation of RAG method to perform question answering on PDF documents. HuggingFace embeddings in addition to the Groq API were used for efficient and accurate information retrieval. Session management was done to ensure embeddings are computed once per document upload to optimize performance by avoiding redundant computations. December 2024LLM Based Q/A on Web Context  - LLM Q/A based on OLLAMALLM Based Q/A on Web Context  - LLM Q/A based on OLLAMALLM Based Q/A on Web Context  - LLM Q/A based on OLLAMADevelopment of Web App that uses open source LLMs to answer user queries based on its own knowledge as well as any reference provided in the form of external webpages.December 2024Next Word - Next'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/', 'start_index': 5335}, page_content='of Web App that uses open source LLMs to answer user queries based on its own knowledge as well as any reference provided in the form of external webpages.December 2024Next Word - Next Word PredictionNext Word - Next Word PredictionNext Word - Next Word Prediction\"Forecasting the Future: Utilizing LSTM for Next-Word Prediction in Natural Language Processing.\" October 2023Churn Prediction and Customer Cohort - Churn Prediction and Customer Cohort - Churn Prediction and Customer Cohort - Development of a Machine Learning Model that takes user\\'s activity data as input and predicts if the user is going to churnSeptember 2023Improper Face Detection at Frontend - Finding faces that do not meet requirementsImproper Face Detection at Frontend - Finding faces that do not meet requirementsImproper Face Detection at Frontend - Finding faces that do not meet requirementsUse image processing and deep learning to alert users at the frontend that their photos might be rejected in the backend as'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/', 'start_index': 6134}, page_content='Face Detection at Frontend - Finding faces that do not meet requirementsUse image processing and deep learning to alert users at the frontend that their photos might be rejected in the backend as their photo do not meet the requirements such as lighting, eyes properly visible, no spoof and properly clothed.October 2022Account Statement Verification - Account Statement Verification - Account Statement Verification - Development of models that extract parameters like account number, ifsc, date, name, duration and so on and matches them with the requirements to accept or reject the account statement as a bank or income proofMarch 2022AI based Name Matcher - Matching 2 Indian Names to match the similarityAI based Name Matcher - Matching 2 Indian Names to match the similarityAI based Name Matcher - Matching 2 Indian Names to match the similarityDevelopment of Machine Learning Model that predicts similarity between 2 names trained on data collected for indian names.July 2021Low Cost'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/', 'start_index': 6929}, page_content='Matcher - Matching 2 Indian Names to match the similarityDevelopment of Machine Learning Model that predicts similarity between 2 names trained on data collected for indian names.July 2021Low Cost Vision-Based Gripper - Camera based gripper to locate, localize and grasp objectLow Cost Vision-Based Gripper - Camera based gripper to locate, localize and grasp objectLow Cost Vision-Based Gripper - Camera based gripper to locate, localize and grasp objectA KUKA robot was automated using a simple camera that enabled to locate, localize, identify the pose and feed the coordinates to robot in order to grisp it. Image processing techniques and deep learning methods were used recognize the object and PnP model was used to find the pose for a perfect grasp.StackSoftware & services I use in my workflow.PythonPythonPythonSQLSQLSQLStreamlitStreamlitStreamlitTensorflowTensorflowTensorflowMoreMoreMoreCertificationsJune 2020CourseraCourseraCourseraNeural Networks and Deep LearningJune'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/', 'start_index': 7713}, page_content='use in my workflow.PythonPythonPythonSQLSQLSQLStreamlitStreamlitStreamlitTensorflowTensorflowTensorflowMoreMoreMoreCertificationsJune 2020CourseraCourseraCourseraNeural Networks and Deep LearningJune 2020CourseraCourseraCourseraImproving Deep Neural Networks: Hyperparameter Tuning, Regularization and OptimizationJuly 2024CourseraCourseraCourseraSupervised Machine Learning: Regression and ClassificationResearch PapersSeptember 2022SpringerReal-time deep learning–based image processing for pose estimation and object localization in autonomous robot applicationsSeptember 2022SpringerReal-time deep learning–based image processing for pose estimation and object localization in autonomous robot applicationsSeptember 2022SpringerReal-time deep learning–based image processing for pose estimation and object localization in autonomous robot applicationsIndexMain HomeAboutResourcesProjectStackContactrupadh17@asu.eduLinkedinIndexMain'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/', 'start_index': 8455}, page_content='deep learning–based image processing for pose estimation and object localization in autonomous robot applicationsIndexMain HomeAboutResourcesProjectStackContactrupadh17@asu.eduLinkedinIndexMain HomeAboutResourcesProjectStackContactrupadh17@asu.eduLinkedinIndexMain HomeAboutResourcesProjectStackContactrupadh17@asu.eduLinkedin'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/', 'start_index': 8783}, page_content='Create a free website with Framer, the website builder loved by startups, designers and agencies.'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/project/old-home', 'start_index': 4}, page_content='404404404The page you are looking for doesn’t existTry to use a correct url or go back to homepage to start againBack to homepage\\n\\nCreate a free website with Framer, the website builder loved by startups, designers and agencies.'),\n",
       " Document(metadata={'source': 'https://scholar.google.com/citations?user=04o0bdcAAAAJ&hl=en', 'start_index': 0}, page_content='Loading...The system can\\'t perform the operation now. Try again later.Citations per yearDuplicate citationsThe following articles are merged in Scholar. Their combined citations are counted only for the first article.Merged citationsThis \"Cited by\" count includes citations to the following articles in Scholar. The ones marked * may be different from the article in the profile.Add co-authorsCo-authorsFollowNew articles by this authorNew citations to this authorNew articles related to this author\\'s researchEmail address for updatesDoneMy profileMy libraryMetricsAlertsSettingsSign inSign inGet my own profileCited byAllSince 2020Citations99h-index11i10-index00063202320242025126FollowRitam UpadhyayGraduate Student @ Arizona State UniversityVerified email at asu.eduArticlesCited byTitleSortSort by citationsSort by yearSort by titleCited byCited byYearReal-time deep learning–based image processing for pose estimation and object localization in autonomous robot applicationsR Upadhyay, A Asi, P'),\n",
       " Document(metadata={'source': 'https://scholar.google.com/citations?user=04o0bdcAAAAJ&hl=en', 'start_index': 803}, page_content=\"citationsSort by yearSort by titleCited byCited byYearReal-time deep learning–based image processing for pose estimation and object localization in autonomous robot applicationsR Upadhyay, A Asi, P Nayak, N Prasad, D Mishra, SK PalThe International Journal of Advanced Manufacturing Technology 127 (3), 1905\\xa0…, 202392023CMT-Bench: Cricket Multi-Table Generation Benchmark for Probing Robustness in Large Language ModelsR Upadhyay, N Ahuja, R Baral, A Garimella, V GuptaarXiv preprint arXiv:2510.18173, 20252025The system can't perform the operation now. Try again later.Articles 1–2Show morePrivacyTermsHelpAbout ScholarSearch help\"),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/project/project/account-statement-verification', 'start_index': 4}, page_content='404404404The page you are looking for doesn’t existTry to use a correct url or go back to homepage to start againBack to homepage\\n\\nCreate a free website with Framer, the website builder loved by startups, designers and agencies.'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/project/', 'start_index': 4}, page_content='Ritam UpadhyayData ScientistHomeAboutProjectsStacksSearchMenuAvailable for workBack to homepageBack to homepageBack to homepageMy Latest WorksRAG-PDF QA MethodImplementation of RAG method to perform question answering on PDF documents. HuggingFace embeddings in addition to the Groq API were used for efficient and accurate information retrieval. Session management was done to ensure embeddings are computed once per document upload to optimize performance by avoiding redundant computations. January 2025RAG-PDF QA MethodImplementation of RAG method to perform question answering on PDF documents. HuggingFace embeddings in addition to the Groq API were used for efficient and accurate information retrieval. Session management was done to ensure embeddings are computed once per document upload to optimize performance by avoiding redundant computations. January 2025RAG-PDF QA MethodImplementation of RAG method to perform question answering on PDF documents. HuggingFace embeddings in addition'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/project/', 'start_index': 805}, page_content='optimize performance by avoiding redundant computations. January 2025RAG-PDF QA MethodImplementation of RAG method to perform question answering on PDF documents. HuggingFace embeddings in addition to the Groq API were used for efficient and accurate information retrieval. Session management was done to ensure embeddings are computed once per document upload to optimize performance by avoiding redundant computations. January 2025LLM Based Q/A on Web Context Development of Web App that uses open source LLMs to answer user queries based on its own knowledge as well as any reference provided in the form of external webpages.December 2024LLM Based Q/A on Web Context Development of Web App that uses open source LLMs to answer user queries based on its own knowledge as well as any reference provided in the form of external webpages.December 2024LLM Based Q/A on Web Context Development of Web App that uses open source LLMs to answer user queries based on its own knowledge as well as any'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/project/', 'start_index': 1601}, page_content='provided in the form of external webpages.December 2024LLM Based Q/A on Web Context Development of Web App that uses open source LLMs to answer user queries based on its own knowledge as well as any reference provided in the form of external webpages.December 2024Next Word\"Forecasting the Future: Utilizing LSTM for Next-Word Prediction in Natural Language Processing.\" December 2024Next Word\"Forecasting the Future: Utilizing LSTM for Next-Word Prediction in Natural Language Processing.\" December 2024Next Word\"Forecasting the Future: Utilizing LSTM for Next-Word Prediction in Natural Language Processing.\" December 2024Churn Prediction and Customer CohortDevelopment of a Machine Learning Model that takes user\\'s activity data as input and predicts if the user is going to churnOctober 2023Churn Prediction and Customer CohortDevelopment of a Machine Learning Model that takes user\\'s activity data as input and predicts if the user is going to churnOctober 2023Churn Prediction and Customer'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/project/', 'start_index': 2402}, page_content=\"Prediction and Customer CohortDevelopment of a Machine Learning Model that takes user's activity data as input and predicts if the user is going to churnOctober 2023Churn Prediction and Customer CohortDevelopment of a Machine Learning Model that takes user's activity data as input and predicts if the user is going to churnOctober 2023Improper Face Detection at FrontendUse image processing and deep learning to alert users at the frontend that their photos might be rejected in the backend as their photo do not meet the requirements such as lighting, eyes properly visible, no spoof and properly clothed.September 2023Improper Face Detection at FrontendUse image processing and deep learning to alert users at the frontend that their photos might be rejected in the backend as their photo do not meet the requirements such as lighting, eyes properly visible, no spoof and properly clothed.September 2023Improper Face Detection at FrontendUse image processing and deep learning to alert users at\"),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/project/', 'start_index': 3201}, page_content='meet the requirements such as lighting, eyes properly visible, no spoof and properly clothed.September 2023Improper Face Detection at FrontendUse image processing and deep learning to alert users at the frontend that their photos might be rejected in the backend as their photo do not meet the requirements such as lighting, eyes properly visible, no spoof and properly clothed.September 2023Account Statement VerificationDevelopment of models that extract parameters like account number, ifsc, date, name, duration and so on and matches them with the requirements to accept or reject the account statement as a bank or income proofOctober 2022Account Statement VerificationDevelopment of models that extract parameters like account number, ifsc, date, name, duration and so on and matches them with the requirements to accept or reject the account statement as a bank or income proofOctober 2022Account Statement VerificationDevelopment of models that extract parameters like account number, ifsc,'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/project/', 'start_index': 4001}, page_content='the requirements to accept or reject the account statement as a bank or income proofOctober 2022Account Statement VerificationDevelopment of models that extract parameters like account number, ifsc, date, name, duration and so on and matches them with the requirements to accept or reject the account statement as a bank or income proofOctober 2022AI based Name MatcherDevelopment of Machine Learning Model that predicts similarity between 2 names trained on data collected for indian names.March 2022AI based Name MatcherDevelopment of Machine Learning Model that predicts similarity between 2 names trained on data collected for indian names.March 2022AI based Name MatcherDevelopment of Machine Learning Model that predicts similarity between 2 names trained on data collected for indian names.March 2022Low Cost Vision-Based GripperA KUKA robot was automated using a simple camera that enabled to locate, localize, identify the pose and feed the coordinates to robot in order to grisp it. Image'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/project/', 'start_index': 4804}, page_content='2022Low Cost Vision-Based GripperA KUKA robot was automated using a simple camera that enabled to locate, localize, identify the pose and feed the coordinates to robot in order to grisp it. Image processing techniques and deep learning methods were used recognize the object and PnP model was used to find the pose for a perfect grasp.July 2021Low Cost Vision-Based GripperA KUKA robot was automated using a simple camera that enabled to locate, localize, identify the pose and feed the coordinates to robot in order to grisp it. Image processing techniques and deep learning methods were used recognize the object and PnP model was used to find the pose for a perfect grasp.July 2021Low Cost Vision-Based GripperA KUKA robot was automated using a simple camera that enabled to locate, localize, identify the pose and feed the coordinates to robot in order to grisp it. Image processing techniques and deep learning methods were used recognize the object and PnP model was used to find the pose for'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/project/', 'start_index': 5609}, page_content='the pose and feed the coordinates to robot in order to grisp it. Image processing techniques and deep learning methods were used recognize the object and PnP model was used to find the pose for a perfect grasp.July 2021IndexMain HomeAboutResourcesProjectStackContactrupadh17@asu.eduLinkedinIndexMain HomeAboutResourcesProjectStackContactrupadh17@asu.eduLinkedinIndexMain HomeAboutResourcesProjectStackContactrupadh17@asu.eduLinkedin'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/project/', 'start_index': 6043}, page_content='Create a free website with Framer, the website builder loved by startups, designers and agencies.'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/project/project/low-cost-vision-based-gripper', 'start_index': 4}, page_content='404404404The page you are looking for doesn’t existTry to use a correct url or go back to homepage to start againBack to homepage\\n\\nCreate a free website with Framer, the website builder loved by startups, designers and agencies.'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/project/project', 'start_index': 4}, page_content='404404404The page you are looking for doesn’t existTry to use a correct url or go back to homepage to start againBack to homepage\\n\\nCreate a free website with Framer, the website builder loved by startups, designers and agencies.'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/project/project/churn-prediction-and-customer-cohort', 'start_index': 4}, page_content='404404404The page you are looking for doesn’t existTry to use a correct url or go back to homepage to start againBack to homepage\\n\\nCreate a free website with Framer, the website builder loved by startups, designers and agencies.'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/project/stack', 'start_index': 4}, page_content='404404404The page you are looking for doesn’t existTry to use a correct url or go back to homepage to start againBack to homepage\\n\\nCreate a free website with Framer, the website builder loved by startups, designers and agencies.'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/project/project/llm-based-q-a-on-web-context', 'start_index': 4}, page_content='404404404The page you are looking for doesn’t existTry to use a correct url or go back to homepage to start againBack to homepage\\n\\nCreate a free website with Framer, the website builder loved by startups, designers and agencies.'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/project/project/ai-based-name-matcher', 'start_index': 4}, page_content='404404404The page you are looking for doesn’t existTry to use a correct url or go back to homepage to start againBack to homepage\\n\\nCreate a free website with Framer, the website builder loved by startups, designers and agencies.'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/project/project/next-word', 'start_index': 4}, page_content='404404404The page you are looking for doesn’t existTry to use a correct url or go back to homepage to start againBack to homepage\\n\\nCreate a free website with Framer, the website builder loved by startups, designers and agencies.'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/project/project/improper-face-detection-at-frontend', 'start_index': 4}, page_content='404404404The page you are looking for doesn’t existTry to use a correct url or go back to homepage to start againBack to homepage\\n\\nCreate a free website with Framer, the website builder loved by startups, designers and agencies.'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/project/project/rag-pdf-qa-method', 'start_index': 4}, page_content='404404404The page you are looking for doesn’t existTry to use a correct url or go back to homepage to start againBack to homepage\\n\\nCreate a free website with Framer, the website builder loved by startups, designers and agencies.')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = load_web_docs([\"https://fearless-writers-028990.framer.app/\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://fearless-writers-028990.framer.app/'}, page_content='\\n\\n\\n\\nRitam UpadhyayData ScientistHomeAboutProjectsStacksSearchMenuAvailable for workBack to homepageBack to homepageBack to homepageRitam UpadhyayData ScientistProblem SolverData ScientistProblem SolverData ScientistProblem SolverData ScientistProblem SolverRitam UpadhyayData ScientistProblem SolverData ScientistProblem SolverData ScientistProblem SolverData ScientistProblem SolverRitam UpadhyayData ScientistProblem SolverData ScientistProblem SolverData ScientistProblem SolverData ScientistProblem SolverAboutWelcome to my portfolio! I am an ASU Master\\'s student (\\'26) with experience at Juniper Networks and Paytm Money. I leverage ML/AI to solve complex business challenges, specializing in NLP, computer vision, and predictive analytics. Seeking roles in Data Science, AI/ML, and NLP to drive impact at scale.My projectsMy projectsMy projectsContactContactContactContactLinkedinritam.upadhyayritam.upadhyayritam.upadhyayGithubritam3ritam3ritam3Emailrupadh17@asu.edurupadh17@asu.edurupadh17@asu.eduEducation2018-2022Bachelor of Technology in Electronics and Communication EngineeringBachelor of Technology in Electronics and Communication EngineeringBachelor of Technology in Electronics and Communication EngineeringBirla Institute of Technology, Mesra2024-2026Master\\'s in Data Science, Analytics and EngineeringMaster\\'s in Data Science, Analytics and EngineeringMaster\\'s in Data Science, Analytics and EngineeringArizona State UniversityWork experienceJune 2025 - August 2025Product Management Intern : Data ScienceProduct Management Intern : Data ScienceProduct Management Intern : Data ScienceJuniper Networks / Mist● Engineered an end-to-end classification pipeline that processed over 13,000 customer support tickets, achieving a 100% automated classification rate and providing proactive insights into emerging issues.\\n● Developed a multi-stage data processing module using LLMs to automatically extract and normalize critical information, including site, MAC address, and issue description, from raw Salesforce ticket data.\\n● Implemented an advanced LLM-based analysis to parse support agent-customer conversations, automatically identifying root causes and agent actions, which helped streamline case resolution and improve the agent knowledge base.\\n● Leveraged BERTopic on a dataset of over 13,000 \"complete issue\" data points to uncover 56 distinct, actionable topics and 728 subtopics, enabling the model to adapt to dynamic support trends without manual retraining.\\n● Built and deployed a final classification model using Facebook BART to automatically route each ticket to the most relevant subtopic, ensuring every new ticket is assigned a precise classification from the unsupervised model\\'s established categories.August 2022 - July 2024Software Engineer : Data ScienceSoftware Engineer : Data ScienceSoftware Engineer : Data SciencePaytm Money● Developed machine learning models that ran 6000+ customer images per day in real-time to alert users regarding improper face uploads at the front end, increasing photo acceptance rate from 80% to 90%. \\n● Created a machine learning model using XGBoost to predict and identify churned users in advance having 93% recall, and led marketing plan campaigns. \\n● Devised CNN classifier for document recognition across 11 classes, achieving 97% precision and 92% accuracy. \\n● Engineered scalable ML models that handle 20000+ calls per day from the backend and frontend, automating the KYC verification process and streamlining the onboarding journey for users. \\n● Designed a K-Means clustering approach to segment 1.1M users according to investment habits in Mutual Funds. \\n● Initiated statistics-based verification of account statements for Income and Bank Proof, automating 85% of account statement pdf uploads.January 2022 - July 2022Software InternSoftware InternSoftware InternPaytm Money● Designed ML models to extract Period and Names from Account Statements with 95% precision.\\n● Built ML model to perform similarity analysis using Random Forest Classifier with 98% precision. \\n● Deployed models as Rest API using FastAPI for scaling.May 2020 - July 2021Research InternResearch InternResearch InternCentre of Excellence in Advanced Manufacturing Technology, IIT Kharagpur● Performed literature survey to identify gripper selection and pose estimation for robotic grippers in assembly lines.\\n● Created an AI-based system based on computer vision for job recognition to estimate object pose and coordinates with 100% accuracy to grasp on the test set.\\n● Published Real Time Deep Learning-Based Image Processing for Pose Estimation and Object Localization in Autonomous Robot Applications, The International Journal of Advanced Manufacturing Technology, Springer.ProjectsJanuary 2025RAG-PDF QA Method - RAG-PDF QA Method - RAG-PDF QA Method - Implementation of RAG method to perform question answering on PDF documents. HuggingFace embeddings in addition to the Groq API were used for efficient and accurate information retrieval. Session management was done to ensure embeddings are computed once per document upload to optimize performance by avoiding redundant computations. December 2024LLM Based Q/A on Web Context  - LLM Q/A based on OLLAMALLM Based Q/A on Web Context  - LLM Q/A based on OLLAMALLM Based Q/A on Web Context  - LLM Q/A based on OLLAMADevelopment of Web App that uses open source LLMs to answer user queries based on its own knowledge as well as any reference provided in the form of external webpages.December 2024Next Word - Next Word PredictionNext Word - Next Word PredictionNext Word - Next Word Prediction\"Forecasting the Future: Utilizing LSTM for Next-Word Prediction in Natural Language Processing.\" October 2023Churn Prediction and Customer Cohort - Churn Prediction and Customer Cohort - Churn Prediction and Customer Cohort - Development of a Machine Learning Model that takes user\\'s activity data as input and predicts if the user is going to churnSeptember 2023Improper Face Detection at Frontend - Finding faces that do not meet requirementsImproper Face Detection at Frontend - Finding faces that do not meet requirementsImproper Face Detection at Frontend - Finding faces that do not meet requirementsUse image processing and deep learning to alert users at the frontend that their photos might be rejected in the backend as their photo do not meet the requirements such as lighting, eyes properly visible, no spoof and properly clothed.October 2022Account Statement Verification - Account Statement Verification - Account Statement Verification - Development of models that extract parameters like account number, ifsc, date, name, duration and so on and matches them with the requirements to accept or reject the account statement as a bank or income proofMarch 2022AI based Name Matcher - Matching 2 Indian Names to match the similarityAI based Name Matcher - Matching 2 Indian Names to match the similarityAI based Name Matcher - Matching 2 Indian Names to match the similarityDevelopment of Machine Learning Model that predicts similarity between 2 names trained on data collected for indian names.July 2021Low Cost Vision-Based Gripper - Camera based gripper to locate, localize and grasp objectLow Cost Vision-Based Gripper - Camera based gripper to locate, localize and grasp objectLow Cost Vision-Based Gripper - Camera based gripper to locate, localize and grasp objectA KUKA robot was automated using a simple camera that enabled to locate, localize, identify the pose and feed the coordinates to robot in order to grisp it. Image processing techniques and deep learning methods were used recognize the object and PnP model was used to find the pose for a perfect grasp.StackSoftware & services I use in my workflow.PythonPythonPythonSQLSQLSQLStreamlitStreamlitStreamlitTensorflowTensorflowTensorflowMoreMoreMoreCertificationsJune 2020CourseraCourseraCourseraNeural Networks and Deep LearningJune 2020CourseraCourseraCourseraImproving Deep Neural Networks: Hyperparameter Tuning, Regularization and OptimizationJuly 2024CourseraCourseraCourseraSupervised Machine Learning: Regression and ClassificationResearch PapersSeptember 2022SpringerReal-time deep learning–based image processing for pose estimation and object localization in autonomous robot applicationsSeptember 2022SpringerReal-time deep learning–based image processing for pose estimation and object localization in autonomous robot applicationsSeptember 2022SpringerReal-time deep learning–based image processing for pose estimation and object localization in autonomous robot applicationsIndexMain HomeAboutResourcesProjectStackContactrupadh17@asu.eduLinkedinIndexMain HomeAboutResourcesProjectStackContactrupadh17@asu.eduLinkedinIndexMain HomeAboutResourcesProjectStackContactrupadh17@asu.eduLinkedin\\n\\nCreate a free website with Framer, the website builder loved by startups, designers and agencies.\\n\\n\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Ritam UpadhyayData ScientistHomeAboutProjectsStacksSearchMenuAvailable for workBack to homepageBack to homepageBack to homepageRitam UpadhyayData ScientistProblem SolverData ScientistProblem SolverData ScientistProblem SolverData ScientistProblem SolverRitam UpadhyayData ScientistProblem SolverData ScientistProblem SolverData ScientistProblem SolverData ScientistProblem SolverRitam UpadhyayData ScientistProblem SolverData ScientistProblem SolverData ScientistProblem SolverData ScientistProblem SolverAboutWelcome to my portfolio! I am an ASU Master's student ('26) with experience at Juniper Networks and Paytm Money. I leverage ML/AI to solve complex business challenges, specializing in NLP, computer vision, and predictive analytics. Seeking roles in Data Science, AI/ML, and NLP to drive impact at scale.My projectsMy projectsMy projectsContactContactContactContactLinkedinritam.upadhyayritam.upadhyayritam.upadhyayGithubritam3ritam3ritam3Emailrupadh17@asu.edurupadh17@asu.edurupadh17@asu.eduEducation2018-2022Bachelor of Technology in Electronics and Communication EngineeringBachelor of Technology in Electronics and Communication EngineeringBachelor of Technology in Electronics and Communication EngineeringBirla Institute of Technology, Mesra2024-2026Master's in Data Science, Analytics and EngineeringMaster's in Data Science, Analytics and EngineeringMaster's in Data Science, Analytics and EngineeringArizona State UniversityWork experienceJune 2025 - August 2025Product Management Intern : Data ScienceProduct Management Intern : Data ScienceProduct Management Intern : Data ScienceJuniper Networks / Mist● Engineered an end-to-end classification pipeline that processed over 13,000 customer support tickets, achieving a 100% automated classification rate and providing proactive insights into emerging issues.\n",
      "● Developed a multi-stage data processing module using LLMs to automatically extract and normalize critical information, including site, MAC address, and issue description, from raw Salesforce ticket data.\n",
      "● Implemented an advanced LLM-based analysis to parse support agent-customer conversations, automatically identifying root causes and agent actions, which helped streamline case resolution and improve the agent knowledge base.\n",
      "● Leveraged BERTopic on a dataset of over 13,000 \"complete issue\" data points to uncover 56 distinct, actionable topics and 728 subtopics, enabling the model to adapt to dynamic support trends without manual retraining.\n",
      "● Built and deployed a final classification model using Facebook BART to automatically route each ticket to the most relevant subtopic, ensuring every new ticket is assigned a precise classification from the unsupervised model's established categories.August 2022 - July 2024Software Engineer : Data ScienceSoftware Engineer : Data ScienceSoftware Engineer : Data SciencePaytm Money● Developed machine learning models that ran 6000+ customer images per day in real-time to alert users regarding improper face uploads at the front end, increasing photo acceptance rate from 80% to 90%. \n",
      "● Created a machine learning model using XGBoost to predict and identify churned users in advance having 93% recall, and led marketing plan campaigns. \n",
      "● Devised CNN classifier for document recognition across 11 classes, achieving 97% precision and 92% accuracy. \n",
      "● Engineered scalable ML models that handle 20000+ calls per day from the backend and frontend, automating the KYC verification process and streamlining the onboarding journey for users. \n",
      "● Designed a K-Means clustering approach to segment 1.1M users according to investment habits in Mutual Funds. \n",
      "● Initiated statistics-based verification of account statements for Income and Bank Proof, automating 85% of account statement pdf uploads.January 2022 - July 2022Software InternSoftware InternSoftware InternPaytm Money● Designed ML models to extract Period and Names from Account Statements with 95% precision.\n",
      "● Built ML model to perform similarity analysis using Random Forest Classifier with 98% precision. \n",
      "● Deployed models as Rest API using FastAPI for scaling.May 2020 - July 2021Research InternResearch InternResearch InternCentre of Excellence in Advanced Manufacturing Technology, IIT Kharagpur● Performed literature survey to identify gripper selection and pose estimation for robotic grippers in assembly lines.\n",
      "● Created an AI-based system based on computer vision for job recognition to estimate object pose and coordinates with 100% accuracy to grasp on the test set.\n",
      "● Published Real Time Deep Learning-Based Image Processing for Pose Estimation and Object Localization in Autonomous Robot Applications, The International Journal of Advanced Manufacturing Technology, Springer.ProjectsJanuary 2025RAG-PDF QA Method - RAG-PDF QA Method - RAG-PDF QA Method - Implementation of RAG method to perform question answering on PDF documents. HuggingFace embeddings in addition to the Groq API were used for efficient and accurate information retrieval. Session management was done to ensure embeddings are computed once per document upload to optimize performance by avoiding redundant computations. December 2024LLM Based Q/A on Web Context  - LLM Q/A based on OLLAMALLM Based Q/A on Web Context  - LLM Q/A based on OLLAMALLM Based Q/A on Web Context  - LLM Q/A based on OLLAMADevelopment of Web App that uses open source LLMs to answer user queries based on its own knowledge as well as any reference provided in the form of external webpages.December 2024Next Word - Next Word PredictionNext Word - Next Word PredictionNext Word - Next Word Prediction\"Forecasting the Future: Utilizing LSTM for Next-Word Prediction in Natural Language Processing.\" October 2023Churn Prediction and Customer Cohort - Churn Prediction and Customer Cohort - Churn Prediction and Customer Cohort - Development of a Machine Learning Model that takes user's activity data as input and predicts if the user is going to churnSeptember 2023Improper Face Detection at Frontend - Finding faces that do not meet requirementsImproper Face Detection at Frontend - Finding faces that do not meet requirementsImproper Face Detection at Frontend - Finding faces that do not meet requirementsUse image processing and deep learning to alert users at the frontend that their photos might be rejected in the backend as their photo do not meet the requirements such as lighting, eyes properly visible, no spoof and properly clothed.October 2022Account Statement Verification - Account Statement Verification - Account Statement Verification - Development of models that extract parameters like account number, ifsc, date, name, duration and so on and matches them with the requirements to accept or reject the account statement as a bank or income proofMarch 2022AI based Name Matcher - Matching 2 Indian Names to match the similarityAI based Name Matcher - Matching 2 Indian Names to match the similarityAI based Name Matcher - Matching 2 Indian Names to match the similarityDevelopment of Machine Learning Model that predicts similarity between 2 names trained on data collected for indian names.July 2021Low Cost Vision-Based Gripper - Camera based gripper to locate, localize and grasp objectLow Cost Vision-Based Gripper - Camera based gripper to locate, localize and grasp objectLow Cost Vision-Based Gripper - Camera based gripper to locate, localize and grasp objectA KUKA robot was automated using a simple camera that enabled to locate, localize, identify the pose and feed the coordinates to robot in order to grisp it. Image processing techniques and deep learning methods were used recognize the object and PnP model was used to find the pose for a perfect grasp.StackSoftware & services I use in my workflow.PythonPythonPythonSQLSQLSQLStreamlitStreamlitStreamlitTensorflowTensorflowTensorflowMoreMoreMoreCertificationsJune 2020CourseraCourseraCourseraNeural Networks and Deep LearningJune 2020CourseraCourseraCourseraImproving Deep Neural Networks: Hyperparameter Tuning, Regularization and OptimizationJuly 2024CourseraCourseraCourseraSupervised Machine Learning: Regression and ClassificationResearch PapersSeptember 2022SpringerReal-time deep learning–based image processing for pose estimation and object localization in autonomous robot applicationsSeptember 2022SpringerReal-time deep learning–based image processing for pose estimation and object localization in autonomous robot applicationsSeptember 2022SpringerReal-time deep learning–based image processing for pose estimation and object localization in autonomous robot applicationsIndexMain HomeAboutResourcesProjectStackContactrupadh17@asu.eduLinkedinIndexMain HomeAboutResourcesProjectStackContactrupadh17@asu.eduLinkedinIndexMain HomeAboutResourcesProjectStackContactrupadh17@asu.eduLinkedin\n",
      "\n",
      "Create a free website with Framer, the website builder loved by startups, designers and agencies.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://fearless-writers-028990.framer.app/', 'start_index': 4}, page_content=\"Ritam UpadhyayData ScientistHomeAboutProjectsStacksSearchMenuAvailable for workBack to homepageBack to homepageBack to homepageRitam UpadhyayData ScientistProblem SolverData ScientistProblem SolverData ScientistProblem SolverData ScientistProblem SolverRitam UpadhyayData ScientistProblem SolverData ScientistProblem SolverData ScientistProblem SolverData ScientistProblem SolverRitam UpadhyayData ScientistProblem SolverData ScientistProblem SolverData ScientistProblem SolverData ScientistProblem SolverAboutWelcome to my portfolio! I am an ASU Master's student ('26) with experience at Juniper Networks and Paytm Money. I leverage ML/AI to solve complex business challenges, specializing in NLP, computer vision, and predictive analytics. Seeking roles in Data Science, AI/ML, and NLP to drive impact at scale.My projectsMy projectsMy\"),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/', 'start_index': 644}, page_content=\"to solve complex business challenges, specializing in NLP, computer vision, and predictive analytics. Seeking roles in Data Science, AI/ML, and NLP to drive impact at scale.My projectsMy projectsMy projectsContactContactContactContactLinkedinritam.upadhyayritam.upadhyayritam.upadhyayGithubritam3ritam3ritam3Emailrupadh17@asu.edurupadh17@asu.edurupadh17@asu.eduEducation2018-2022Bachelor of Technology in Electronics and Communication EngineeringBachelor of Technology in Electronics and Communication EngineeringBachelor of Technology in Electronics and Communication EngineeringBirla Institute of Technology, Mesra2024-2026Master's in Data Science, Analytics and EngineeringMaster's in Data Science, Analytics and EngineeringMaster's in Data Science, Analytics and EngineeringArizona State UniversityWork experienceJune 2025 - August 2025Product Management Intern : Data ScienceProduct Management Intern : Data ScienceProduct Management Intern : Data ScienceJuniper Networks / Mist● Engineered an\"),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/', 'start_index': 1451}, page_content='experienceJune 2025 - August 2025Product Management Intern : Data ScienceProduct Management Intern : Data ScienceProduct Management Intern : Data ScienceJuniper Networks / Mist● Engineered an end-to-end classification pipeline that processed over 13,000 customer support tickets, achieving a 100% automated classification rate and providing proactive insights into emerging issues.'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/', 'start_index': 1833}, page_content='● Developed a multi-stage data processing module using LLMs to automatically extract and normalize critical information, including site, MAC address, and issue description, from raw Salesforce ticket data.\\n● Implemented an advanced LLM-based analysis to parse support agent-customer conversations, automatically identifying root causes and agent actions, which helped streamline case resolution and improve the agent knowledge base.\\n● Leveraged BERTopic on a dataset of over 13,000 \"complete issue\" data points to uncover 56 distinct, actionable topics and 728 subtopics, enabling the model to adapt to dynamic support trends without manual retraining.'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/', 'start_index': 2486}, page_content=\"● Built and deployed a final classification model using Facebook BART to automatically route each ticket to the most relevant subtopic, ensuring every new ticket is assigned a precise classification from the unsupervised model's established categories.August 2022 - July 2024Software Engineer : Data ScienceSoftware Engineer : Data ScienceSoftware Engineer : Data SciencePaytm Money● Developed machine learning models that ran 6000+ customer images per day in real-time to alert users regarding improper face uploads at the front end, increasing photo acceptance rate from 80% to 90%. \\n● Created a machine learning model using XGBoost to predict and identify churned users in advance having 93% recall, and led marketing plan campaigns. \\n● Devised CNN classifier for document recognition across 11 classes, achieving 97% precision and 92% accuracy.\"),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/', 'start_index': 3224}, page_content='● Devised CNN classifier for document recognition across 11 classes, achieving 97% precision and 92% accuracy. \\n● Engineered scalable ML models that handle 20000+ calls per day from the backend and frontend, automating the KYC verification process and streamlining the onboarding journey for users. \\n● Designed a K-Means clustering approach to segment 1.1M users according to investment habits in Mutual Funds. \\n● Initiated statistics-based verification of account statements for Income and Bank Proof, automating 85% of account statement pdf uploads.January 2022 - July 2022Software InternSoftware InternSoftware InternPaytm Money● Designed ML models to extract Period and Names from Account Statements with 95% precision.\\n● Built ML model to perform similarity analysis using Random Forest Classifier with 98% precision.'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/', 'start_index': 3948}, page_content='● Built ML model to perform similarity analysis using Random Forest Classifier with 98% precision. \\n● Deployed models as Rest API using FastAPI for scaling.May 2020 - July 2021Research InternResearch InternResearch InternCentre of Excellence in Advanced Manufacturing Technology, IIT Kharagpur● Performed literature survey to identify gripper selection and pose estimation for robotic grippers in assembly lines.\\n● Created an AI-based system based on computer vision for job recognition to estimate object pose and coordinates with 100% accuracy to grasp on the test set.'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/', 'start_index': 4520}, page_content='● Published Real Time Deep Learning-Based Image Processing for Pose Estimation and Object Localization in Autonomous Robot Applications, The International Journal of Advanced Manufacturing Technology, Springer.ProjectsJanuary 2025RAG-PDF QA Method - RAG-PDF QA Method - RAG-PDF QA Method - Implementation of RAG method to perform question answering on PDF documents. HuggingFace embeddings in addition to the Groq API were used for efficient and accurate information retrieval. Session management was done to ensure embeddings are computed once per document upload to optimize performance by avoiding redundant computations. December 2024LLM Based Q/A on Web Context  - LLM Q/A based on OLLAMALLM Based Q/A on Web Context  - LLM Q/A based on OLLAMALLM Based Q/A on Web Context  - LLM Q/A based on OLLAMADevelopment of Web App that uses open source LLMs to answer user queries based on its own knowledge as well as any reference provided in the form of external webpages.December 2024Next Word - Next'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/', 'start_index': 5335}, page_content='of Web App that uses open source LLMs to answer user queries based on its own knowledge as well as any reference provided in the form of external webpages.December 2024Next Word - Next Word PredictionNext Word - Next Word PredictionNext Word - Next Word Prediction\"Forecasting the Future: Utilizing LSTM for Next-Word Prediction in Natural Language Processing.\" October 2023Churn Prediction and Customer Cohort - Churn Prediction and Customer Cohort - Churn Prediction and Customer Cohort - Development of a Machine Learning Model that takes user\\'s activity data as input and predicts if the user is going to churnSeptember 2023Improper Face Detection at Frontend - Finding faces that do not meet requirementsImproper Face Detection at Frontend - Finding faces that do not meet requirementsImproper Face Detection at Frontend - Finding faces that do not meet requirementsUse image processing and deep learning to alert users at the frontend that their photos might be rejected in the backend as'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/', 'start_index': 6134}, page_content='Face Detection at Frontend - Finding faces that do not meet requirementsUse image processing and deep learning to alert users at the frontend that their photos might be rejected in the backend as their photo do not meet the requirements such as lighting, eyes properly visible, no spoof and properly clothed.October 2022Account Statement Verification - Account Statement Verification - Account Statement Verification - Development of models that extract parameters like account number, ifsc, date, name, duration and so on and matches them with the requirements to accept or reject the account statement as a bank or income proofMarch 2022AI based Name Matcher - Matching 2 Indian Names to match the similarityAI based Name Matcher - Matching 2 Indian Names to match the similarityAI based Name Matcher - Matching 2 Indian Names to match the similarityDevelopment of Machine Learning Model that predicts similarity between 2 names trained on data collected for indian names.July 2021Low Cost'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/', 'start_index': 6929}, page_content='Matcher - Matching 2 Indian Names to match the similarityDevelopment of Machine Learning Model that predicts similarity between 2 names trained on data collected for indian names.July 2021Low Cost Vision-Based Gripper - Camera based gripper to locate, localize and grasp objectLow Cost Vision-Based Gripper - Camera based gripper to locate, localize and grasp objectLow Cost Vision-Based Gripper - Camera based gripper to locate, localize and grasp objectA KUKA robot was automated using a simple camera that enabled to locate, localize, identify the pose and feed the coordinates to robot in order to grisp it. Image processing techniques and deep learning methods were used recognize the object and PnP model was used to find the pose for a perfect grasp.StackSoftware & services I use in my workflow.PythonPythonPythonSQLSQLSQLStreamlitStreamlitStreamlitTensorflowTensorflowTensorflowMoreMoreMoreCertificationsJune 2020CourseraCourseraCourseraNeural Networks and Deep LearningJune'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/', 'start_index': 7713}, page_content='use in my workflow.PythonPythonPythonSQLSQLSQLStreamlitStreamlitStreamlitTensorflowTensorflowTensorflowMoreMoreMoreCertificationsJune 2020CourseraCourseraCourseraNeural Networks and Deep LearningJune 2020CourseraCourseraCourseraImproving Deep Neural Networks: Hyperparameter Tuning, Regularization and OptimizationJuly 2024CourseraCourseraCourseraSupervised Machine Learning: Regression and ClassificationResearch PapersSeptember 2022SpringerReal-time deep learning–based image processing for pose estimation and object localization in autonomous robot applicationsSeptember 2022SpringerReal-time deep learning–based image processing for pose estimation and object localization in autonomous robot applicationsSeptember 2022SpringerReal-time deep learning–based image processing for pose estimation and object localization in autonomous robot applicationsIndexMain HomeAboutResourcesProjectStackContactrupadh17@asu.eduLinkedinIndexMain'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/', 'start_index': 8455}, page_content='deep learning–based image processing for pose estimation and object localization in autonomous robot applicationsIndexMain HomeAboutResourcesProjectStackContactrupadh17@asu.eduLinkedinIndexMain HomeAboutResourcesProjectStackContactrupadh17@asu.eduLinkedinIndexMain HomeAboutResourcesProjectStackContactrupadh17@asu.eduLinkedin'),\n",
       " Document(metadata={'source': 'https://fearless-writers-028990.framer.app/', 'start_index': 8783}, page_content='Create a free website with Framer, the website builder loved by startups, designers and agencies.')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_docs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rag_core/index_builder.py\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import requests\n",
    "from langchain_community.document_loaders import (\n",
    "    OnlinePDFLoader,\n",
    "    PyPDFLoader,\n",
    ")\n",
    "from langchain_text_splitters import (\n",
    "    HTMLSectionSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "# ---------- Helpers for URLs ----------\n",
    "\n",
    "def _is_gdrive_file(url: str) -> bool:\n",
    "    \"\"\"Return True if this looks like a Google Drive file view URL.\"\"\"\n",
    "    return \"drive.google.com\" in url and \"/file/d/\" in url\n",
    "\n",
    "\n",
    "def _gdrive_view_to_download(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert a Google Drive view URL to a direct download URL.\n",
    "\n",
    "    Example:\n",
    "      https://drive.google.com/file/d/<ID>/view\n",
    "      -> https://drive.google.com/uc?export=download&id=<ID>\n",
    "    \"\"\"\n",
    "    m = re.search(r\"/file/d/([^/]+)/\", url)\n",
    "    if not m:\n",
    "        return url\n",
    "    file_id = m.group(1)\n",
    "    return f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
    "\n",
    "\n",
    "def _infer_section_label_from_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Heuristic: guess a section label from the URL path.\n",
    "    e.g.\n",
    "      https://your-site.com/about                 -> 'about'\n",
    "      https://your-site.com/experience/juniper   -> 'experience/juniper'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        path = url.split(\"://\", 1)[-1].split(\"/\", 1)[-1]\n",
    "    except Exception:\n",
    "        return url\n",
    "    path = path.strip(\"/\")\n",
    "    if not path:\n",
    "        return \"root\"\n",
    "    return path\n",
    "\n",
    "\n",
    "# ---------- Local resume PDF ----------\n",
    "\n",
    "\n",
    "# ---------- Remote docs: HTML (via HTMLSectionSplitter) + PDFs ----------\n",
    "\n",
    "def load_remote_docs(urls: List[str]):\n",
    "    \"\"\"\n",
    "    Load documents from remote URLs.\n",
    "\n",
    "    - HTML URLs: use `requests` to fetch HTML string, then HTMLSectionSplitter.split_text(html_string).\n",
    "    - PDF URLs (including Google Drive /file/d/.../view): use OnlinePDFLoader.\n",
    "    \"\"\"\n",
    "    html_urls: List[str] = []\n",
    "    pdf_urls: List[str] = []\n",
    "\n",
    "    for url in urls:\n",
    "        u = url.strip()\n",
    "        if not u:\n",
    "            continue\n",
    "\n",
    "        html_urls.append(u)\n",
    "\n",
    "    docs = []\n",
    "\n",
    "    # --- HTML: fetch raw HTML and split with HTMLSectionSplitter ---\n",
    "    if html_urls:\n",
    "        print(f\"[index_builder] Loading HTML for {len(html_urls)} URLs with HTMLSectionSplitter\")\n",
    "\n",
    "        headers_to_split_on = [\n",
    "            (\"h1\", \"Header 1\"),\n",
    "            (\"h2\", \"Header 2\"),\n",
    "        ]\n",
    "        html_splitter = HTMLSectionSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "        for url in html_urls:\n",
    "            try:\n",
    "                print(f\"[index_builder]   Fetching HTML from {url}\")\n",
    "                resp = requests.get(url, timeout=15)\n",
    "                resp.raise_for_status()\n",
    "                html_string = resp.text\n",
    "\n",
    "                # This is exactly the pattern you requested:\n",
    "                # html_header_splits = html_splitter.split_text(html_string)\n",
    "                html_header_splits = html_splitter.split_text(html_string)\n",
    "\n",
    "                section_label = _infer_section_label_from_url(url)\n",
    "                print(f\"[index_builder]   {url}: {len(html_header_splits)} HTML sections\")\n",
    "\n",
    "                for d in html_header_splits:\n",
    "                    d.metadata[\"source\"] = url\n",
    "                    d.metadata[\"section_label\"] = section_label  # existing label you already set\n",
    "                    # extract a sensible header line to use when matching queries\n",
    "                    # prefer an explicit header tag if present (HTMLSectionSplitter often places it at the top)\n",
    "                    first_lines = [ln.strip() for ln in d.page_content.splitlines() if ln.strip()]\n",
    "                    header_line = first_lines[0] if first_lines else \"\"\n",
    "                    # normalize header to be short (cut long inline text)\n",
    "                    if len(header_line) > 200:\n",
    "                        header_line = header_line[:200] + \"...\"\n",
    "                    d.metadata[\"section_header\"] = header_line\n",
    "                    d.metadata[\"section_type\"] = \"remote_html\"\n",
    "                # then append these docs to your docs list as usual\n",
    "                docs.extend(html_header_splits)\n",
    "            except Exception as e:\n",
    "                print(f\"[index_builder] Error processing HTML from {url}: {e}\")\n",
    "\n",
    "    # --- PDFs (remote, including Drive) ---\n",
    "    for pdf_url in pdf_urls:\n",
    "        print(f\"[index_builder] Loading PDF from {pdf_url}\")\n",
    "        try:\n",
    "            pdf_loader = OnlinePDFLoader(pdf_url)\n",
    "            pdf_docs = pdf_loader.load()\n",
    "            section_label = _infer_section_label_from_url(pdf_url)\n",
    "            for d in pdf_docs:\n",
    "                d.metadata[\"source\"] = pdf_url\n",
    "                d.metadata[\"section_label\"] = section_label\n",
    "                d.metadata[\"section_type\"] = \"remote_pdf\"\n",
    "            docs.extend(pdf_docs)\n",
    "            print(f\"[index_builder]   {pdf_url}: {len(pdf_docs)} PDF pages\")\n",
    "        except Exception as e:\n",
    "            print(f\"[index_builder] Failed to load PDF from {pdf_url}: {e}\")\n",
    "\n",
    "    return docs\n",
    "\n",
    "\n",
    "# ---------- Chunking ----------\n",
    "\n",
    "def split_docs(docs):\n",
    "    \"\"\"\n",
    "    Split documents into chunks.\n",
    "\n",
    "    - HTML docs: already split into section-level chunks by HTMLSectionSplitter → keep as-is.\n",
    "    - Non-HTML docs (PDF/resume): use RecursiveCharacterTextSplitter.\n",
    "    \"\"\"\n",
    "    html_docs = [d for d in docs if d.metadata.get(\"section_type\") == \"remote_html\"]\n",
    "    other_docs = [d for d in docs if d.metadata.get(\"section_type\") != \"remote_html\"]\n",
    "\n",
    "    chunks: List = []\n",
    "\n",
    "    # HTML docs are already section chunks\n",
    "    chunks.extend(html_docs)\n",
    "\n",
    "    # PDFs / resume get text splitting\n",
    "    if other_docs:\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=2000,\n",
    "            chunk_overlap=200,\n",
    "            add_start_index=True,\n",
    "        )\n",
    "        other_chunks = splitter.split_documents(other_docs)\n",
    "        for c in other_chunks:\n",
    "            c.metadata.setdefault(\"section_label\", c.metadata.get(\"source\", \"unknown\"))\n",
    "            c.metadata.setdefault(\n",
    "                \"section_type\",\n",
    "                c.metadata.get(\"section_type\", \"unknown\")\n",
    "            )\n",
    "        chunks.extend(other_chunks)\n",
    "\n",
    "    print(\n",
    "        f\"[index_builder] split_docs: {len(html_docs)} HTML section chunks, \"\n",
    "        f\"{len(chunks) - len(html_docs)} non-HTML chunks\"\n",
    "    )\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# ---------- Build & load index ----------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[index_builder] Loading HTML for 1 URLs with HTMLSectionSplitter\n",
      "[index_builder]   Fetching HTML from https://fearless-writers-028990.framer.app\n",
      "[index_builder]   https://fearless-writers-028990.framer.app: 9 HTML sections\n"
     ]
    }
   ],
   "source": [
    "docs = load_remote_docs([\"https://fearless-writers-028990.framer.app\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Work experience \n",
      " \n",
      " $ \n",
      " June 2025 - August 2025 \n",
      " \n",
      " $ \n",
      " $ \n",
      " Product Management Intern : Data Science \n",
      " \n",
      " \n",
      " /$ \n",
      " \n",
      " \n",
      " $ \n",
      " Product Management Intern : Data Science \n",
      " \n",
      " \n",
      " /$ \n",
      " \n",
      " \n",
      " $ \n",
      " Product Management Intern : Data Science \n",
      " \n",
      " \n",
      " /$ \n",
      " \n",
      " /$ \n",
      " Juniper Networks / Mist \n",
      " â Engineered an end-to-end classification pipeline that processed over 13,000 customer support tickets, achieving a 100% automated classification rate and providing proactive insights into emerging issues.\n",
      "â Developed a multi-stage data processing module using LLMs to automatically extract and normalize critical information, including site, MAC address, and issue description, from raw Salesforce ticket data.\n",
      "â Implemented an advanced LLM-based analysis to parse support agent-customer conversations, automatically identifying root causes and agent actions, which helped streamline case resolution and improve the agent knowledge base.\n",
      "â Leveraged BERTopic on a dataset of over 13,000 \"complete issue\" data points to uncover 56 distinct, actionable topics and 728 subtopics, enabling the model to adapt to dynamic support trends without manual retraining.\n",
      "â Built and deployed a final classification model using Facebook BART to automatically route each ticket to the most relevant subtopic, ensuring every new ticket is assigned a precise classification from the unsupervised model's established categories. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " August 2022 - July 2024 \n",
      " \n",
      " $ \n",
      " $ \n",
      " Software Engineer : Data Science \n",
      " \n",
      " \n",
      " /$ \n",
      " \n",
      " \n",
      " $ \n",
      " Software Engineer : Data Science \n",
      " \n",
      " \n",
      " /$ \n",
      " \n",
      " \n",
      " $ \n",
      " Software Engineer : Data Science \n",
      " \n",
      " \n",
      " /$ \n",
      " \n",
      " /$ \n",
      " Paytm Money \n",
      " â Developed machine learning models that ran 6000+ customer images per day in real-time to alert users regarding improper face uploads at the front end, increasing photo acceptance rate from 80% to 90%. \n",
      "â Created a machine learning model using XGBoost to predict and identify churned users in advance having 93% recall, and led marketing plan campaigns. \n",
      "â Devised CNN classifier for document recognition across 11 classes, achieving 97% precision and 92% accuracy. \n",
      "â Engineered scalable ML models that handle 20000+ calls per day from the backend and frontend, automating the KYC verification process and streamlining the onboarding journey for users. \n",
      "â Designed a K-Means clustering approach to segment 1.1M users according to investment habits in Mutual Funds. \n",
      "â Initiated statistics-based verification of account statements for Income and Bank Proof, automating 85% of account statement pdf uploads. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " January 2022 - July 2022 \n",
      " \n",
      " $ \n",
      " $ \n",
      " Software Intern \n",
      " \n",
      " \n",
      " /$ \n",
      " \n",
      " \n",
      " $ \n",
      " Software Intern \n",
      " \n",
      " \n",
      " /$ \n",
      " \n",
      " \n",
      " $ \n",
      " Software Intern \n",
      " \n",
      " \n",
      " /$ \n",
      " \n",
      " /$ \n",
      " Paytm Money \n",
      " â Designed ML models to extract Period and Names from Account Statements with 95% precision.\n",
      "â Built ML model to perform similarity analysis using Random Forest Classifier with 98% precision. \n",
      "â Deployed models as Rest API using FastAPI for scaling. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " May 2020 - July 2021 \n",
      " \n",
      " $ \n",
      " $ \n",
      " Research Intern \n",
      " \n",
      " \n",
      " /$ \n",
      " \n",
      " \n",
      " $ \n",
      " Research Intern \n",
      " \n",
      " \n",
      " /$ \n",
      " \n",
      " \n",
      " $ \n",
      " Research Intern \n",
      " \n",
      " \n",
      " /$ \n",
      " \n",
      " /$ \n",
      " Centre of Excellence in Advanced Manufacturing Technology, IIT Kharagpur \n",
      " â Performed literature survey to identify gripper selection and pose estimation for robotic grippers in assembly lines.\n",
      "â Created an AI-based system based on computer vision for job recognition to estimate object pose and coordinates with 100% accuracy to grasp on the test set.\n",
      "â Published Real Time Deep Learning-Based Image Processing for Pose Estimation and Object Localization in Autonomous Robot Applications, The International Journal of Advanced Manufacturing Technology, Springer. \n",
      " \n",
      " \n",
      " \n",
      " /$\n"
     ]
    }
   ],
   "source": [
    "print(docs[4].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': '#TITLE#', 'source': 'https://fearless-writers-028990.framer.app', 'section_label': 'fearless-writers-028990.framer.app', 'section_header': 'Start of bodyStart', 'section_type': 'remote_html'}, page_content='Start of bodyStart  \\n  End of bodyStart  \\n \\n $ html body { background: var(--token-29c1a320-cdc0-4d5e-a54f-37299311641d, rgb(255, 255, 255)); } \\n \\n \\n \\n \\n $ \\n \\n Ritam Upadhyay \\n Data Scientist \\n /$ \\n \\n \\n $ \\n $ \\n Home \\n /$ \\n \\n /$ $ \\n $ \\n About \\n /$ \\n \\n /$ $ \\n $ \\n Projects \\n /$ \\n \\n /$ $ \\n $ \\n Stacks \\n /$ \\n \\n /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Search \\n \\n \\n \\n \\n $ /$ \\n \\n Menu \\n \\n \\n \\n \\n \\n \\n \\n \\n $ /$ \\n \\n \\n $ \\n $ /$ \\n \\n /$ $ \\n $ /$ \\n \\n /$ $ \\n $ /$ \\n \\n /$ $ \\n $ /$ \\n \\n /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Available for work \\n \\n \\n \\n $ \\n $ \\n Back to homepage \\n \\n \\n /$ \\n \\n \\n $ \\n Back to homepage \\n \\n \\n /$ \\n \\n \\n $ \\n Back to homepage \\n \\n \\n /$ \\n \\n /$ \\n \\n \\n \\n $ \\n \\n Ritam Upadhyay \\n \\n Data Scientist \\n Problem Solver \\n Data Scientist \\n Problem Solver \\n Data Scientist \\n Problem Solver \\n Data Scientist \\n Problem Solver \\n \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n Ritam Upadhyay \\n \\n Data Scientist \\n Problem Solver \\n Data Scientist \\n Problem Solver \\n Data Scientist \\n Problem Solver \\n Data Scientist \\n Problem Solver \\n \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n Ritam Upadhyay \\n \\n Data Scientist \\n Problem Solver \\n Data Scientist \\n Problem Solver \\n Data Scientist \\n Problem Solver \\n Data Scientist \\n Problem Solver \\n \\n \\n \\n \\n /$'),\n",
       " Document(metadata={'Header 2': 'About', 'source': 'https://fearless-writers-028990.framer.app', 'section_label': 'fearless-writers-028990.framer.app', 'section_header': 'About', 'section_type': 'remote_html'}, page_content=\"About \\n Welcome to my portfolio! I am an ASU Master's student ('26) with experience at Juniper Networks and Paytm Money. I leverage ML/AI to solve complex business challenges, specializing in NLP, computer vision, and predictive analytics. Seeking roles in Data Science, AI/ML, and NLP to drive impact at scale. \\n \\n \\n $ \\n $ My projects \\n /$ \\n \\n \\n $ My projects \\n /$ \\n \\n \\n $ My projects \\n /$ \\n \\n /$ $ \\n $ Contact /$ \\n \\n \\n $ Contact /$ \\n \\n \\n $ Contact /$ \\n \\n /$\"),\n",
       " Document(metadata={'Header 2': 'Contact', 'source': 'https://fearless-writers-028990.framer.app', 'section_label': 'fearless-writers-028990.framer.app', 'section_header': 'Contact', 'section_type': 'remote_html'}, page_content='Contact \\n \\n \\n Linkedin \\n \\n $ \\n ritam.upadhyay \\n \\n \\n /$ \\n \\n \\n $ \\n ritam.upadhyay \\n \\n \\n /$ \\n \\n \\n $ \\n ritam.upadhyay \\n \\n \\n /$ \\n \\n \\n \\n Github \\n \\n $ \\n ritam3 \\n \\n \\n /$ \\n \\n \\n $ \\n ritam3 \\n \\n \\n /$ \\n \\n \\n $ \\n ritam3 \\n \\n \\n /$ \\n \\n \\n \\n Email \\n \\n $ \\n rupadh17@asu.edu \\n \\n \\n /$ \\n \\n \\n $ \\n rupadh17@asu.edu \\n \\n \\n /$ \\n \\n \\n $ \\n rupadh17@asu.edu \\n \\n \\n /$'),\n",
       " Document(metadata={'Header 2': 'Education', 'source': 'https://fearless-writers-028990.framer.app', 'section_label': 'fearless-writers-028990.framer.app', 'section_header': 'Education', 'section_type': 'remote_html'}, page_content=\"Education \\n \\n \\n 2018-2022 \\n \\n \\n $ \\n Bachelor of Technology in Electronics and Communication Engineering \\n \\n \\n /$ \\n \\n \\n $ \\n Bachelor of Technology in Electronics and Communication Engineering \\n \\n \\n /$ \\n \\n \\n $ \\n Bachelor of Technology in Electronics and Communication Engineering \\n \\n \\n /$ \\n \\n Birla Institute of Technology, Mesra \\n \\n \\n \\n 2024-2026 \\n \\n \\n $ \\n Master's in Data Science, Analytics and Engineering \\n \\n \\n /$ \\n \\n \\n $ \\n Master's in Data Science, Analytics and Engineering \\n \\n \\n /$ \\n \\n \\n $ \\n Master's in Data Science, Analytics and Engineering \\n \\n \\n /$ \\n \\n Arizona State University\"),\n",
       " Document(metadata={'Header 2': 'Work experience', 'source': 'https://fearless-writers-028990.framer.app', 'section_label': 'fearless-writers-028990.framer.app', 'section_header': 'Work experience', 'section_type': 'remote_html'}, page_content='Work experience \\n \\n $ \\n June 2025 - August 2025 \\n \\n $ \\n $ \\n Product Management Intern : Data Science \\n \\n \\n /$ \\n \\n \\n $ \\n Product Management Intern : Data Science \\n \\n \\n /$ \\n \\n \\n $ \\n Product Management Intern : Data Science \\n \\n \\n /$ \\n \\n /$ \\n Juniper Networks / Mist \\n â\\x97\\x8f Engineered an end-to-end classification pipeline that processed over 13,000 customer support tickets, achieving a 100% automated classification rate and providing proactive insights into emerging issues.\\nâ\\x97\\x8f Developed a multi-stage data processing module using LLMs to automatically extract and normalize critical information, including site, MAC address, and issue description, from raw Salesforce ticket data.\\nâ\\x97\\x8f Implemented an advanced LLM-based analysis to parse support agent-customer conversations, automatically identifying root causes and agent actions, which helped streamline case resolution and improve the agent knowledge base.\\nâ\\x97\\x8f Leveraged BERTopic on a dataset of over 13,000 \"complete issue\" data points to uncover 56 distinct, actionable topics and 728 subtopics, enabling the model to adapt to dynamic support trends without manual retraining.\\nâ\\x97\\x8f Built and deployed a final classification model using Facebook BART to automatically route each ticket to the most relevant subtopic, ensuring every new ticket is assigned a precise classification from the unsupervised model\\'s established categories. \\n \\n \\n \\n \\n August 2022 - July 2024 \\n \\n $ \\n $ \\n Software Engineer : Data Science \\n \\n \\n /$ \\n \\n \\n $ \\n Software Engineer : Data Science \\n \\n \\n /$ \\n \\n \\n $ \\n Software Engineer : Data Science \\n \\n \\n /$ \\n \\n /$ \\n Paytm Money \\n â\\x97\\x8f Developed machine learning models that ran 6000+ customer images per day in real-time to alert users regarding improper face uploads at the front end, increasing photo acceptance rate from 80% to 90%. \\nâ\\x97\\x8f Created a machine learning model using XGBoost to predict and identify churned users in advance having 93% recall, and led marketing plan campaigns. \\nâ\\x97\\x8f Devised CNN classifier for document recognition across 11 classes, achieving 97% precision and 92% accuracy. \\nâ\\x97\\x8f Engineered scalable ML models that handle 20000+ calls per day from the backend and frontend, automating the KYC verification process and streamlining the onboarding journey for users. \\nâ\\x97\\x8f Designed a K-Means clustering approach to segment 1.1M users according to investment habits in Mutual Funds. \\nâ\\x97\\x8f Initiated statistics-based verification of account statements for Income and Bank Proof, automating 85% of account statement pdf uploads. \\n \\n \\n \\n \\n January 2022 - July 2022 \\n \\n $ \\n $ \\n Software Intern \\n \\n \\n /$ \\n \\n \\n $ \\n Software Intern \\n \\n \\n /$ \\n \\n \\n $ \\n Software Intern \\n \\n \\n /$ \\n \\n /$ \\n Paytm Money \\n â\\x97\\x8f Designed ML models to extract Period and Names from Account Statements with 95% precision.\\nâ\\x97\\x8f Built ML model to perform similarity analysis using Random Forest Classifier with 98% precision. \\nâ\\x97\\x8f Deployed models as Rest API using FastAPI for scaling. \\n \\n \\n \\n \\n May 2020 - July 2021 \\n \\n $ \\n $ \\n Research Intern \\n \\n \\n /$ \\n \\n \\n $ \\n Research Intern \\n \\n \\n /$ \\n \\n \\n $ \\n Research Intern \\n \\n \\n /$ \\n \\n /$ \\n Centre of Excellence in Advanced Manufacturing Technology, IIT Kharagpur \\n â\\x97\\x8f Performed literature survey to identify gripper selection and pose estimation for robotic grippers in assembly lines.\\nâ\\x97\\x8f Created an AI-based system based on computer vision for job recognition to estimate object pose and coordinates with 100% accuracy to grasp on the test set.\\nâ\\x97\\x8f Published Real Time Deep Learning-Based Image Processing for Pose Estimation and Object Localization in Autonomous Robot Applications, The International Journal of Advanced Manufacturing Technology, Springer. \\n \\n \\n \\n /$'),\n",
       " Document(metadata={'Header 2': 'Projects', 'source': 'https://fearless-writers-028990.framer.app', 'section_label': 'fearless-writers-028990.framer.app', 'section_header': 'Projects', 'section_type': 'remote_html'}, page_content='Projects \\n \\n $ \\n January 2025 \\n \\n $ \\n $ \\n RAG-PDF QA Method -  \\n \\n \\n /$ \\n \\n \\n $ \\n RAG-PDF QA Method -  \\n \\n \\n /$ \\n \\n \\n $ \\n RAG-PDF QA Method -  \\n \\n \\n /$ \\n \\n /$ \\n \\n Implementation of RAG method to perform question answering on PDF documents. HuggingFace embeddings in addition to the Groq API were used for efficient and accurate information retrieval. Session management was done to ensure embeddings are computed once per document upload to optimize performance by avoiding redundant computations.  \\n \\n \\n \\n \\n December 2024 \\n \\n $ \\n $ \\n LLM Based Q/A on Web Context  - LLM Q/A based on OLLAMA \\n \\n \\n /$ \\n \\n \\n $ \\n LLM Based Q/A on Web Context  - LLM Q/A based on OLLAMA \\n \\n \\n /$ \\n \\n \\n $ \\n LLM Based Q/A on Web Context  - LLM Q/A based on OLLAMA \\n \\n \\n /$ \\n \\n /$ \\n \\n Development of Web App that uses open source LLMs to answer user queries based on its own knowledge as well as any reference provided in the form of external webpages. \\n \\n \\n \\n \\n December 2024 \\n \\n $ \\n $ \\n Next Word - Next Word Prediction \\n \\n \\n /$ \\n \\n \\n $ \\n Next Word - Next Word Prediction \\n \\n \\n /$ \\n \\n \\n $ \\n Next Word - Next Word Prediction \\n \\n \\n /$ \\n \\n /$ \\n \\n \"Forecasting the Future: Utilizing LSTM for Next-Word Prediction in Natural Language Processing.\"  \\n \\n \\n \\n \\n October 2023 \\n \\n $ \\n $ \\n Churn Prediction and Customer Cohort -  \\n \\n \\n /$ \\n \\n \\n $ \\n Churn Prediction and Customer Cohort -  \\n \\n \\n /$ \\n \\n \\n $ \\n Churn Prediction and Customer Cohort -  \\n \\n \\n /$ \\n \\n /$ \\n \\n Development of a Machine Learning Model that takes user\\'s activity data as input and predicts if the user is going to churn \\n \\n \\n \\n \\n September 2023 \\n \\n $ \\n $ \\n Improper Face Detection at Frontend - Finding faces that do not meet requirements \\n \\n \\n /$ \\n \\n \\n $ \\n Improper Face Detection at Frontend - Finding faces that do not meet requirements \\n \\n \\n /$ \\n \\n \\n $ \\n Improper Face Detection at Frontend - Finding faces that do not meet requirements \\n \\n \\n /$ \\n \\n /$ \\n \\n Use image processing and deep learning to alert users at the frontend that their photos might be rejected in the backend as their photo do not meet the requirements such as lighting, eyes properly visible, no spoof and properly clothed. \\n \\n \\n \\n \\n October 2022 \\n \\n $ \\n $ \\n Account Statement Verification -  \\n \\n \\n /$ \\n \\n \\n $ \\n Account Statement Verification -  \\n \\n \\n /$ \\n \\n \\n $ \\n Account Statement Verification -  \\n \\n \\n /$ \\n \\n /$ \\n \\n Development of models that extract parameters like account number, ifsc, date, name, duration and so on and matches them with the requirements to accept or reject the account statement as a bank or income proof \\n \\n \\n \\n \\n March 2022 \\n \\n $ \\n $ \\n AI based Name Matcher - Matching 2 Indian Names to match the similarity \\n \\n \\n /$ \\n \\n \\n $ \\n AI based Name Matcher - Matching 2 Indian Names to match the similarity \\n \\n \\n /$ \\n \\n \\n $ \\n AI based Name Matcher - Matching 2 Indian Names to match the similarity \\n \\n \\n /$ \\n \\n /$ \\n \\n Development of Machine Learning Model that predicts similarity between 2 names trained on data collected for indian names. \\n \\n \\n \\n \\n July 2021 \\n \\n $ \\n $ \\n Low Cost Vision-Based Gripper - Camera based gripper to locate, localize and grasp object \\n \\n \\n /$ \\n \\n \\n $ \\n Low Cost Vision-Based Gripper - Camera based gripper to locate, localize and grasp object \\n \\n \\n /$ \\n \\n \\n $ \\n Low Cost Vision-Based Gripper - Camera based gripper to locate, localize and grasp object \\n \\n \\n /$ \\n \\n /$ \\n \\n A KUKA robot was automated using a simple camera that enabled to locate, localize, identify the pose and feed the coordinates to robot in order to grisp it. Image processing techniques and deep learning methods were used recognize the object and PnP model was used to find the pose for a perfect grasp. \\n \\n \\n \\n /$'),\n",
       " Document(metadata={'Header 2': 'Stack', 'source': 'https://fearless-writers-028990.framer.app', 'section_label': 'fearless-writers-028990.framer.app', 'section_header': 'Stack', 'section_type': 'remote_html'}, page_content='Stack \\n Software & services I use in my workflow. \\n \\n \\n $ $ \\n $ \\n \\n \\n Python \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Python \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Python \\n \\n \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n SQL \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n SQL \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n SQL \\n \\n \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n Streamlit \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Streamlit \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Streamlit \\n \\n \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n Tensorflow \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Tensorflow \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Tensorflow \\n \\n \\n \\n /$ \\n \\n /$ /$ \\n \\n $ \\n $ More \\n /$ \\n \\n \\n $ More \\n /$ \\n \\n \\n $ More \\n /$ \\n \\n /$'),\n",
       " Document(metadata={'Header 2': 'Certifications', 'source': 'https://fearless-writers-028990.framer.app', 'section_label': 'fearless-writers-028990.framer.app', 'section_header': 'Certifications', 'section_type': 'remote_html'}, page_content='Certifications \\n \\n \\n June 2020 \\n \\n \\n $ \\n Coursera \\n \\n \\n /$ \\n \\n \\n $ \\n Coursera \\n \\n \\n /$ \\n \\n \\n $ \\n Coursera \\n \\n \\n /$ \\n \\n Neural Networks and Deep Learning \\n \\n \\n \\n June 2020 \\n \\n \\n $ \\n Coursera \\n \\n \\n /$ \\n \\n \\n $ \\n Coursera \\n \\n \\n /$ \\n \\n \\n $ \\n Coursera \\n \\n \\n /$ \\n \\n Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization \\n \\n \\n \\n July 2024 \\n \\n \\n $ \\n Coursera \\n \\n \\n /$ \\n \\n \\n $ \\n Coursera \\n \\n \\n /$ \\n \\n \\n $ \\n Coursera \\n \\n \\n /$ \\n \\n Supervised Machine Learning: Regression and Classification'),\n",
       " Document(metadata={'Header 2': 'Research Papers', 'source': 'https://fearless-writers-028990.framer.app', 'section_label': 'fearless-writers-028990.framer.app', 'section_header': 'Research Papers', 'section_type': 'remote_html'}, page_content='Research Papers \\n \\n \\n September 2022 \\n \\n \\n $ \\n Springer \\n \\n \\n /$ \\n \\n Real-time deep learningâ\\x80\\x93based image processing for pose estimation and object localization in autonomous robot applications \\n \\n \\n \\n September 2022 \\n \\n \\n $ \\n Springer \\n \\n \\n /$ \\n \\n Real-time deep learningâ\\x80\\x93based image processing for pose estimation and object localization in autonomous robot applications \\n \\n \\n \\n September 2022 \\n \\n \\n $ \\n Springer \\n \\n \\n /$ \\n \\n Real-time deep learningâ\\x80\\x93based image processing for pose estimation and object localization in autonomous robot applications \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Index \\n \\n $ Main Home /$ \\n $ About /$ \\n \\n \\n \\n Resources \\n \\n $ Project /$ \\n $ Stack /$ \\n \\n \\n \\n Contact \\n \\n $ rupadh17@asu.edu /$ \\n $ Linkedin /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Index \\n \\n $ Main Home /$ \\n $ About /$ \\n \\n \\n \\n Resources \\n \\n $ Project /$ \\n $ Stack /$ \\n \\n \\n \\n Contact \\n \\n $ rupadh17@asu.edu /$ \\n $ Linkedin /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Index \\n \\n $ Main Home /$ \\n $ About /$ \\n \\n \\n \\n Resources \\n \\n $ Project /$ \\n $ Stack /$ \\n \\n \\n \\n Contact \\n \\n $ rupadh17@asu.edu /$ \\n $ Linkedin /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n /$ \\n \\n (()=>{function u(){function n(t,e,i){let r=document.createElement(\"a\");r.href=t,r.target=i,r.rel=e,document.body.appendChild(r),r.click(),r.remove()}function o(t){if(this.dataset.hydrated){this.removeEventListener(\"click\",o);return}t.preventDefault(),t.stopPropagation();let e=this.getAttribute(\"href\");if(!e)return;if(/Mac|iPod|iPhone|iPad/u.test(navigator.userAgent)?t.metaKey:t.ctrlKey)return n(e,\"\",\"_blank\");let r=this.getAttribute(\"rel\")??\"\",c=this.getAttribute(\"target\")??\"\";n(e,r,c)}function a(t){if(this.dataset.hydrated){this.removeEventListener(\"auxclick\",o);return}t.preventDefault(),t.stopPropagation();let e=this.getAttribute(\"href\");e&&n(e,\"\",\"_blank\")}function s(t){if(this.dataset.hydrated){this.removeEventListener(\"keydown\",s);return}if(t.key!==\"Enter\")return;t.preventDefault(),t.stopPropagation();let e=this.getAttribute(\"href\");if(!e)return;let i=this.getAttribute(\"rel\")??\"\",r=this.getAttribute(\"target\")??\"\";n(e,i,r)}document.querySelectorAll(\"[data-nested-link]\").forEach(t=>{t instanceof HTMLElement&&(t.addEventListener(\"click\",o),t.addEventListener(\"auxclick\",a),t.addEventListener(\"keydown\",s))})}return u})()() (()=>{function i(){for(let e of document.querySelectorAll(\"[data-framer-original-sizes]\")){let t=e.getAttribute(\"data-framer-original-sizes\");t===\"\"?e.removeAttribute(\"sizes\"):e.setAttribute(\"sizes\",t),e.removeAttribute(\"data-framer-original-sizes\")}}function a(){window.__framer_onRewriteBreakpoints=i}return a})()() \\n !function(){function c(t,r){let e=r.indexOf(\"#\"),n=e===-1?r:r.substring(0,e),o=e===-1?\"\":r.substring(e),a=n.indexOf(\"?\");if(a===-1)return n+t+o;let d=new URLSearchParams(t),h=n.substring(a+1),s=new URLSearchParams(h);for(let[i,m]of d)s.has(i)||s.append(i,m);return n.substring(0,a+1)+s.toString()+o}var l=\\'div#main a[href^=\"#\"],div#main a[href^=\"/\"],div#main a[href^=\".\"]\\',u=\"div#main a[data-framer-preserve-params]\",f,g=(f=document.currentScript)==null?void 0:f.hasAttribute(\"data-preserve-internal-params\");if(window.location.search&&!/bot|-google|google-|yandex|ia_archiver|crawl|spider/iu.test(navigator.userAgent)){let t=document.querySelectorAll(g?`${l},${u}`:u);for(let r of t){let e=c(window.location.search,r.href);r.setAttribute(\"href\",e)}}\\r\\n}() \\n \\n $ $ $ \\n \\n \\n $ Create a free website with Framer, the website builder loved by startups, designers and agencies. \\n \\n /$ \\n \\n \\n /$ /$ /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  Start of bodyEnd  \\n  End of bodyEnd')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using rag_core.index_builder.load_vectorstore()\n",
      "Loading vectorstore from disk (this may take a moment)...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error in faiss::FileIOReader::FileIOReader(const char *) at /Users/runner/work/faiss-wheels/faiss-wheels/faiss/faiss/impl/io.cpp:68: Error: 'f' failed: could not open /Users/ritamupadhyay/Documents/Ritam_QA/data/vectorstore/career_faiss_index/index.faiss for reading: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m9/hmk3frsn52x_p8njd0pzf5hr0000gn/T/ipykernel_9738/3159940620.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Could not find load_vectorstore(). Edit the import above to point to your index_builder module.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# 1) Load vectorstore (this uses get_embeddings() internally per your index_builder.py)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading vectorstore from disk (this may take a moment)...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mvectorstore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_vectorstore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Vectorstore loaded:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorstore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# 2) Extract all Documents from the vectorstore docstore (works for FAISS/most langchain stores)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Ritam_QA/rag_core/index_builder.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m    291\u001b[0m     \"\"\"\n\u001b[1;32m    292\u001b[0m     \u001b[0mLoad\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mFAISS\u001b[0m \u001b[0mvector\u001b[0m \u001b[0mstore\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdisk\u001b[0m \u001b[0musing\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0membedding\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \"\"\"\n\u001b[1;32m    294\u001b[0m     \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m     vs = FAISS.load_local(\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mVECTORSTORE_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mallow_dangerous_deserialization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/genai/lib/python3.10/site-packages/langchain_community/vectorstores/faiss.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(cls, folder_path, embeddings, index_name, allow_dangerous_deserialization, **kwargs)\u001b[0m\n\u001b[1;32m   1199\u001b[0m             )\n\u001b[1;32m   1200\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[0;31m# load index separately since it is not picklable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m         \u001b[0mfaiss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdependable_faiss_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34mf\"{index_name}.faiss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m         \u001b[0;31m# load docstore and index_to_docstore_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34mf\"{index_name}.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/genai/lib/python3.10/site-packages/faiss/swigfaiss.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m  11321\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 11322\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_swigfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Error in faiss::FileIOReader::FileIOReader(const char *) at /Users/runner/work/faiss-wheels/faiss-wheels/faiss/faiss/impl/io.cpp:68: Error: 'f' failed: could not open /Users/ritamupadhyay/Documents/Ritam_QA/data/vectorstore/career_faiss_index/index.faiss for reading: No such file or directory"
     ]
    }
   ],
   "source": [
    "# === Notebook cell: Load your FAISS vectorstore and inspect retrieved docs ===\n",
    "import importlib\n",
    "import traceback\n",
    "from typing import List, Any, Set, Tuple\n",
    "from pprint import pprint\n",
    "\n",
    "# LangChain objects\n",
    "from langchain.schema import Document, BaseRetriever\n",
    "\n",
    "# Try to import your project's load_vectorstore (index_builder.load_vectorstore)\n",
    "# Adjust the module path if your repo layout differs (you showed rag_core/index_builder.py).\n",
    "load_vectorstore = None\n",
    "try:\n",
    "    mod = importlib.import_module(\"rag_core.index_builder\")\n",
    "    if hasattr(mod, \"load_vectorstore\"):\n",
    "        load_vectorstore = getattr(mod, \"load_vectorstore\")\n",
    "        print(\"Using rag_core.index_builder.load_vectorstore()\")\n",
    "except Exception as e:\n",
    "    print(\"Could not import rag_core.index_builder.load_vectorstore():\", e)\n",
    "    traceback.print_exc()\n",
    "\n",
    "if load_vectorstore is None:\n",
    "    # fallback: try index_builder at repo root\n",
    "    try:\n",
    "        mod = importlib.import_module(\"index_builder\")\n",
    "        if hasattr(mod, \"load_vectorstore\"):\n",
    "            load_vectorstore = getattr(mod, \"load_vectorstore\")\n",
    "            print(\"Using index_builder.load_vectorstore()\")\n",
    "    except Exception as e:\n",
    "        print(\"Fallback import index_builder.load_vectorstore() failed:\", e)\n",
    "\n",
    "if load_vectorstore is None:\n",
    "    raise RuntimeError(\"Could not find load_vectorstore(). Edit the import above to point to your index_builder module.\")\n",
    "\n",
    "# 1) Load vectorstore (this uses get_embeddings() internally per your index_builder.py)\n",
    "print(\"Loading vectorstore from disk (this may take a moment)...\")\n",
    "vectorstore = load_vectorstore()\n",
    "print(\"Vectorstore loaded:\", type(vectorstore))\n",
    "\n",
    "# 2) Extract all Documents from the vectorstore docstore (works for FAISS/most langchain stores)\n",
    "def extract_all_docs_from_vectorstore(vs) -> List[Document]:\n",
    "    # Preferred: docstore._dict\n",
    "    if hasattr(vs, \"docstore\") and hasattr(vs.docstore, \"_dict\"):\n",
    "        docs = list(vs.docstore._dict.values())\n",
    "        return docs\n",
    "    # If not, try similarity_search trick (get top candidates, then dedupe)\n",
    "    try:\n",
    "        cand = vs.similarity_search(\"test\", k=100)\n",
    "        # ensure Document objects\n",
    "        docs = [d for d in cand if isinstance(d, Document)]\n",
    "        if docs:\n",
    "            return docs\n",
    "    except Exception as e:\n",
    "        print(\"similarity_search fallback failed:\", e)\n",
    "    raise RuntimeError(\"Could not enumerate docs from vectorstore. Inspect object manually in the notebook.\")\n",
    "\n",
    "all_docs = extract_all_docs_from_vectorstore(vectorstore)\n",
    "print(f\"Extracted {len(all_docs)} documents from vectorstore\")\n",
    "\n",
    "# 3) Build or import PrefixRetriever (use your local one if present)\n",
    "PrefixRetriever = None\n",
    "try:\n",
    "    pr_mod = importlib.import_module(\"PrefixRetreiver\")\n",
    "    PrefixRetriever = getattr(pr_mod, \"PrefixRetriever\", None)\n",
    "    if PrefixRetriever:\n",
    "        print(\"Imported PrefixRetriever from PrefixRetreiver.py\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "if PrefixRetriever is None:\n",
    "    print(\"Using notebook fallback PrefixRetriever\")\n",
    "    class PrefixRetriever:\n",
    "        def __init__(self, docs: List[Document], k:int=3, max_lines:int=8):\n",
    "            self.docs = docs\n",
    "            self.k = k\n",
    "            self.max_lines = max_lines\n",
    "        def _head(self, content: str) -> str:\n",
    "            lines = [ln for ln in content.splitlines() if ln.strip()]\n",
    "            return \"\\n\".join(lines[: self.max_lines])\n",
    "        def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "            q = (query or \"\").lower().strip()\n",
    "            tokens = [t for t in q.split() if t]\n",
    "            out = []\n",
    "            for d in self.docs:\n",
    "                header = d.metadata.get(\"section_header\") or d.metadata.get(\"section_label\") or \"\"\n",
    "                head_text = (header + \"\\n\" + self._head(d.page_content)).lower()\n",
    "                if q and (q in head_text or all(tok in head_text for tok in tokens)):\n",
    "                    out.append(d)\n",
    "                if len(out) >= self.k:\n",
    "                    break\n",
    "            return out\n",
    "\n",
    "# 4) Build a small Pydantic-friendly fused retriever (works with your vector retriever)\n",
    "# try to get a vector retriever using vectorstore.as_retriever\n",
    "vector_retriever = None\n",
    "try:\n",
    "    vector_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 8})\n",
    "    print(\"Constructed vector_retriever using vectorstore.as_retriever(...)\")\n",
    "except Exception as e:\n",
    "    print(\"vectorstore.as_retriever(...) failed:\", e)\n",
    "\n",
    "# Define FusedRetriever that is accepted by LangChain (Pydantic BaseModel-backed)\n",
    "class FusedRetriever(BaseRetriever):\n",
    "    prefix_retriever: Any\n",
    "    vector_retriever: Any\n",
    "    k: int = 4\n",
    "    prefix_first: bool = True\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "    def __init__(self, prefix_retriever: Any, vector_retriever: Any = None, k: int = 4, prefix_first: bool = True, **kwargs):\n",
    "        super().__init__(prefix_retriever=prefix_retriever, vector_retriever=vector_retriever, k=k, prefix_first=prefix_first, **kwargs)\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        # get prefix candidates\n",
    "        prefix_docs = []\n",
    "        if hasattr(self.prefix_retriever, \"get_relevant_documents\"):\n",
    "            prefix_docs = self.prefix_retriever.get_relevant_documents(query)\n",
    "        elif hasattr(self.prefix_retriever, \"_get_relevant_documents\"):\n",
    "            prefix_docs = self.prefix_retriever._get_relevant_documents(query)\n",
    "        # get vector candidates\n",
    "        vector_docs = []\n",
    "        if self.vector_retriever is not None:\n",
    "            try:\n",
    "                if hasattr(self.vector_retriever, \"get_relevant_documents\"):\n",
    "                    vector_docs = self.vector_retriever.get_relevant_documents(query)\n",
    "                elif hasattr(self.vector_retriever, \"retrieve\"):\n",
    "                    vector_docs = self.vector_retriever.retrieve(query)\n",
    "            except Exception:\n",
    "                vector_docs = []\n",
    "        # fuse + dedupe\n",
    "        seen = set()\n",
    "        out = []\n",
    "        def add_docs(docs):\n",
    "            for d in docs:\n",
    "                key = (d.metadata.get(\"source\"), d.page_content[:200])\n",
    "                if key in seen:\n",
    "                    continue\n",
    "                seen.add(key)\n",
    "                out.append(d)\n",
    "                if len(out) >= self.k:\n",
    "                    return True\n",
    "            return False\n",
    "        if self.prefix_first:\n",
    "            finished = add_docs(prefix_docs)\n",
    "            if not finished:\n",
    "                add_docs(vector_docs)\n",
    "        else:\n",
    "            finished = add_docs(vector_docs)\n",
    "            if not finished:\n",
    "                add_docs(prefix_docs)\n",
    "        return out[: self.k]\n",
    "\n",
    "# instantiate retrievers\n",
    "prefix_retriever = PrefixRetriever(docs=all_docs, k=6, max_lines=8)\n",
    "fused_retriever = FusedRetriever(prefix_retriever=prefix_retriever, vector_retriever=vector_retriever, k=6, prefix_first=True)\n",
    "\n",
    "# Helper to pretty-print retrieved docs\n",
    "def inspect_docs(docs: List[Document], title: str, preview_chars: int = 400):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(title)\n",
    "    print(\"=\"*80)\n",
    "    for i, d in enumerate(docs, 1):\n",
    "        print(f\"\\n--- DOC {i} ---\")\n",
    "        print(\"HEADER       :\", d.metadata.get(\"section_header\"))\n",
    "        print(\"SECTION LABEL:\", d.metadata.get(\"section_label\"))\n",
    "        print(\"SOURCE       :\", d.metadata.get(\"source\"))\n",
    "        print(\"CONTENT LEN  :\", len(d.page_content))\n",
    "        print(\"CONTENT PREV :\")\n",
    "        print(d.page_content[:preview_chars].strip())\n",
    "        print(\"...\")\n",
    "\n",
    "# 5) Run a few queries and inspect results\n",
    "queries = [\"tell me about your work experience\"]\n",
    "\n",
    "for q in queries:\n",
    "    print(\"\\n\\n\" + \"#\"*20 + f\" QUERY: {q}\" + \"#\"*20)\n",
    "    try:\n",
    "        pdocs = prefix_retriever.get_relevant_documents(q)\n",
    "        inspect_docs(pdocs, \"PREFIX RETRIEVER RESULTS\")\n",
    "    except Exception as e:\n",
    "        print(\"Prefix retriever error:\", e, traceback.format_exc())\n",
    "    try:\n",
    "        vdocs = []\n",
    "        if vector_retriever is not None:\n",
    "            vdocs = vector_retriever.get_relevant_documents(q)\n",
    "            inspect_docs(vdocs, \"VECTOR RETRIEVER RESULTS\")\n",
    "        else:\n",
    "            print(\"Vector retriever not available (vectorstore.as_retriever failed earlier).\")\n",
    "    except Exception as e:\n",
    "        print(\"Vector retriever error:\", e, traceback.format_exc())\n",
    "    try:\n",
    "        fdocs = fused_retriever.get_relevant_documents(q)\n",
    "        inspect_docs(fdocs, \"FUSED RETRIEVER RESULTS\")\n",
    "    except Exception as e:\n",
    "        print(\"Fused retriever error:\", e, traceback.format_exc())\n",
    "\n",
    "print(\"\\nDone — inspect the HEADER and CONTENT LEN in FUSED RETRIEVER RESULTS for 'work experience'. If it looks like fragments (short content), we'll adjust chunking / merging next.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 69 docs from FAISS vectorstore\n",
      "Found 34 candidate labels (examples): ['Start of bodyStart', 'RAG-PDF QA Method -', 'Project Overview', 'Key Highlights', 'Rag Pdf Qa Method', 'Account Statement Verification -', 'Account Statement Verification', 'Python', 'Other stack', \"Let's Transform Ideas into Reality\", 'My Latest Works', 'Stack', 'Streamlit', 'Next Word - Next Word Prediction', 'Next Word', 'About', 'Contact', 'Education', 'Work experience', 'Projects']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "QUERY: where did you work and what did you do at Juniper?\n",
      "Mapped labels (label,score): [('Work experience', 0.9035468697547913), ('Low Cost Vision Based Gripper', 0.4155164361000061), ('Project Overview', 0.29346197843551636), ('Low Cost Vision-Based Gripper - Camera based gripper to locate, localize and grasp object', 0.24844245612621307), ('Key Highlights', 0.24241366982460022)]\n",
      "Found 19 docs matching labels\n",
      "\n",
      "--- FETCHED DOC 1 ---\n",
      "HEADER: Project Overview\n",
      "LABEL: None\n",
      "SOURCE: https://fearless-writers-028990.framer.app/project/rag-pdf-qa-method\n",
      "CONTENT LEN: 372\n",
      "PREVIEW: Project Overview \n",
      " \n",
      " This project aimed to build an interface so that user could provide a PDF and ask specific questions to the LLM based on the pdf content. The LLM also indicated the paragraphs and texts that led to the answer that it provided. This reduced the chances of hallucinations by the model. \n",
      " $ https://github.com/ritam3/RAG-PDF-HuggingFace-Groq/tree/main /$\n",
      "\n",
      "--- FETCHED DOC 2 ---\n",
      "HEADER: Key Highlights\n",
      "LABEL: None\n",
      "SOURCE: https://fearless-writers-028990.framer.app/project/rag-pdf-qa-method\n",
      "CONTENT LEN: 548\n",
      "PREVIEW: Key Highlights \n",
      " \n",
      " Parsing of PDF Document: Supports parsing and preprocessing PDF documents for QA. \n",
      " HuggingFace Embeddings: Used HuggingFace models to generate embeddings for document text. \n",
      " Groq API Integration: Used Groq API for embedding retrieval and similarity search. \n",
      " Session Management: Maintains session state so redundant calculations of embeddings can be avoided; hence, it will be more efficient. \n",
      " Interactive QA: It allows users to ask queries after uploading documents. It retrieves relevant information from the processed PDFs.\n",
      "\n",
      "--- FETCHED DOC 3 ---\n",
      "HEADER: Project Overview\n",
      "LABEL: None\n",
      "SOURCE: https://fearless-writers-028990.framer.app/project/account-statement-verification\n",
      "CONTENT LEN: 312\n",
      "PREVIEW: Project Overview \n",
      " Users had to upload their account statements as bank proof or income proof to open a trading account with Paytm Money. We devised various methods using OCR and text extraction to find parameters like account number, ifsc-code, name, duration, closing balance, and so on to verify the document.\n",
      "\n",
      "--- FETCHED DOC 4 ---\n",
      "HEADER: Key Highlights\n",
      "LABEL: None\n",
      "SOURCE: https://fearless-writers-028990.framer.app/project/account-statement-verification\n",
      "CONTENT LEN: 576\n",
      "PREVIEW: Key Highlights \n",
      " \n",
      " Analyzed account statement PDFs and patterns to extract account number, IFSC, date, and name to match it with the details confirmed by the user. \n",
      " Data mining methods were used to extract fields crucial to validate the statements uploaded by users as bank and income proofs. \n",
      " Using data mining methods, I spear headed the automation of verification for the bank proof upload at 40%. \n",
      " Trained a CNN model using Python and Tensorflow to identify account statements based on visual characteristics. \n",
      " Used Python and FastAPI to create RESTful API for checks.\n",
      "\n",
      "--- FETCHED DOC 5 ---\n",
      "HEADER: Project Overview\n",
      "LABEL: None\n",
      "SOURCE: https://fearless-writers-028990.framer.app/project/next-word\n",
      "CONTENT LEN: 449\n",
      "PREVIEW: Project Overview \n",
      " \n",
      " The prediction of the next word, using LSTM, is to guess the next word in a sentence. LSTM is a particular kind of neural network that remembers word context. This makes it very good for applications like text suggestions, chatbots, or writing assistants. The model predicts the next word by analyzing the words provided so far; this helps make typing faster and interactions smoother. \n",
      " $ https://github.com/ritam3/Next_Word /$\n",
      "\n",
      "--- FETCHED DOC 6 ---\n",
      "HEADER: Key Highlights\n",
      "LABEL: None\n",
      "SOURCE: https://fearless-writers-028990.framer.app/project/next-word\n",
      "CONTENT LEN: 271\n",
      "PREVIEW: Key Highlights \n",
      " \n",
      " Used the \"Adventures of Sherlock Holmes\" dataset to train the LSTM model. \n",
      " The data was cleaned and tokenized and the final data was generated using the n-gram method. \n",
      " The model was built using TensorFlow. \n",
      " User Interface was built using Streamlit.\n",
      "\n",
      "--- FETCHED DOC 7 ---\n",
      "HEADER: Work experience\n",
      "LABEL: None\n",
      "SOURCE: https://fearless-writers-028990.framer.app/\n",
      "CONTENT LEN: 3671\n",
      "PREVIEW: Work experience \n",
      " \n",
      " $ \n",
      " June 2025 - August 2025 \n",
      " \n",
      " $ \n",
      " $ \n",
      " Product Management Intern : Data Science \n",
      " \n",
      " \n",
      " /$ \n",
      " \n",
      " \n",
      " $ \n",
      " Product Management Intern : Data Science \n",
      " \n",
      " \n",
      " /$ \n",
      " \n",
      " \n",
      " $ \n",
      " Product Management Intern : Data Science \n",
      " \n",
      " \n",
      " /$ \n",
      " \n",
      " /$ \n",
      " Juniper Networks / Mist \n",
      " â Engineered an end-to-end classification pipeline that processed over 13,000 customer support tickets, achieving a 100% automated classification rate and providing proactive insights into emerging issues.\n",
      "â Developed a multi-stage data processing module using LLMs to automatically extract and normalize critical information, incl\n",
      "\n",
      "--- FETCHED DOC 8 ---\n",
      "HEADER: Project Overview\n",
      "LABEL: None\n",
      "SOURCE: https://fearless-writers-028990.framer.app/project/churn-prediction-and-customer-cohort\n",
      "CONTENT LEN: 279\n",
      "PREVIEW: Project Overview \n",
      " This project pinpointed those users on the Paytm Money platform that were active the previous month but were predicted to churn in the upcoming month. Insights helped marketing and growth teams proactively target these users and implement retention strategies.\n",
      "\n",
      "--- FETCHED DOC 9 ---\n",
      "HEADER: Key Highlights\n",
      "LABEL: None\n",
      "SOURCE: https://fearless-writers-028990.framer.app/project/churn-prediction-and-customer-cohort\n",
      "CONTENT LEN: 528\n",
      "PREVIEW: Key Highlights \n",
      " \n",
      " To identify the users who traded in the platform the previous month but are likely to churn for the current month to assist the marketing and growth team targets the users and plans. \n",
      " Used Python to implement clustering techniques like KNN and DB-SCAN to create cohorts of users that helped to segment the user based on trading habits. \n",
      " ML methods like Random Forests, Gradient Boosting, and Deep Neural Networks were used to predict and identify the churned users with a Recall of 88% and Precision of 70%.\n",
      "\n",
      "--- FETCHED DOC 10 ---\n",
      "HEADER: Project Overview\n",
      "LABEL: None\n",
      "SOURCE: https://fearless-writers-028990.framer.app/project/llm-based-q-a-on-web-context\n",
      "CONTENT LEN: 518\n",
      "PREVIEW: Project Overview \n",
      " \n",
      " This project aimed to build an interface so that user could provide an URL and ask specific questions to the LLM based on that webpage where the URL leads. The LLM also indicated the paragraphs and texts that led to the answer that it provided. This reduced the chances of hallucinations by the model. The user could also see the answer the model gave based on its own knowledge and could choose the one that suited him the most. \n",
      " $ https://github.com/ritam3/LLM-Based-Q-A-based-on-Web-Context /$\n",
      "\n",
      "--- FETCHED DOC 11 ---\n",
      "HEADER: Key Highlights\n",
      "LABEL: None\n",
      "SOURCE: https://fearless-writers-028990.framer.app/project/llm-based-q-a-on-web-context\n",
      "CONTENT LEN: 412\n",
      "PREVIEW: Key Highlights \n",
      " \n",
      " Used Langchain to work with LLMs and loading web based data, splitting text and converting them into documents. \n",
      " FAISS to store the encodings. \n",
      " Worked with OLLAMA to include models like LLAMA 3.2 and Gemma:2b. \n",
      " Used Streamlit to build the User Interface. \n",
      " User was given the ability to ask a question and the chosen LLM model returned the answer based on web context and its own knowledge.\n",
      "\n",
      "--- FETCHED DOC 12 ---\n",
      "HEADER: Project Overview\n",
      "LABEL: None\n",
      "SOURCE: https://fearless-writers-028990.framer.app/project/improper-face-detection-at-frontend\n",
      "CONTENT LEN: 310\n",
      "PREVIEW: Project Overview \n",
      " The users had to upload a selfie to open a trading account with Paytm Money. That photo had to match their ID uploaded earlier. The main job was to prompt users that the selfie clicked did not match the requirements and that  the user should upload a new image to reduce his onboarding time.\n",
      "\n",
      "--- FETCHED DOC 13 ---\n",
      "HEADER: Key Highlights\n",
      "LABEL: None\n",
      "SOURCE: https://fearless-writers-028990.framer.app/project/improper-face-detection-at-frontend\n",
      "CONTENT LEN: 458\n",
      "PREVIEW: Key Highlights \n",
      " \n",
      " To alert the users who upload improper photos of their faces during the onboarding process with mentioned guidelines regarding the quality of the photo such as brightness, contrast, face too close to boundaries, face is spoofed, user not wearing a proper shirt, etc. \n",
      " Used ML models and image processing techniques for filters, reducing the photo rejection at the backend by 5%. \n",
      " Used Python and FastAPI to create RESTful API for checks.\n",
      "\n",
      "--- FETCHED DOC 14 ---\n",
      "HEADER: Project Overview\n",
      "LABEL: None\n",
      "SOURCE: https://fearless-writers-028990.framer.app/project/ai-based-name-matcher\n",
      "CONTENT LEN: 299\n",
      "PREVIEW: Project Overview \n",
      " Fin-tech industries use a method called penny-drop to verify bank account numbers mentioned by users by crediting Re 1 to the account number and obtaining the name associated with the account number. This helps to find if the account number that the user mentioned belongs to him.\n",
      "\n",
      "--- FETCHED DOC 15 ---\n",
      "HEADER: Key Highlights\n",
      "LABEL: None\n",
      "SOURCE: https://fearless-writers-028990.framer.app/project/ai-based-name-matcher\n",
      "CONTENT LEN: 483\n",
      "PREVIEW: Key Highlights \n",
      " \n",
      " The account holder name obtained by crediting Re 1 is matched with the name mentioned in the PAN of the user to confirm if the bank account belongs to the user. \n",
      " AI-based Name Matcher uses the Random Forest model to calculate the similarity based on both phonetic and distance specially trained on Indian names. \n",
      " Used Python to preprocess the data and train the ML model. \n",
      " The acceptance went from 70% to 80% compared to the algorithmic name-matcher used prior.\n",
      "\n",
      "--- FETCHED DOC 16 ---\n",
      "HEADER: Low Cost Vision-Based Gripper - Camera based gripper to locate, localize and grasp object\n",
      "LABEL: None\n",
      "SOURCE: https://fearless-writers-028990.framer.app/project/low-cost-vision-based-gripper\n",
      "CONTENT LEN: 218\n",
      "PREVIEW: Low Cost Vision-Based Gripper - Camera based gripper to locate, localize and grasp object \n",
      " \n",
      " \n",
      " Date \n",
      " July 2021 \n",
      " \n",
      " \n",
      " Service \n",
      " \n",
      " \n",
      " \n",
      " Employer \n",
      " Centre of Excellence in Advanced Manufacturing Technology, IIT Kharagpur\n",
      "\n",
      "--- FETCHED DOC 17 ---\n",
      "HEADER: Project Overview\n",
      "LABEL: None\n",
      "SOURCE: https://fearless-writers-028990.framer.app/project/low-cost-vision-based-gripper\n",
      "CONTENT LEN: 439\n",
      "PREVIEW: Project Overview \n",
      " \n",
      " This project focused on automating object detection and grasping for a KUKA robot in an assembly line. The objective was to identify whether an object was cylindrical or cuboid to select the appropriate gripper type (three-fingered for cylindrical objects, parallel plate for cuboids). YOLOv4 was used to identify the cuboid and cylindrical objects. \n",
      " Watch the video : $ https://www.youtube.com/watch?v=g1MSxzrRrLM /$\n",
      "\n",
      "--- FETCHED DOC 18 ---\n",
      "HEADER: Key Highlights\n",
      "LABEL: None\n",
      "SOURCE: https://fearless-writers-028990.framer.app/project/low-cost-vision-based-gripper\n",
      "CONTENT LEN: 835\n",
      "PREVIEW: Key Highlights \n",
      " \n",
      " To pick up an object, it is essential to find whether the object is cylindrical or cuboid so that the appropriate gripper is used for grasping. The three-fingered gripper is generally used for cylindrical surfaces whereas parallel plate grippers are used for cuboids. \n",
      " YOLOv4 was used for object detection and to differentiate between cylindrical and cuboid. \n",
      " In addition, the vertices of a cuboid were found using segmentation of the object and it was used to estimate pose of the cuboid. OpenCV was used to perform segmentation and automate the KUKA robot. \n",
      " The camera mounted\n",
      "\n",
      "--- FETCHED DOC 19 ---\n",
      "HEADER: Low Cost Vision Based Gripper\n",
      "LABEL: Low Cost Vision Based Gripper\n",
      "SOURCE: https://fearless-writers-028990.framer.app/project/low-cost-vision-based-gripper\n",
      "CONTENT LEN: 5380\n",
      "PREVIEW: More Project \n",
      " \n",
      " $ $ \n",
      " $ \n",
      " \n",
      " \n",
      " RAG-PDF QA Method \n",
      " Implementation of RAG method to perform question answering on PDF documents. HuggingFace embeddings in addition to the Groq API were used for efficient and accurate information retrieval. Session management was done to ensure embeddings are computed once per document upload to optimize performance by avoiding redundant computations.  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " January 2025 \n",
      " \n",
      " /$ \n",
      " \n",
      " \n",
      " $ \n",
      " \n",
      " \n",
      " RAG-PDF QA Method \n",
      " Implementation of RAG method to perform question answering on PDF documents. HuggingFace embeddings in addition to the Groq API were used for efficie\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# === Notebook cell: Map query -> section labels with BART zero-shot, then fetch docs ===\n",
    "import importlib\n",
    "from typing import List, Dict, Tuple\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "# HF pipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "# LangChain Document\n",
    "from langchain.schema import Document\n",
    "\n",
    "# ---- helper: load your docs / vectorstore (adapt if needed) ----\n",
    "# Try to import your loader; adjust module path if different\n",
    "try:\n",
    "    idx_mod = importlib.import_module(\"rag_core.index_builder\")\n",
    "    load_vectorstore = getattr(idx_mod, \"load_vectorstore\")\n",
    "    vs = load_vectorstore()\n",
    "    all_docs = list(vs.docstore._dict.values())\n",
    "    vector_retriever = vs.as_retriever(search_kwargs={\"k\": 6})\n",
    "    print(f\"Loaded {len(all_docs)} docs from FAISS vectorstore\")\n",
    "except Exception as e:\n",
    "    print(\"Could not load vectorstore via rag_core.index_builder.load_vectorstore(); try loading serialized docs instead:\", e)\n",
    "    # fallback: try a docs pickle/json in data/ (you can plug your loader here)\n",
    "    raise\n",
    "\n",
    "# ---- 1) build a label vocabulary from indexed docs ----\n",
    "def build_label_vocab(docs: List[Document]) -> List[str]:\n",
    "    labels = []\n",
    "    seen = set()\n",
    "    for d in docs:\n",
    "        header = (d.metadata.get(\"section_header\") or \"\").strip()\n",
    "        s_label = (d.metadata.get(\"section_label\") or \"\").strip()\n",
    "        # prefer a short header label if available\n",
    "        candidates = [header, s_label]\n",
    "        for c in candidates:\n",
    "            if not c:\n",
    "                continue\n",
    "            # Normalization: remove newlines, collapse whitespace, limit length\n",
    "            normalized = \" \".join(c.split())\n",
    "            if len(normalized) > 120:\n",
    "                normalized = normalized[:120] + \"...\"\n",
    "            if normalized not in seen:\n",
    "                seen.add(normalized)\n",
    "                labels.append(normalized)\n",
    "    return labels\n",
    "\n",
    "labels = build_label_vocab(all_docs)\n",
    "print(f\"Found {len(labels)} candidate labels (examples):\", labels[:20])\n",
    "\n",
    "# ---- 2) zero-shot classifier using BART-MNLI ----\n",
    "# This uses HuggingFace pipeline (\"zero-shot-classification\") with a NLI model.\n",
    "# It scores each label as how well the query fits that label.\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "def map_query_to_labels_zero_shot(query: str, candidate_labels: List[str], top_k:int=5, score_threshold:float=0.40) -> List[Tuple[str,float]]:\n",
    "    \"\"\"\n",
    "    Returns a list of (label, score) pairs selected for this query.\n",
    "    - top_k: number of highest scoring labels to consider\n",
    "    - score_threshold: min probability for accepting a label\n",
    "    \"\"\"\n",
    "    if not candidate_labels:\n",
    "        return []\n",
    "\n",
    "    # The pipeline accepts a list; we pass all labels and request top_k\n",
    "    out = classifier(query, candidate_labels, multi_label=True)\n",
    "    # out structure: {'sequence':..., 'labels':[...], 'scores':[...]}\n",
    "    labels_out = out[\"labels\"]\n",
    "    scores_out = out[\"scores\"]\n",
    "    selected = []\n",
    "    for lbl, score in zip(labels_out[:top_k], scores_out[:top_k]):\n",
    "        if score >= score_threshold:\n",
    "            selected.append((lbl, float(score)))\n",
    "    # If nothing passed threshold, return the top label (best guess) to avoid empty result\n",
    "    if not selected and labels_out:\n",
    "        selected = [(labels_out[0], float(scores_out[0]))]\n",
    "    return selected\n",
    "\n",
    "# ---- 3) fetch docs that match chosen labels ----\n",
    "def fetch_docs_by_labels(selected_labels: List[str], docs: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Return all docs whose section_header or section_label matches any selected_label.\n",
    "    Matching is fuzzy: case-insensitive substring match against metadata fields.\n",
    "    \"\"\"\n",
    "    if not selected_labels:\n",
    "        return []\n",
    "    out = []\n",
    "    sel_norm = [s.lower() for s in selected_labels]\n",
    "    for d in docs:\n",
    "        header = (d.metadata.get(\"section_header\") or \"\").lower()\n",
    "        s_label = (d.metadata.get(\"section_label\") or \"\").lower()\n",
    "        combined = header + \" \" + s_label\n",
    "        for s in sel_norm:\n",
    "            if not s:\n",
    "                continue\n",
    "            if s in combined:\n",
    "                out.append(d)\n",
    "                break\n",
    "    return out\n",
    "\n",
    "# ---- 4) end-to-end helper: map query -> docs (with vector fallback) ----\n",
    "def retrieve_by_label_mapping(query: str, docs: List[Document], classifier_pipeline=None, top_k_labels=5, score_threshold=0.40, vector_retriever=None):\n",
    "    if classifier_pipeline is None:\n",
    "        # use the prebuilt classifier if not passed\n",
    "        classifier_pipeline = classifier\n",
    "\n",
    "    candidate_labels = build_label_vocab(docs)\n",
    "    mapped = map_query_to_labels_zero_shot(query, candidate_labels, top_k=top_k_labels, score_threshold=score_threshold)\n",
    "    print(\"Mapped labels (label,score):\", mapped)\n",
    "\n",
    "    chosen_labels = [lbl for lbl, _ in mapped]\n",
    "    fetched = fetch_docs_by_labels(chosen_labels, docs)\n",
    "    print(f\"Found {len(fetched)} docs matching labels\")\n",
    "\n",
    "    if not fetched and vector_retriever is not None:\n",
    "        # fallback: return semantic vector search results\n",
    "        print(\"Label mapping returned nothing; falling back to vector retriever.\")\n",
    "        fetched = vector_retriever.get_relevant_documents(query)\n",
    "    return mapped, fetched\n",
    "\n",
    "# ---- 5) try it ----\n",
    "queries = [\n",
    "    #\"show me work experience\",\n",
    "    \"where did you work and what did you do at Juniper?\",\n",
    "    #\"education details\",\n",
    "    #\"list publications\",\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"QUERY:\", q)\n",
    "    mapped, docs_fetched = retrieve_by_label_mapping(q, all_docs, classifier_pipeline=classifier, top_k_labels=5, score_threshold=0, vector_retriever=vector_retriever)\n",
    "    for i, d in enumerate(docs_fetched, 1):\n",
    "        print(f\"\\n--- FETCHED DOC {i} ---\")\n",
    "        print(\"HEADER:\", d.metadata.get(\"section_header\"))\n",
    "        print(\"LABEL:\", d.metadata.get(\"section_label\"))\n",
    "        print(\"SOURCE:\", d.metadata.get(\"source\"))\n",
    "        print(\"CONTENT LEN:\", len(d.page_content))\n",
    "        print(\"PREVIEW:\", d.page_content[:600].strip()[:600])\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# ---- end cell ----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 57 docs from FAISS vectorstore\n",
      "Found 29 candidate labels (examples): ['Start of bodyStart', 'Account Statement Verification -', 'Project Overview', 'Key Highlights', 'Account Statement Verification', 'About', 'Contact', 'Education', 'Work experience', 'Projects', 'Stack', 'Certifications', 'Research Papers', 'RAG-PDF QA Method -', 'Rag Pdf Qa Method', 'AI based Name Matcher - Matching 2 Indian Names to match the similarity', 'Ai Based Name Matcher', 'My Latest Works', 'Improper Face Detection at Frontend - Finding faces that do not meet requirements', 'Improper Face Detection At Frontend']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "QUERY: DO you have experience with NLP if so then describe where you have used it\n",
      "Mapped labels (label,score): [('About', 0.5211944580078125), ('Contact', 0.48030656576156616), ('Next Word - Next Word Prediction', 0.35503268241882324)]\n",
      "Found 3 docs matching labels\n",
      "\n",
      "--- FETCHED DOC 1 ---\n",
      "SCORE: 0.5211944580078125\n",
      "MATCHED LABELS: ['about']\n",
      "HEADER: About\n",
      "LABEL: None\n",
      "SOURCE: https://fearless-writers-028990.framer.app/\n",
      "CONTENT LEN: 459\n",
      "PREVIEW: About \n",
      " Welcome to my portfolio! I am an ASU Master's student ('26) with experience at Juniper Networks and Paytm Money. I leverage ML/AI to solve complex business challenges, specializing in NLP, computer vision, and predictive analytics. Seeking roles in Data Science, AI/ML, and NLP to drive impact at scale. \n",
      " \n",
      " \n",
      " $ \n",
      " $ My projects \n",
      " /$ \n",
      " \n",
      " \n",
      " $ My projects \n",
      " /$ \n",
      " \n",
      " \n",
      " $ My projects \n",
      " /$ \n",
      " \n",
      " /$ $ \n",
      " $ Contact /$ \n",
      " \n",
      " \n",
      " $ Contact /$ \n",
      " \n",
      " \n",
      " $ Contact /$ \n",
      " \n",
      " /$\n",
      "\n",
      "--- FETCHED DOC 2 ---\n",
      "SCORE: 0.48030656576156616\n",
      "MATCHED LABELS: ['contact']\n",
      "HEADER: Contact\n",
      "LABEL: None\n",
      "SOURCE: https://fearless-writers-028990.framer.app/\n",
      "CONTENT LEN: 333\n",
      "PREVIEW: Contact \n",
      " \n",
      " \n",
      " Linkedin \n",
      " \n",
      " $ \n",
      " ritam.upadhyay \n",
      " \n",
      " \n",
      " /$ \n",
      " \n",
      " \n",
      " $ \n",
      " ritam.upadhyay \n",
      " \n",
      " \n",
      " /$ \n",
      " \n",
      " \n",
      " $ \n",
      " ritam.upadhyay \n",
      " \n",
      " \n",
      " /$ \n",
      " \n",
      " \n",
      " \n",
      " Github \n",
      " \n",
      " $ \n",
      " ritam3 \n",
      " \n",
      " \n",
      " /$ \n",
      " \n",
      " \n",
      " $ \n",
      " ritam3 \n",
      " \n",
      " \n",
      " /$ \n",
      " \n",
      " \n",
      " $ \n",
      " ritam3 \n",
      " \n",
      " \n",
      " /$ \n",
      " \n",
      " \n",
      " \n",
      " Email \n",
      " \n",
      " $ \n",
      " rupadh17@asu.edu \n",
      " \n",
      " \n",
      " /$ \n",
      " \n",
      " \n",
      " $ \n",
      " rupadh17@asu.edu \n",
      " \n",
      " \n",
      " /$ \n",
      " \n",
      " \n",
      " $ \n",
      " rupadh17@asu.edu \n",
      " \n",
      " \n",
      " /$\n",
      "\n",
      "--- FETCHED DOC 3 ---\n",
      "SCORE: 0.35503268241882324\n",
      "MATCHED LABELS: ['next word - next word prediction']\n",
      "HEADER: Next Word - Next Word Prediction\n",
      "LABEL: None\n",
      "SOURCE: https://fearless-writers-028990.framer.app/project/next-word\n",
      "CONTENT LEN: 90\n",
      "PREVIEW: Next Word - Next Word Prediction \n",
      " \n",
      " \n",
      " Date \n",
      " December 2024 \n",
      " \n",
      " \n",
      " Service \n",
      " \n",
      " \n",
      " \n",
      " Employer\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# === Notebook cell: Map query -> section labels with BART zero-shot, then fetch docs ===\n",
    "import importlib\n",
    "from typing import List, Dict, Tuple\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "# HF pipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "# LangChain Document\n",
    "from langchain.schema import Document\n",
    "\n",
    "# ---- helper: load your docs / vectorstore (adapt if needed) ----\n",
    "# Try to import your loader; adjust module path if different\n",
    "try:\n",
    "    idx_mod = importlib.import_module(\"rag_core.index_builder\")\n",
    "    load_vectorstore = getattr(idx_mod, \"load_vectorstore\")\n",
    "    vs = load_vectorstore()\n",
    "    all_docs = list(vs.docstore._dict.values())\n",
    "    vector_retriever = vs.as_retriever(search_kwargs={\"k\": 6})\n",
    "    print(f\"Loaded {len(all_docs)} docs from FAISS vectorstore\")\n",
    "except Exception as e:\n",
    "    print(\"Could not load vectorstore via rag_core.index_builder.load_vectorstore(); try loading serialized docs instead:\", e)\n",
    "    # fallback: try a docs pickle/json in data/ (you can plug your loader here)\n",
    "    raise\n",
    "\n",
    "# ---- 1) build a label vocabulary from indexed docs ----\n",
    "def build_label_vocab(docs: List[Document]) -> List[str]:\n",
    "    labels = []\n",
    "    seen = set()\n",
    "    for d in docs:\n",
    "        header = (d.metadata.get(\"section_header\") or \"\").strip()\n",
    "        s_label = (d.metadata.get(\"section_label\") or \"\").strip()\n",
    "        # prefer a short header label if available\n",
    "        candidates = [header, s_label]\n",
    "        for c in candidates:\n",
    "            if not c:\n",
    "                continue\n",
    "            # Normalization: remove newlines, collapse whitespace, limit length\n",
    "            normalized = \" \".join(c.split())\n",
    "            if len(normalized) > 120:\n",
    "                normalized = normalized[:120] + \"...\"\n",
    "            if normalized not in seen:\n",
    "                seen.add(normalized)\n",
    "                labels.append(normalized)\n",
    "    return labels\n",
    "\n",
    "labels = build_label_vocab(all_docs)\n",
    "print(f\"Found {len(labels)} candidate labels (examples):\", labels[:20])\n",
    "\n",
    "# ---- 2) zero-shot classifier using BART-MNLI ----\n",
    "# This uses HuggingFace pipeline (\"zero-shot-classification\") with a NLI model.\n",
    "# It scores each label as how well the query fits that label.\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "def map_query_to_labels_zero_shot(query: str, candidate_labels: List[str], top_k:int=5, score_threshold:float=0.40) -> List[Tuple[str,float]]:\n",
    "    \"\"\"\n",
    "    Returns a list of (label, score) pairs selected for this query.\n",
    "    - top_k: number of highest scoring labels to consider\n",
    "    - score_threshold: min probability for accepting a label\n",
    "    \"\"\"\n",
    "    if not candidate_labels:\n",
    "        return []\n",
    "\n",
    "    # The pipeline accepts a list; we pass all labels and request top_k\n",
    "    out = classifier(query, candidate_labels, multi_label=True)\n",
    "    # out structure: {'sequence':..., 'labels':[...], 'scores':[...]}\n",
    "    labels_out = out[\"labels\"]\n",
    "    scores_out = out[\"scores\"]\n",
    "    selected = []\n",
    "    for lbl, score in zip(labels_out[:top_k], scores_out[:top_k]):\n",
    "        if score >= score_threshold:\n",
    "            selected.append((lbl, float(score)))\n",
    "    # If nothing passed threshold, return the top label (best guess) to avoid empty result\n",
    "    if not selected and labels_out:\n",
    "        selected = [(labels_out[0], float(scores_out[0]))]\n",
    "    return selected\n",
    "\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "def fetch_docs_by_labels_with_scores(selected_labels: List[Tuple[str, float]], docs: List[Document]) -> List[Tuple[Document, float, List[str]]]:\n",
    "    \"\"\"\n",
    "    For each doc, determine which of the selected_labels it matches (substring match\n",
    "    against section_header or section_label). Return list of tuples:\n",
    "      (Document, score, matched_label_list)\n",
    "    where score is the max label score among matched labels.\n",
    "    \"\"\"\n",
    "    if not selected_labels:\n",
    "        return []\n",
    "\n",
    "    # map label text -> score\n",
    "    label_score: Dict[str, float] = {lbl.lower(): sc for lbl, sc in selected_labels}\n",
    "\n",
    "    out: List[Tuple[Document, float, List[str]]] = []\n",
    "    for d in docs:\n",
    "        header = (d.metadata.get(\"section_header\") or \"\").lower()\n",
    "        s_label = (d.metadata.get(\"section_label\") or \"\").lower()\n",
    "        combined = header + \" \" + s_label\n",
    "\n",
    "        matched_labels = []\n",
    "        matched_scores = []\n",
    "        for lbl_lower, sc in label_score.items():\n",
    "            if lbl_lower and lbl_lower in combined:\n",
    "                matched_labels.append(lbl_lower)\n",
    "                matched_scores.append(sc)\n",
    "\n",
    "        if matched_labels:\n",
    "            # score for this doc is the max score among matched labels\n",
    "            doc_score = max(matched_scores)\n",
    "            out.append((d, doc_score, matched_labels))\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def retrieve_by_label_mapping_ranked(query: str, docs: List[Document], classifier_pipeline=None, top_k_labels=5, score_threshold=0.40, vector_retriever=None):\n",
    "    \"\"\"\n",
    "    Map query -> labels (with scores), fetch docs that match those labels, and\n",
    "    return (mapped_labels, ranked_docs) where ranked_docs is a list of tuples:\n",
    "      (Document, score, matched_labels)\n",
    "    sorted by score desc.\n",
    "    Falls back to vector retriever if no docs found via labels.\n",
    "    \"\"\"\n",
    "    if classifier_pipeline is None:\n",
    "        classifier_pipeline = classifier\n",
    "\n",
    "    candidate_labels = build_label_vocab(docs)\n",
    "    mapped = map_query_to_labels_zero_shot(query, candidate_labels, top_k=top_k_labels, score_threshold=score_threshold)\n",
    "    print(\"Mapped labels (label,score):\", mapped)\n",
    "\n",
    "    # fetch docs with associated label scores\n",
    "    docs_with_scores = fetch_docs_by_labels_with_scores(mapped, docs)\n",
    "    print(f\"Found {len(docs_with_scores)} docs matching labels\")\n",
    "\n",
    "    if not docs_with_scores and vector_retriever is not None:\n",
    "        # fallback: return semantic vector search results (assign them a default score based on similarity rank)\n",
    "        print(\"Label mapping returned nothing; falling back to vector retriever.\")\n",
    "        vec_docs = vector_retriever.get_relevant_documents(query)\n",
    "        # assign decreasing confidence scores starting from 0.6 downwards to indicate fallback\n",
    "        ranked = []\n",
    "        base = 0.6\n",
    "        step = 0.02\n",
    "        for i, d in enumerate(vec_docs):\n",
    "            score = max(0.0, base - i * step)\n",
    "            ranked.append((d, score, [\"vector_fallback\"]))\n",
    "        return mapped, ranked\n",
    "\n",
    "    # If a doc matched multiple labels, we already collapsed to max score in fetch helper.\n",
    "    # Now sort descending by score, and dedupe by (source + content[:200]) keeping highest-scored occurrence.\n",
    "    seen_keys = set()\n",
    "    deduped_sorted = []\n",
    "    for d, sc, matched in sorted(docs_with_scores, key=lambda x: x[1], reverse=True):\n",
    "        key = (d.metadata.get(\"source\"), d.page_content[:200])\n",
    "        if key in seen_keys:\n",
    "            continue\n",
    "        seen_keys.add(key)\n",
    "        deduped_sorted.append((d, sc, matched))\n",
    "\n",
    "    return mapped, deduped_sorted\n",
    " \n",
    "\n",
    "# ---- 5) try it ----\n",
    "queries = [\n",
    "    # \"Tell me about your work in python and LLMs\", \n",
    "    # \"What are the companies you have worked at ?\",\n",
    "    # \"Tell me the companies you have visited so far ?\",\n",
    "    # \"show me work experience and projects\",\n",
    "    # \"where did you work and what did you do at Juniper?\",\n",
    "    # \"education details\",\n",
    "    # \"list publications\",\n",
    "    # \"Tell me about the project regarding account statement verification\",\n",
    "    # \"Tell me about the improper face uploads\",\n",
    "    # \"Tell me something about yourself\",\n",
    "    # \"Describe your biggest struggle\",\n",
    "    # \"Explain anything where you have used python\",\n",
    "    \"DO you have experience with NLP if so then describe where you have used it\"\n",
    "    \n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"QUERY:\", q)\n",
    "    mapped, ranked_docs = retrieve_by_label_mapping_ranked(q, all_docs, classifier_pipeline=classifier, top_k_labels=5, score_threshold=0.35, vector_retriever=vector_retriever)\n",
    "    for i, (d, score, matched_labels) in enumerate(ranked_docs, 1):\n",
    "        print(f\"\\n--- FETCHED DOC {i} ---\")\n",
    "        print(\"SCORE:\", score)\n",
    "        print(\"MATCHED LABELS:\", matched_labels)\n",
    "        print(\"HEADER:\", d.metadata.get(\"section_header\"))\n",
    "        print(\"LABEL:\", d.metadata.get(\"section_label\"))\n",
    "        print(\"SOURCE:\", d.metadata.get(\"source\"))\n",
    "        print(\"CONTENT LEN:\", len(d.page_content))\n",
    "        print(\"PREVIEW:\", d.page_content[:600].strip())\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# ---- end cell ----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': '#TITLE#', 'source': 'https://fearless-writers-028990.framer.app/project/account-statement-verification', 'section_header': 'Start of bodyStart', 'section_type': 'remote_html'}, page_content='Start of bodyStart  \\n  End of bodyStart  \\n \\n $ \\n \\n \\n \\n $ \\n \\n Ritam Upadhyay \\n Data Scientist \\n /$ \\n \\n \\n $ \\n $ \\n Home \\n /$ \\n \\n /$ $ \\n $ \\n About \\n /$ \\n \\n /$ $ \\n $ \\n Projects \\n /$ \\n \\n /$ $ \\n $ \\n Stacks \\n /$ \\n \\n /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Search \\n \\n \\n \\n \\n $ /$ \\n \\n \\n $ \\n $ /$ \\n \\n /$ $ \\n $ /$ \\n \\n /$ $ \\n $ /$ \\n \\n /$ $ \\n $ /$ \\n \\n /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n $ /$ \\n \\n Menu \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Available for work'),\n",
       " Document(metadata={'Header 1': 'Account Statement Verification -', 'source': 'https://fearless-writers-028990.framer.app/project/account-statement-verification', 'section_header': 'Account Statement Verification -', 'section_type': 'remote_html'}, page_content='Account Statement Verification -  \\n \\n \\n Date \\n October 2022 \\n \\n \\n Service \\n \\n \\n \\n Employer \\n Paytm Money'),\n",
       " Document(metadata={'Header 2': 'Project Overview', 'source': 'https://fearless-writers-028990.framer.app/project/account-statement-verification', 'section_header': 'Project Overview', 'section_type': 'remote_html'}, page_content='Project Overview \\n Users had to upload their account statements as bank proof or income proof to open a trading account with Paytm Money. We devised various methods using OCR and text extraction to find parameters like account number, ifsc-code, name, duration, closing balance, and so on to verify the document.'),\n",
       " Document(metadata={'Header 2': 'Key Highlights', 'source': 'https://fearless-writers-028990.framer.app/project/account-statement-verification', 'section_header': 'Key Highlights', 'section_type': 'remote_html'}, page_content='Key Highlights \\n \\n Analyzed account statement PDFs and patterns to extract account number, IFSC, date, and name to match it with the details confirmed by the user. \\n Data mining methods were used to extract fields crucial to validate the statements uploaded by users as bank and income proofs. \\n Using data mining methods, I spear headed the automation of verification for the bank proof upload at 40%. \\n Trained a CNN model using Python and Tensorflow to identify account statements based on visual characteristics. \\n Used Python and FastAPI to create RESTful API for checks.'),\n",
       " Document(metadata={'Header 2': 'More Project', 'source': 'https://fearless-writers-028990.framer.app/project/account-statement-verification', 'section_header': 'Account Statement Verification', 'section_label': 'Account Statement Verification', 'section_type': 'remote_html'}, page_content='More Project \\n \\n $ $ \\n $ \\n \\n \\n RAG-PDF QA Method \\n Implementation of RAG method to perform question answering on PDF documents. HuggingFace embeddings in addition to the Groq API were used for efficient and accurate information retrieval. Session management was done to ensure embeddings are computed once per document upload to optimize performance by avoiding redundant computations.  \\n \\n \\n \\n \\n January 2025 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n RAG-PDF QA Method \\n Implementation of RAG method to perform question answering on PDF documents. HuggingFace embeddings in addition to the Groq API were used for efficient and accurate information retrieval. Session management was done to ensure embeddings are computed once per document upload to optimize performance by avoiding redundant computations.  \\n \\n \\n \\n \\n January 2025 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n RAG-PDF QA Method \\n Implementation of RAG method to perform question answering on PDF documents. HuggingFace embeddings in addition to the Groq API were used for efficient and accurate information retrieval. Session management was done to ensure embeddings are computed once per document upload to optimize performance by avoiding redundant computations.  \\n \\n \\n \\n \\n January 2025 \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n LLM Based Q/A on Web Context  \\n Development of Web App that uses open source LLMs to answer user queries based on its own knowledge as well as any reference provided in the form of external webpages. \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n LLM Based Q/A on Web Context  \\n Development of Web App that uses open source LLMs to answer user queries based on its own knowledge as well as any reference provided in the form of external webpages. \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n LLM Based Q/A on Web Context  \\n Development of Web App that uses open source LLMs to answer user queries based on its own knowledge as well as any reference provided in the form of external webpages. \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n Next Word \\n \"Forecasting the Future: Utilizing LSTM for Next-Word Prediction in Natural Language Processing.\"  \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Next Word \\n \"Forecasting the Future: Utilizing LSTM for Next-Word Prediction in Natural Language Processing.\"  \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Next Word \\n \"Forecasting the Future: Utilizing LSTM for Next-Word Prediction in Natural Language Processing.\"  \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n /$ /$ \\n \\n \\n \\n \\n \\n \\n \\n Index \\n \\n $ Main Home /$ \\n $ About /$ \\n \\n \\n \\n Resources \\n \\n $ Project /$ \\n $ Stack /$ \\n \\n \\n \\n Contact \\n \\n $ rupadh17@asu.edu /$ \\n $ Linkedin /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Index \\n \\n $ Main Home /$ \\n $ About /$ \\n \\n \\n \\n Resources \\n \\n $ Project /$ \\n $ Stack /$ \\n \\n \\n \\n Contact \\n \\n $ rupadh17@asu.edu /$ \\n $ Linkedin /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Index \\n \\n $ Main Home /$ \\n $ About /$ \\n \\n \\n \\n Resources \\n \\n $ Project /$ \\n $ Stack /$ \\n \\n \\n \\n Contact \\n \\n $ rupadh17@asu.edu /$ \\n $ Linkedin /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n /$ \\n \\n (()=>{function u(){function n(t,e,i){let r=document.createElement(\"a\");r.href=t,r.target=i,r.rel=e,document.body.appendChild(r),r.click(),r.remove()}function o(t){if(this.dataset.hydrated){this.removeEventListener(\"click\",o);return}t.preventDefault(),t.stopPropagation();let e=this.getAttribute(\"href\");if(!e)return;if(/Mac|iPod|iPhone|iPad/u.test(navigator.userAgent)?t.metaKey:t.ctrlKey)return n(e,\"\",\"_blank\");let r=this.getAttribute(\"rel\")??\"\",c=this.getAttribute(\"target\")??\"\";n(e,r,c)}function a(t){if(this.dataset.hydrated){this.removeEventListener(\"auxclick\",o);return}t.preventDefault(),t.stopPropagation();let e=this.getAttribute(\"href\");e&&n(e,\"\",\"_blank\")}function s(t){if(this.dataset.hydrated){this.removeEventListener(\"keydown\",s);return}if(t.key!==\"Enter\")return;t.preventDefault(),t.stopPropagation();let e=this.getAttribute(\"href\");if(!e)return;let i=this.getAttribute(\"rel\")??\"\",r=this.getAttribute(\"target\")??\"\";n(e,i,r)}document.querySelectorAll(\"[data-nested-link]\").forEach(t=>{t instanceof HTMLElement&&(t.addEventListener(\"click\",o),t.addEventListener(\"auxclick\",a),t.addEventListener(\"keydown\",s))})}return u})()() (()=>{function i(){for(let e of document.querySelectorAll(\"[data-framer-original-sizes]\")){let t=e.getAttribute(\"data-framer-original-sizes\");t===\"\"?e.removeAttribute(\"sizes\"):e.setAttribute(\"sizes\",t),e.removeAttribute(\"data-framer-original-sizes\")}}function a(){window.__framer_onRewriteBreakpoints=i}return a})()() \\n !function(){function c(t,r){let e=r.indexOf(\"#\"),n=e===-1?r:r.substring(0,e),o=e===-1?\"\":r.substring(e),a=n.indexOf(\"?\");if(a===-1)return n+t+o;let d=new URLSearchParams(t),h=n.substring(a+1),s=new URLSearchParams(h);for(let[i,m]of d)s.has(i)||s.append(i,m);return n.substring(0,a+1)+s.toString()+o}var l=\\'div#main a[href^=\"#\"],div#main a[href^=\"/\"],div#main a[href^=\".\"]\\',u=\"div#main a[data-framer-preserve-params]\",f,g=(f=document.currentScript)==null?void 0:f.hasAttribute(\"data-preserve-internal-params\");if(window.location.search&&!/bot|-google|google-|yandex|ia_archiver|crawl|spider/iu.test(navigator.userAgent)){let t=document.querySelectorAll(g?`${l},${u}`:u);for(let r of t){let e=c(window.location.search,r.href);r.setAttribute(\"href\",e)}}\\r\\n}() \\n \\n $ $ $ \\n \\n \\n $ Create a free website with Framer, the website builder loved by startups, designers and agencies. \\n \\n /$ \\n \\n \\n /$ /$ /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  Start of bodyEnd  \\n  End of bodyEnd'),\n",
       " Document(metadata={'Header 1': '#TITLE#', 'source': 'https://fearless-writers-028990.framer.app/', 'section_header': 'Start of bodyStart', 'section_type': 'remote_html'}, page_content='Start of bodyStart  \\n  End of bodyStart  \\n \\n $ html body { background: var(--token-29c1a320-cdc0-4d5e-a54f-37299311641d, rgb(255, 255, 255)); } \\n \\n \\n \\n \\n $ \\n \\n Ritam Upadhyay \\n Data Scientist \\n /$ \\n \\n \\n $ \\n $ \\n Home \\n /$ \\n \\n /$ $ \\n $ \\n About \\n /$ \\n \\n /$ $ \\n $ \\n Projects \\n /$ \\n \\n /$ $ \\n $ \\n Stacks \\n /$ \\n \\n /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Search \\n \\n \\n \\n \\n $ /$ \\n \\n Menu \\n \\n \\n \\n \\n \\n \\n \\n \\n $ /$ \\n \\n \\n $ \\n $ /$ \\n \\n /$ $ \\n $ /$ \\n \\n /$ $ \\n $ /$ \\n \\n /$ $ \\n $ /$ \\n \\n /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Available for work \\n \\n \\n \\n $ \\n $ \\n Back to homepage \\n \\n \\n /$ \\n \\n \\n $ \\n Back to homepage \\n \\n \\n /$ \\n \\n \\n $ \\n Back to homepage \\n \\n \\n /$ \\n \\n /$ \\n \\n \\n \\n $ \\n \\n Ritam Upadhyay \\n \\n Data Scientist \\n Problem Solver \\n Data Scientist \\n Problem Solver \\n Data Scientist \\n Problem Solver \\n Data Scientist \\n Problem Solver \\n \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n Ritam Upadhyay \\n \\n Data Scientist \\n Problem Solver \\n Data Scientist \\n Problem Solver \\n Data Scientist \\n Problem Solver \\n Data Scientist \\n Problem Solver \\n \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n Ritam Upadhyay \\n \\n Data Scientist \\n Problem Solver \\n Data Scientist \\n Problem Solver \\n Data Scientist \\n Problem Solver \\n Data Scientist \\n Problem Solver \\n \\n \\n \\n \\n /$'),\n",
       " Document(metadata={'Header 2': 'About', 'source': 'https://fearless-writers-028990.framer.app/', 'section_header': 'About', 'section_type': 'remote_html'}, page_content=\"About \\n Welcome to my portfolio! I am an ASU Master's student ('26) with experience at Juniper Networks and Paytm Money. I leverage ML/AI to solve complex business challenges, specializing in NLP, computer vision, and predictive analytics. Seeking roles in Data Science, AI/ML, and NLP to drive impact at scale. \\n \\n \\n $ \\n $ My projects \\n /$ \\n \\n \\n $ My projects \\n /$ \\n \\n \\n $ My projects \\n /$ \\n \\n /$ $ \\n $ Contact /$ \\n \\n \\n $ Contact /$ \\n \\n \\n $ Contact /$ \\n \\n /$\"),\n",
       " Document(metadata={'Header 2': 'Contact', 'source': 'https://fearless-writers-028990.framer.app/', 'section_header': 'Contact', 'section_type': 'remote_html'}, page_content='Contact \\n \\n \\n Linkedin \\n \\n $ \\n ritam.upadhyay \\n \\n \\n /$ \\n \\n \\n $ \\n ritam.upadhyay \\n \\n \\n /$ \\n \\n \\n $ \\n ritam.upadhyay \\n \\n \\n /$ \\n \\n \\n \\n Github \\n \\n $ \\n ritam3 \\n \\n \\n /$ \\n \\n \\n $ \\n ritam3 \\n \\n \\n /$ \\n \\n \\n $ \\n ritam3 \\n \\n \\n /$ \\n \\n \\n \\n Email \\n \\n $ \\n rupadh17@asu.edu \\n \\n \\n /$ \\n \\n \\n $ \\n rupadh17@asu.edu \\n \\n \\n /$ \\n \\n \\n $ \\n rupadh17@asu.edu \\n \\n \\n /$'),\n",
       " Document(metadata={'Header 2': 'Education', 'source': 'https://fearless-writers-028990.framer.app/', 'section_header': 'Education', 'section_type': 'remote_html'}, page_content=\"Education \\n \\n \\n 2018-2022 \\n \\n \\n $ \\n Bachelor of Technology in Electronics and Communication Engineering \\n \\n \\n /$ \\n \\n \\n $ \\n Bachelor of Technology in Electronics and Communication Engineering \\n \\n \\n /$ \\n \\n \\n $ \\n Bachelor of Technology in Electronics and Communication Engineering \\n \\n \\n /$ \\n \\n Birla Institute of Technology, Mesra \\n \\n \\n \\n 2024-2026 \\n \\n \\n $ \\n Master's in Data Science, Analytics and Engineering \\n \\n \\n /$ \\n \\n \\n $ \\n Master's in Data Science, Analytics and Engineering \\n \\n \\n /$ \\n \\n \\n $ \\n Master's in Data Science, Analytics and Engineering \\n \\n \\n /$ \\n \\n Arizona State University\"),\n",
       " Document(metadata={'Header 2': 'Work experience', 'source': 'https://fearless-writers-028990.framer.app/', 'section_header': 'Work experience', 'section_type': 'remote_html'}, page_content='Work experience \\n \\n $ \\n June 2025 - August 2025 \\n \\n $ \\n $ \\n Product Management Intern : Data Science \\n \\n \\n /$ \\n \\n \\n $ \\n Product Management Intern : Data Science \\n \\n \\n /$ \\n \\n \\n $ \\n Product Management Intern : Data Science \\n \\n \\n /$ \\n \\n /$ \\n Juniper Networks / Mist \\n â\\x97\\x8f Engineered an end-to-end classification pipeline that processed over 13,000 customer support tickets, achieving a 100% automated classification rate and providing proactive insights into emerging issues.\\nâ\\x97\\x8f Developed a multi-stage data processing module using LLMs to automatically extract and normalize critical information, including site, MAC address, and issue description, from raw Salesforce ticket data.\\nâ\\x97\\x8f Implemented an advanced LLM-based analysis to parse support agent-customer conversations, automatically identifying root causes and agent actions, which helped streamline case resolution and improve the agent knowledge base.\\nâ\\x97\\x8f Leveraged BERTopic on a dataset of over 13,000 \"complete issue\" data points to uncover 56 distinct, actionable topics and 728 subtopics, enabling the model to adapt to dynamic support trends without manual retraining.\\nâ\\x97\\x8f Built and deployed a final classification model using Facebook BART to automatically route each ticket to the most relevant subtopic, ensuring every new ticket is assigned a precise classification from the unsupervised model\\'s established categories. \\n \\n \\n \\n \\n August 2022 - July 2024 \\n \\n $ \\n $ \\n Software Engineer : Data Science \\n \\n \\n /$ \\n \\n \\n $ \\n Software Engineer : Data Science \\n \\n \\n /$ \\n \\n \\n $ \\n Software Engineer : Data Science \\n \\n \\n /$ \\n \\n /$ \\n Paytm Money \\n â\\x97\\x8f Developed machine learning models that ran 6000+ customer images per day in real-time to alert users regarding improper face uploads at the front end, increasing photo acceptance rate from 80% to 90%. \\nâ\\x97\\x8f Created a machine learning model using XGBoost to predict and identify churned users in advance having 93% recall, and led marketing plan campaigns. \\nâ\\x97\\x8f Devised CNN classifier for document recognition across 11 classes, achieving 97% precision and 92% accuracy. \\nâ\\x97\\x8f Engineered scalable ML models that handle 20000+ calls per day from the backend and frontend, automating the KYC verification process and streamlining the onboarding journey for users. \\nâ\\x97\\x8f Designed a K-Means clustering approach to segment 1.1M users according to investment habits in Mutual Funds. \\nâ\\x97\\x8f Initiated statistics-based verification of account statements for Income and Bank Proof, automating 85% of account statement pdf uploads. \\n \\n \\n \\n \\n January 2022 - July 2022 \\n \\n $ \\n $ \\n Software Intern \\n \\n \\n /$ \\n \\n \\n $ \\n Software Intern \\n \\n \\n /$ \\n \\n \\n $ \\n Software Intern \\n \\n \\n /$ \\n \\n /$ \\n Paytm Money \\n â\\x97\\x8f Designed ML models to extract Period and Names from Account Statements with 95% precision.\\nâ\\x97\\x8f Built ML model to perform similarity analysis using Random Forest Classifier with 98% precision. \\nâ\\x97\\x8f Deployed models as Rest API using FastAPI for scaling. \\n \\n \\n \\n \\n May 2020 - July 2021 \\n \\n $ \\n $ \\n Research Intern \\n \\n \\n /$ \\n \\n \\n $ \\n Research Intern \\n \\n \\n /$ \\n \\n \\n $ \\n Research Intern \\n \\n \\n /$ \\n \\n /$ \\n Centre of Excellence in Advanced Manufacturing Technology, IIT Kharagpur \\n â\\x97\\x8f Performed literature survey to identify gripper selection and pose estimation for robotic grippers in assembly lines.\\nâ\\x97\\x8f Created an AI-based system based on computer vision for job recognition to estimate object pose and coordinates with 100% accuracy to grasp on the test set.\\nâ\\x97\\x8f Published Real Time Deep Learning-Based Image Processing for Pose Estimation and Object Localization in Autonomous Robot Applications, The International Journal of Advanced Manufacturing Technology, Springer. \\n \\n \\n \\n /$'),\n",
       " Document(metadata={'Header 2': 'Projects', 'source': 'https://fearless-writers-028990.framer.app/', 'section_header': 'Projects', 'section_type': 'remote_html'}, page_content='Projects \\n \\n $ \\n January 2025 \\n \\n $ \\n $ \\n RAG-PDF QA Method -  \\n \\n \\n /$ \\n \\n \\n $ \\n RAG-PDF QA Method -  \\n \\n \\n /$ \\n \\n \\n $ \\n RAG-PDF QA Method -  \\n \\n \\n /$ \\n \\n /$ \\n \\n Implementation of RAG method to perform question answering on PDF documents. HuggingFace embeddings in addition to the Groq API were used for efficient and accurate information retrieval. Session management was done to ensure embeddings are computed once per document upload to optimize performance by avoiding redundant computations.  \\n \\n \\n \\n \\n December 2024 \\n \\n $ \\n $ \\n LLM Based Q/A on Web Context  - LLM Q/A based on OLLAMA \\n \\n \\n /$ \\n \\n \\n $ \\n LLM Based Q/A on Web Context  - LLM Q/A based on OLLAMA \\n \\n \\n /$ \\n \\n \\n $ \\n LLM Based Q/A on Web Context  - LLM Q/A based on OLLAMA \\n \\n \\n /$ \\n \\n /$ \\n \\n Development of Web App that uses open source LLMs to answer user queries based on its own knowledge as well as any reference provided in the form of external webpages. \\n \\n \\n \\n \\n December 2024 \\n \\n $ \\n $ \\n Next Word - Next Word Prediction \\n \\n \\n /$ \\n \\n \\n $ \\n Next Word - Next Word Prediction \\n \\n \\n /$ \\n \\n \\n $ \\n Next Word - Next Word Prediction \\n \\n \\n /$ \\n \\n /$ \\n \\n \"Forecasting the Future: Utilizing LSTM for Next-Word Prediction in Natural Language Processing.\"  \\n \\n \\n \\n \\n October 2023 \\n \\n $ \\n $ \\n Churn Prediction and Customer Cohort -  \\n \\n \\n /$ \\n \\n \\n $ \\n Churn Prediction and Customer Cohort -  \\n \\n \\n /$ \\n \\n \\n $ \\n Churn Prediction and Customer Cohort -  \\n \\n \\n /$ \\n \\n /$ \\n \\n Development of a Machine Learning Model that takes user\\'s activity data as input and predicts if the user is going to churn \\n \\n \\n \\n \\n September 2023 \\n \\n $ \\n $ \\n Improper Face Detection at Frontend - Finding faces that do not meet requirements \\n \\n \\n /$ \\n \\n \\n $ \\n Improper Face Detection at Frontend - Finding faces that do not meet requirements \\n \\n \\n /$ \\n \\n \\n $ \\n Improper Face Detection at Frontend - Finding faces that do not meet requirements \\n \\n \\n /$ \\n \\n /$ \\n \\n Use image processing and deep learning to alert users at the frontend that their photos might be rejected in the backend as their photo do not meet the requirements such as lighting, eyes properly visible, no spoof and properly clothed. \\n \\n \\n \\n \\n October 2022 \\n \\n $ \\n $ \\n Account Statement Verification -  \\n \\n \\n /$ \\n \\n \\n $ \\n Account Statement Verification -  \\n \\n \\n /$ \\n \\n \\n $ \\n Account Statement Verification -  \\n \\n \\n /$ \\n \\n /$ \\n \\n Development of models that extract parameters like account number, ifsc, date, name, duration and so on and matches them with the requirements to accept or reject the account statement as a bank or income proof \\n \\n \\n \\n \\n March 2022 \\n \\n $ \\n $ \\n AI based Name Matcher - Matching 2 Indian Names to match the similarity \\n \\n \\n /$ \\n \\n \\n $ \\n AI based Name Matcher - Matching 2 Indian Names to match the similarity \\n \\n \\n /$ \\n \\n \\n $ \\n AI based Name Matcher - Matching 2 Indian Names to match the similarity \\n \\n \\n /$ \\n \\n /$ \\n \\n Development of Machine Learning Model that predicts similarity between 2 names trained on data collected for indian names. \\n \\n \\n \\n \\n July 2021 \\n \\n $ \\n $ \\n Low Cost Vision-Based Gripper - Camera based gripper to locate, localize and grasp object \\n \\n \\n /$ \\n \\n \\n $ \\n Low Cost Vision-Based Gripper - Camera based gripper to locate, localize and grasp object \\n \\n \\n /$ \\n \\n \\n $ \\n Low Cost Vision-Based Gripper - Camera based gripper to locate, localize and grasp object \\n \\n \\n /$ \\n \\n /$ \\n \\n A KUKA robot was automated using a simple camera that enabled to locate, localize, identify the pose and feed the coordinates to robot in order to grisp it. Image processing techniques and deep learning methods were used recognize the object and PnP model was used to find the pose for a perfect grasp. \\n \\n \\n \\n /$'),\n",
       " Document(metadata={'Header 2': 'Stack', 'source': 'https://fearless-writers-028990.framer.app/', 'section_header': 'Stack', 'section_type': 'remote_html'}, page_content='Stack \\n Software & services I use in my workflow. \\n \\n \\n $ $ \\n $ \\n \\n \\n Python \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Python \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Python \\n \\n \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n SQL \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n SQL \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n SQL \\n \\n \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n Streamlit \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Streamlit \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Streamlit \\n \\n \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n Tensorflow \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Tensorflow \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Tensorflow \\n \\n \\n \\n /$ \\n \\n /$ /$ \\n \\n $ \\n $ More \\n /$ \\n \\n \\n $ More \\n /$ \\n \\n \\n $ More \\n /$ \\n \\n /$'),\n",
       " Document(metadata={'Header 2': 'Certifications', 'source': 'https://fearless-writers-028990.framer.app/', 'section_header': 'Certifications', 'section_type': 'remote_html'}, page_content='Certifications \\n \\n \\n June 2020 \\n \\n \\n $ \\n Coursera \\n \\n \\n /$ \\n \\n \\n $ \\n Coursera \\n \\n \\n /$ \\n \\n \\n $ \\n Coursera \\n \\n \\n /$ \\n \\n Neural Networks and Deep Learning \\n \\n \\n \\n June 2020 \\n \\n \\n $ \\n Coursera \\n \\n \\n /$ \\n \\n \\n $ \\n Coursera \\n \\n \\n /$ \\n \\n \\n $ \\n Coursera \\n \\n \\n /$ \\n \\n Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization \\n \\n \\n \\n July 2024 \\n \\n \\n $ \\n Coursera \\n \\n \\n /$ \\n \\n \\n $ \\n Coursera \\n \\n \\n /$ \\n \\n \\n $ \\n Coursera \\n \\n \\n /$ \\n \\n Supervised Machine Learning: Regression and Classification'),\n",
       " Document(metadata={'Header 2': 'Research Papers', 'source': 'https://fearless-writers-028990.framer.app/', 'section_header': 'Research Papers', 'section_type': 'remote_html'}, page_content='Research Papers \\n \\n \\n September 2022 \\n \\n \\n $ \\n Springer \\n \\n \\n /$ \\n \\n Real-time deep learningâ\\x80\\x93based image processing for pose estimation and object localization in autonomous robot applications \\n \\n \\n \\n September 2022 \\n \\n \\n $ \\n Springer \\n \\n \\n /$ \\n \\n Real-time deep learningâ\\x80\\x93based image processing for pose estimation and object localization in autonomous robot applications \\n \\n \\n \\n September 2022 \\n \\n \\n $ \\n Springer \\n \\n \\n /$ \\n \\n Real-time deep learningâ\\x80\\x93based image processing for pose estimation and object localization in autonomous robot applications \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Index \\n \\n $ Main Home /$ \\n $ About /$ \\n \\n \\n \\n Resources \\n \\n $ Project /$ \\n $ Stack /$ \\n \\n \\n \\n Contact \\n \\n $ rupadh17@asu.edu /$ \\n $ Linkedin /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Index \\n \\n $ Main Home /$ \\n $ About /$ \\n \\n \\n \\n Resources \\n \\n $ Project /$ \\n $ Stack /$ \\n \\n \\n \\n Contact \\n \\n $ rupadh17@asu.edu /$ \\n $ Linkedin /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Index \\n \\n $ Main Home /$ \\n $ About /$ \\n \\n \\n \\n Resources \\n \\n $ Project /$ \\n $ Stack /$ \\n \\n \\n \\n Contact \\n \\n $ rupadh17@asu.edu /$ \\n $ Linkedin /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n /$ \\n \\n (()=>{function u(){function n(t,e,i){let r=document.createElement(\"a\");r.href=t,r.target=i,r.rel=e,document.body.appendChild(r),r.click(),r.remove()}function o(t){if(this.dataset.hydrated){this.removeEventListener(\"click\",o);return}t.preventDefault(),t.stopPropagation();let e=this.getAttribute(\"href\");if(!e)return;if(/Mac|iPod|iPhone|iPad/u.test(navigator.userAgent)?t.metaKey:t.ctrlKey)return n(e,\"\",\"_blank\");let r=this.getAttribute(\"rel\")??\"\",c=this.getAttribute(\"target\")??\"\";n(e,r,c)}function a(t){if(this.dataset.hydrated){this.removeEventListener(\"auxclick\",o);return}t.preventDefault(),t.stopPropagation();let e=this.getAttribute(\"href\");e&&n(e,\"\",\"_blank\")}function s(t){if(this.dataset.hydrated){this.removeEventListener(\"keydown\",s);return}if(t.key!==\"Enter\")return;t.preventDefault(),t.stopPropagation();let e=this.getAttribute(\"href\");if(!e)return;let i=this.getAttribute(\"rel\")??\"\",r=this.getAttribute(\"target\")??\"\";n(e,i,r)}document.querySelectorAll(\"[data-nested-link]\").forEach(t=>{t instanceof HTMLElement&&(t.addEventListener(\"click\",o),t.addEventListener(\"auxclick\",a),t.addEventListener(\"keydown\",s))})}return u})()() (()=>{function i(){for(let e of document.querySelectorAll(\"[data-framer-original-sizes]\")){let t=e.getAttribute(\"data-framer-original-sizes\");t===\"\"?e.removeAttribute(\"sizes\"):e.setAttribute(\"sizes\",t),e.removeAttribute(\"data-framer-original-sizes\")}}function a(){window.__framer_onRewriteBreakpoints=i}return a})()() \\n !function(){function c(t,r){let e=r.indexOf(\"#\"),n=e===-1?r:r.substring(0,e),o=e===-1?\"\":r.substring(e),a=n.indexOf(\"?\");if(a===-1)return n+t+o;let d=new URLSearchParams(t),h=n.substring(a+1),s=new URLSearchParams(h);for(let[i,m]of d)s.has(i)||s.append(i,m);return n.substring(0,a+1)+s.toString()+o}var l=\\'div#main a[href^=\"#\"],div#main a[href^=\"/\"],div#main a[href^=\".\"]\\',u=\"div#main a[data-framer-preserve-params]\",f,g=(f=document.currentScript)==null?void 0:f.hasAttribute(\"data-preserve-internal-params\");if(window.location.search&&!/bot|-google|google-|yandex|ia_archiver|crawl|spider/iu.test(navigator.userAgent)){let t=document.querySelectorAll(g?`${l},${u}`:u);for(let r of t){let e=c(window.location.search,r.href);r.setAttribute(\"href\",e)}}\\r\\n}() \\n \\n $ $ $ \\n \\n \\n $ Create a free website with Framer, the website builder loved by startups, designers and agencies. \\n \\n /$ \\n \\n \\n /$ /$ /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  Start of bodyEnd  \\n  End of bodyEnd'),\n",
       " Document(metadata={'Header 1': '#TITLE#', 'source': 'https://fearless-writers-028990.framer.app/project/rag-pdf-qa-method', 'section_header': 'Start of bodyStart', 'section_type': 'remote_html'}, page_content='Start of bodyStart  \\n  End of bodyStart  \\n \\n $ \\n \\n \\n \\n $ \\n \\n Ritam Upadhyay \\n Data Scientist \\n /$ \\n \\n \\n $ \\n $ \\n Home \\n /$ \\n \\n /$ $ \\n $ \\n About \\n /$ \\n \\n /$ $ \\n $ \\n Projects \\n /$ \\n \\n /$ $ \\n $ \\n Stacks \\n /$ \\n \\n /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Search \\n \\n \\n \\n \\n $ /$ \\n \\n \\n $ \\n $ /$ \\n \\n /$ $ \\n $ /$ \\n \\n /$ $ \\n $ /$ \\n \\n /$ $ \\n $ /$ \\n \\n /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n $ /$ \\n \\n Menu \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Available for work'),\n",
       " Document(metadata={'Header 1': 'RAG-PDF QA Method -', 'source': 'https://fearless-writers-028990.framer.app/project/rag-pdf-qa-method', 'section_header': 'RAG-PDF QA Method -', 'section_type': 'remote_html'}, page_content='RAG-PDF QA Method -  \\n \\n \\n Date \\n January 2025 \\n \\n \\n Service \\n \\n \\n \\n Employer'),\n",
       " Document(metadata={'Header 2': 'Project Overview', 'source': 'https://fearless-writers-028990.framer.app/project/rag-pdf-qa-method', 'section_header': 'Project Overview', 'section_type': 'remote_html'}, page_content='Project Overview \\n \\n This project aimed to build an interface so that user could provide a PDF and ask specific questions to the LLM based on the pdf content. The LLM also indicated the paragraphs and texts that led to the answer that it provided. This reduced the chances of hallucinations by the model. \\n $ https://github.com/ritam3/RAG-PDF-HuggingFace-Groq/tree/main /$'),\n",
       " Document(metadata={'Header 2': 'Key Highlights', 'source': 'https://fearless-writers-028990.framer.app/project/rag-pdf-qa-method', 'section_header': 'Key Highlights', 'section_type': 'remote_html'}, page_content='Key Highlights \\n \\n Parsing of PDF Document: Supports parsing and preprocessing PDF documents for QA. \\n HuggingFace Embeddings: Used HuggingFace models to generate embeddings for document text. \\n Groq API Integration: Used Groq API for embedding retrieval and similarity search. \\n Session Management: Maintains session state so redundant calculations of embeddings can be avoided; hence, it will be more efficient. \\n Interactive QA: It allows users to ask queries after uploading documents. It retrieves relevant information from the processed PDFs.'),\n",
       " Document(metadata={'Header 2': 'More Project', 'source': 'https://fearless-writers-028990.framer.app/project/rag-pdf-qa-method', 'section_header': 'Rag Pdf Qa Method', 'section_label': 'Rag Pdf Qa Method', 'section_type': 'remote_html'}, page_content='More Project \\n \\n $ $ \\n $ \\n \\n \\n LLM Based Q/A on Web Context  \\n Development of Web App that uses open source LLMs to answer user queries based on its own knowledge as well as any reference provided in the form of external webpages. \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n LLM Based Q/A on Web Context  \\n Development of Web App that uses open source LLMs to answer user queries based on its own knowledge as well as any reference provided in the form of external webpages. \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n LLM Based Q/A on Web Context  \\n Development of Web App that uses open source LLMs to answer user queries based on its own knowledge as well as any reference provided in the form of external webpages. \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n Next Word \\n \"Forecasting the Future: Utilizing LSTM for Next-Word Prediction in Natural Language Processing.\"  \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Next Word \\n \"Forecasting the Future: Utilizing LSTM for Next-Word Prediction in Natural Language Processing.\"  \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Next Word \\n \"Forecasting the Future: Utilizing LSTM for Next-Word Prediction in Natural Language Processing.\"  \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n Churn Prediction and Customer Cohort \\n Development of a Machine Learning Model that takes user\\'s activity data as input and predicts if the user is going to churn \\n \\n \\n \\n \\n October 2023 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Churn Prediction and Customer Cohort \\n Development of a Machine Learning Model that takes user\\'s activity data as input and predicts if the user is going to churn \\n \\n \\n \\n \\n October 2023 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Churn Prediction and Customer Cohort \\n Development of a Machine Learning Model that takes user\\'s activity data as input and predicts if the user is going to churn \\n \\n \\n \\n \\n October 2023 \\n \\n /$ \\n \\n /$ /$ \\n \\n \\n \\n \\n \\n \\n \\n Index \\n \\n $ Main Home /$ \\n $ About /$ \\n \\n \\n \\n Resources \\n \\n $ Project /$ \\n $ Stack /$ \\n \\n \\n \\n Contact \\n \\n $ rupadh17@asu.edu /$ \\n $ Linkedin /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Index \\n \\n $ Main Home /$ \\n $ About /$ \\n \\n \\n \\n Resources \\n \\n $ Project /$ \\n $ Stack /$ \\n \\n \\n \\n Contact \\n \\n $ rupadh17@asu.edu /$ \\n $ Linkedin /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Index \\n \\n $ Main Home /$ \\n $ About /$ \\n \\n \\n \\n Resources \\n \\n $ Project /$ \\n $ Stack /$ \\n \\n \\n \\n Contact \\n \\n $ rupadh17@asu.edu /$ \\n $ Linkedin /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n /$ \\n \\n (()=>{function u(){function n(t,e,i){let r=document.createElement(\"a\");r.href=t,r.target=i,r.rel=e,document.body.appendChild(r),r.click(),r.remove()}function o(t){if(this.dataset.hydrated){this.removeEventListener(\"click\",o);return}t.preventDefault(),t.stopPropagation();let e=this.getAttribute(\"href\");if(!e)return;if(/Mac|iPod|iPhone|iPad/u.test(navigator.userAgent)?t.metaKey:t.ctrlKey)return n(e,\"\",\"_blank\");let r=this.getAttribute(\"rel\")??\"\",c=this.getAttribute(\"target\")??\"\";n(e,r,c)}function a(t){if(this.dataset.hydrated){this.removeEventListener(\"auxclick\",o);return}t.preventDefault(),t.stopPropagation();let e=this.getAttribute(\"href\");e&&n(e,\"\",\"_blank\")}function s(t){if(this.dataset.hydrated){this.removeEventListener(\"keydown\",s);return}if(t.key!==\"Enter\")return;t.preventDefault(),t.stopPropagation();let e=this.getAttribute(\"href\");if(!e)return;let i=this.getAttribute(\"rel\")??\"\",r=this.getAttribute(\"target\")??\"\";n(e,i,r)}document.querySelectorAll(\"[data-nested-link]\").forEach(t=>{t instanceof HTMLElement&&(t.addEventListener(\"click\",o),t.addEventListener(\"auxclick\",a),t.addEventListener(\"keydown\",s))})}return u})()() (()=>{function i(){for(let e of document.querySelectorAll(\"[data-framer-original-sizes]\")){let t=e.getAttribute(\"data-framer-original-sizes\");t===\"\"?e.removeAttribute(\"sizes\"):e.setAttribute(\"sizes\",t),e.removeAttribute(\"data-framer-original-sizes\")}}function a(){window.__framer_onRewriteBreakpoints=i}return a})()() \\n !function(){function c(t,r){let e=r.indexOf(\"#\"),n=e===-1?r:r.substring(0,e),o=e===-1?\"\":r.substring(e),a=n.indexOf(\"?\");if(a===-1)return n+t+o;let d=new URLSearchParams(t),h=n.substring(a+1),s=new URLSearchParams(h);for(let[i,m]of d)s.has(i)||s.append(i,m);return n.substring(0,a+1)+s.toString()+o}var l=\\'div#main a[href^=\"#\"],div#main a[href^=\"/\"],div#main a[href^=\".\"]\\',u=\"div#main a[data-framer-preserve-params]\",f,g=(f=document.currentScript)==null?void 0:f.hasAttribute(\"data-preserve-internal-params\");if(window.location.search&&!/bot|-google|google-|yandex|ia_archiver|crawl|spider/iu.test(navigator.userAgent)){let t=document.querySelectorAll(g?`${l},${u}`:u);for(let r of t){let e=c(window.location.search,r.href);r.setAttribute(\"href\",e)}}\\r\\n}() \\n \\n $ $ $ \\n \\n \\n $ Create a free website with Framer, the website builder loved by startups, designers and agencies. \\n \\n /$ \\n \\n \\n /$ /$ /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  Start of bodyEnd  \\n  End of bodyEnd'),\n",
       " Document(metadata={'Header 1': '#TITLE#', 'source': 'https://fearless-writers-028990.framer.app/project/ai-based-name-matcher', 'section_header': 'Start of bodyStart', 'section_type': 'remote_html'}, page_content='Start of bodyStart  \\n  End of bodyStart  \\n \\n $ \\n \\n \\n \\n $ \\n \\n Ritam Upadhyay \\n Data Scientist \\n /$ \\n \\n \\n $ \\n $ \\n Home \\n /$ \\n \\n /$ $ \\n $ \\n About \\n /$ \\n \\n /$ $ \\n $ \\n Projects \\n /$ \\n \\n /$ $ \\n $ \\n Stacks \\n /$ \\n \\n /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Search \\n \\n \\n \\n \\n $ /$ \\n \\n \\n $ \\n $ /$ \\n \\n /$ $ \\n $ /$ \\n \\n /$ $ \\n $ /$ \\n \\n /$ $ \\n $ /$ \\n \\n /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n $ /$ \\n \\n Menu \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Available for work'),\n",
       " Document(metadata={'Header 1': 'AI based Name Matcher - Matching 2 Indian Names to match the similarity', 'source': 'https://fearless-writers-028990.framer.app/project/ai-based-name-matcher', 'section_header': 'AI based Name Matcher - Matching 2 Indian Names to match the similarity', 'section_type': 'remote_html'}, page_content='AI based Name Matcher - Matching 2 Indian Names to match the similarity \\n \\n \\n Date \\n March 2022 \\n \\n \\n Service \\n \\n \\n \\n Employer \\n Paytm Money'),\n",
       " Document(metadata={'Header 2': 'Project Overview', 'source': 'https://fearless-writers-028990.framer.app/project/ai-based-name-matcher', 'section_header': 'Project Overview', 'section_type': 'remote_html'}, page_content='Project Overview \\n Fin-tech industries use a method called penny-drop to verify bank account numbers mentioned by users by crediting Re 1 to the account number and obtaining the name associated with the account number. This helps to find if the account number that the user mentioned belongs to him.'),\n",
       " Document(metadata={'Header 2': 'Key Highlights', 'source': 'https://fearless-writers-028990.framer.app/project/ai-based-name-matcher', 'section_header': 'Key Highlights', 'section_type': 'remote_html'}, page_content='Key Highlights \\n \\n The account holder name obtained by crediting Re 1 is matched with the name mentioned in the PAN of the user to confirm if the bank account belongs to the user. \\n AI-based Name Matcher uses the Random Forest model to calculate the similarity based on both phonetic and distance specially trained on Indian names. \\n Used Python to preprocess the data and train the ML model. \\n The acceptance went from 70% to 80% compared to the algorithmic name-matcher used prior.'),\n",
       " Document(metadata={'Header 2': 'More Project', 'source': 'https://fearless-writers-028990.framer.app/project/ai-based-name-matcher', 'section_header': 'Ai Based Name Matcher', 'section_label': 'Ai Based Name Matcher', 'section_type': 'remote_html'}, page_content='More Project \\n \\n $ $ \\n $ \\n \\n \\n RAG-PDF QA Method \\n Implementation of RAG method to perform question answering on PDF documents. HuggingFace embeddings in addition to the Groq API were used for efficient and accurate information retrieval. Session management was done to ensure embeddings are computed once per document upload to optimize performance by avoiding redundant computations.  \\n \\n \\n \\n \\n January 2025 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n RAG-PDF QA Method \\n Implementation of RAG method to perform question answering on PDF documents. HuggingFace embeddings in addition to the Groq API were used for efficient and accurate information retrieval. Session management was done to ensure embeddings are computed once per document upload to optimize performance by avoiding redundant computations.  \\n \\n \\n \\n \\n January 2025 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n RAG-PDF QA Method \\n Implementation of RAG method to perform question answering on PDF documents. HuggingFace embeddings in addition to the Groq API were used for efficient and accurate information retrieval. Session management was done to ensure embeddings are computed once per document upload to optimize performance by avoiding redundant computations.  \\n \\n \\n \\n \\n January 2025 \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n LLM Based Q/A on Web Context  \\n Development of Web App that uses open source LLMs to answer user queries based on its own knowledge as well as any reference provided in the form of external webpages. \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n LLM Based Q/A on Web Context  \\n Development of Web App that uses open source LLMs to answer user queries based on its own knowledge as well as any reference provided in the form of external webpages. \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n LLM Based Q/A on Web Context  \\n Development of Web App that uses open source LLMs to answer user queries based on its own knowledge as well as any reference provided in the form of external webpages. \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n Next Word \\n \"Forecasting the Future: Utilizing LSTM for Next-Word Prediction in Natural Language Processing.\"  \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Next Word \\n \"Forecasting the Future: Utilizing LSTM for Next-Word Prediction in Natural Language Processing.\"  \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Next Word \\n \"Forecasting the Future: Utilizing LSTM for Next-Word Prediction in Natural Language Processing.\"  \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n /$ /$ \\n \\n \\n \\n \\n \\n \\n \\n Index \\n \\n $ Main Home /$ \\n $ About /$ \\n \\n \\n \\n Resources \\n \\n $ Project /$ \\n $ Stack /$ \\n \\n \\n \\n Contact \\n \\n $ rupadh17@asu.edu /$ \\n $ Linkedin /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Index \\n \\n $ Main Home /$ \\n $ About /$ \\n \\n \\n \\n Resources \\n \\n $ Project /$ \\n $ Stack /$ \\n \\n \\n \\n Contact \\n \\n $ rupadh17@asu.edu /$ \\n $ Linkedin /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Index \\n \\n $ Main Home /$ \\n $ About /$ \\n \\n \\n \\n Resources \\n \\n $ Project /$ \\n $ Stack /$ \\n \\n \\n \\n Contact \\n \\n $ rupadh17@asu.edu /$ \\n $ Linkedin /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n /$ \\n \\n (()=>{function u(){function n(t,e,i){let r=document.createElement(\"a\");r.href=t,r.target=i,r.rel=e,document.body.appendChild(r),r.click(),r.remove()}function o(t){if(this.dataset.hydrated){this.removeEventListener(\"click\",o);return}t.preventDefault(),t.stopPropagation();let e=this.getAttribute(\"href\");if(!e)return;if(/Mac|iPod|iPhone|iPad/u.test(navigator.userAgent)?t.metaKey:t.ctrlKey)return n(e,\"\",\"_blank\");let r=this.getAttribute(\"rel\")??\"\",c=this.getAttribute(\"target\")??\"\";n(e,r,c)}function a(t){if(this.dataset.hydrated){this.removeEventListener(\"auxclick\",o);return}t.preventDefault(),t.stopPropagation();let e=this.getAttribute(\"href\");e&&n(e,\"\",\"_blank\")}function s(t){if(this.dataset.hydrated){this.removeEventListener(\"keydown\",s);return}if(t.key!==\"Enter\")return;t.preventDefault(),t.stopPropagation();let e=this.getAttribute(\"href\");if(!e)return;let i=this.getAttribute(\"rel\")??\"\",r=this.getAttribute(\"target\")??\"\";n(e,i,r)}document.querySelectorAll(\"[data-nested-link]\").forEach(t=>{t instanceof HTMLElement&&(t.addEventListener(\"click\",o),t.addEventListener(\"auxclick\",a),t.addEventListener(\"keydown\",s))})}return u})()() (()=>{function i(){for(let e of document.querySelectorAll(\"[data-framer-original-sizes]\")){let t=e.getAttribute(\"data-framer-original-sizes\");t===\"\"?e.removeAttribute(\"sizes\"):e.setAttribute(\"sizes\",t),e.removeAttribute(\"data-framer-original-sizes\")}}function a(){window.__framer_onRewriteBreakpoints=i}return a})()() \\n !function(){function c(t,r){let e=r.indexOf(\"#\"),n=e===-1?r:r.substring(0,e),o=e===-1?\"\":r.substring(e),a=n.indexOf(\"?\");if(a===-1)return n+t+o;let d=new URLSearchParams(t),h=n.substring(a+1),s=new URLSearchParams(h);for(let[i,m]of d)s.has(i)||s.append(i,m);return n.substring(0,a+1)+s.toString()+o}var l=\\'div#main a[href^=\"#\"],div#main a[href^=\"/\"],div#main a[href^=\".\"]\\',u=\"div#main a[data-framer-preserve-params]\",f,g=(f=document.currentScript)==null?void 0:f.hasAttribute(\"data-preserve-internal-params\");if(window.location.search&&!/bot|-google|google-|yandex|ia_archiver|crawl|spider/iu.test(navigator.userAgent)){let t=document.querySelectorAll(g?`${l},${u}`:u);for(let r of t){let e=c(window.location.search,r.href);r.setAttribute(\"href\",e)}}\\r\\n}() \\n \\n $ $ $ \\n \\n \\n $ Create a free website with Framer, the website builder loved by startups, designers and agencies. \\n \\n /$ \\n \\n \\n /$ /$ /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  Start of bodyEnd  \\n  End of bodyEnd'),\n",
       " Document(metadata={'Header 1': '#TITLE#', 'source': 'https://fearless-writers-028990.framer.app/project', 'section_header': 'Start of bodyStart', 'section_type': 'remote_html'}, page_content='Start of bodyStart  \\n  End of bodyStart  \\n \\n $ \\n \\n \\n \\n $ \\n \\n Ritam Upadhyay \\n Data Scientist \\n /$ \\n \\n \\n $ \\n $ \\n Home \\n /$ \\n \\n /$ $ \\n $ \\n About \\n /$ \\n \\n /$ $ \\n $ \\n Projects \\n /$ \\n \\n /$ $ \\n $ \\n Stacks \\n /$ \\n \\n /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Search \\n \\n \\n \\n \\n $ /$ \\n \\n Menu \\n \\n \\n \\n \\n \\n \\n \\n \\n $ /$ \\n \\n \\n $ \\n $ /$ \\n \\n /$ $ \\n $ /$ \\n \\n /$ $ \\n $ /$ \\n \\n /$ $ \\n $ /$ \\n \\n /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Available for work \\n \\n \\n \\n $ \\n $ \\n Back to homepage \\n \\n \\n /$ \\n \\n \\n $ \\n Back to homepage \\n \\n \\n /$ \\n \\n \\n $ \\n Back to homepage \\n \\n \\n /$ \\n \\n /$'),\n",
       " Document(metadata={'Header 2': 'My Latest Works', 'source': 'https://fearless-writers-028990.framer.app/project', 'section_header': 'My Latest Works', 'section_type': 'remote_html'}, page_content='My Latest Works \\n \\n $ $ \\n $ \\n \\n \\n RAG-PDF QA Method \\n Implementation of RAG method to perform question answering on PDF documents. HuggingFace embeddings in addition to the Groq API were used for efficient and accurate information retrieval. Session management was done to ensure embeddings are computed once per document upload to optimize performance by avoiding redundant computations.  \\n \\n \\n \\n \\n January 2025 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n RAG-PDF QA Method \\n Implementation of RAG method to perform question answering on PDF documents. HuggingFace embeddings in addition to the Groq API were used for efficient and accurate information retrieval. Session management was done to ensure embeddings are computed once per document upload to optimize performance by avoiding redundant computations.  \\n \\n \\n \\n \\n January 2025 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n RAG-PDF QA Method \\n Implementation of RAG method to perform question answering on PDF documents. HuggingFace embeddings in addition to the Groq API were used for efficient and accurate information retrieval. Session management was done to ensure embeddings are computed once per document upload to optimize performance by avoiding redundant computations.  \\n \\n \\n \\n \\n January 2025 \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n LLM Based Q/A on Web Context  \\n Development of Web App that uses open source LLMs to answer user queries based on its own knowledge as well as any reference provided in the form of external webpages. \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n LLM Based Q/A on Web Context  \\n Development of Web App that uses open source LLMs to answer user queries based on its own knowledge as well as any reference provided in the form of external webpages. \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n LLM Based Q/A on Web Context  \\n Development of Web App that uses open source LLMs to answer user queries based on its own knowledge as well as any reference provided in the form of external webpages. \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n Next Word \\n \"Forecasting the Future: Utilizing LSTM for Next-Word Prediction in Natural Language Processing.\"  \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Next Word \\n \"Forecasting the Future: Utilizing LSTM for Next-Word Prediction in Natural Language Processing.\"  \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Next Word \\n \"Forecasting the Future: Utilizing LSTM for Next-Word Prediction in Natural Language Processing.\"  \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n Churn Prediction and Customer Cohort \\n Development of a Machine Learning Model that takes user\\'s activity data as input and predicts if the user is going to churn \\n \\n \\n \\n \\n October 2023 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Churn Prediction and Customer Cohort \\n Development of a Machine Learning Model that takes user\\'s activity data as input and predicts if the user is going to churn \\n \\n \\n \\n \\n October 2023 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Churn Prediction and Customer Cohort \\n Development of a Machine Learning Model that takes user\\'s activity data as input and predicts if the user is going to churn \\n \\n \\n \\n \\n October 2023 \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n Improper Face Detection at Frontend \\n Use image processing and deep learning to alert users at the frontend that their photos might be rejected in the backend as their photo do not meet the requirements such as lighting, eyes properly visible, no spoof and properly clothed. \\n \\n \\n \\n \\n September 2023 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Improper Face Detection at Frontend \\n Use image processing and deep learning to alert users at the frontend that their photos might be rejected in the backend as their photo do not meet the requirements such as lighting, eyes properly visible, no spoof and properly clothed. \\n \\n \\n \\n \\n September 2023 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Improper Face Detection at Frontend \\n Use image processing and deep learning to alert users at the frontend that their photos might be rejected in the backend as their photo do not meet the requirements such as lighting, eyes properly visible, no spoof and properly clothed. \\n \\n \\n \\n \\n September 2023 \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n Account Statement Verification \\n Development of models that extract parameters like account number, ifsc, date, name, duration and so on and matches them with the requirements to accept or reject the account statement as a bank or income proof \\n \\n \\n \\n \\n October 2022 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Account Statement Verification \\n Development of models that extract parameters like account number, ifsc, date, name, duration and so on and matches them with the requirements to accept or reject the account statement as a bank or income proof \\n \\n \\n \\n \\n October 2022 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Account Statement Verification \\n Development of models that extract parameters like account number, ifsc, date, name, duration and so on and matches them with the requirements to accept or reject the account statement as a bank or income proof \\n \\n \\n \\n \\n October 2022 \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n AI based Name Matcher \\n Development of Machine Learning Model that predicts similarity between 2 names trained on data collected for indian names. \\n \\n \\n \\n \\n March 2022 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n AI based Name Matcher \\n Development of Machine Learning Model that predicts similarity between 2 names trained on data collected for indian names. \\n \\n \\n \\n \\n March 2022 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n AI based Name Matcher \\n Development of Machine Learning Model that predicts similarity between 2 names trained on data collected for indian names. \\n \\n \\n \\n \\n March 2022 \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n Low Cost Vision-Based Gripper \\n A KUKA robot was automated using a simple camera that enabled to locate, localize, identify the pose and feed the coordinates to robot in order to grisp it. Image processing techniques and deep learning methods were used recognize the object and PnP model was used to find the pose for a perfect grasp. \\n \\n \\n \\n \\n July 2021 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Low Cost Vision-Based Gripper \\n A KUKA robot was automated using a simple camera that enabled to locate, localize, identify the pose and feed the coordinates to robot in order to grisp it. Image processing techniques and deep learning methods were used recognize the object and PnP model was used to find the pose for a perfect grasp. \\n \\n \\n \\n \\n July 2021 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Low Cost Vision-Based Gripper \\n A KUKA robot was automated using a simple camera that enabled to locate, localize, identify the pose and feed the coordinates to robot in order to grisp it. Image processing techniques and deep learning methods were used recognize the object and PnP model was used to find the pose for a perfect grasp. \\n \\n \\n \\n \\n July 2021 \\n \\n /$ \\n \\n /$ /$ \\n \\n \\n \\n \\n \\n \\n \\n Index \\n \\n $ Main Home /$ \\n $ About /$ \\n \\n \\n \\n Resources \\n \\n $ Project /$ \\n $ Stack /$ \\n \\n \\n \\n Contact \\n \\n $ rupadh17@asu.edu /$ \\n $ Linkedin /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Index \\n \\n $ Main Home /$ \\n $ About /$ \\n \\n \\n \\n Resources \\n \\n $ Project /$ \\n $ Stack /$ \\n \\n \\n \\n Contact \\n \\n $ rupadh17@asu.edu /$ \\n $ Linkedin /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Index \\n \\n $ Main Home /$ \\n $ About /$ \\n \\n \\n \\n Resources \\n \\n $ Project /$ \\n $ Stack /$ \\n \\n \\n \\n Contact \\n \\n $ rupadh17@asu.edu /$ \\n $ Linkedin /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n /$ \\n \\n (()=>{function u(){function n(t,e,i){let r=document.createElement(\"a\");r.href=t,r.target=i,r.rel=e,document.body.appendChild(r),r.click(),r.remove()}function o(t){if(this.dataset.hydrated){this.removeEventListener(\"click\",o);return}t.preventDefault(),t.stopPropagation();let e=this.getAttribute(\"href\");if(!e)return;if(/Mac|iPod|iPhone|iPad/u.test(navigator.userAgent)?t.metaKey:t.ctrlKey)return n(e,\"\",\"_blank\");let r=this.getAttribute(\"rel\")??\"\",c=this.getAttribute(\"target\")??\"\";n(e,r,c)}function a(t){if(this.dataset.hydrated){this.removeEventListener(\"auxclick\",o);return}t.preventDefault(),t.stopPropagation();let e=this.getAttribute(\"href\");e&&n(e,\"\",\"_blank\")}function s(t){if(this.dataset.hydrated){this.removeEventListener(\"keydown\",s);return}if(t.key!==\"Enter\")return;t.preventDefault(),t.stopPropagation();let e=this.getAttribute(\"href\");if(!e)return;let i=this.getAttribute(\"rel\")??\"\",r=this.getAttribute(\"target\")??\"\";n(e,i,r)}document.querySelectorAll(\"[data-nested-link]\").forEach(t=>{t instanceof HTMLElement&&(t.addEventListener(\"click\",o),t.addEventListener(\"auxclick\",a),t.addEventListener(\"keydown\",s))})}return u})()() (()=>{function i(){for(let e of document.querySelectorAll(\"[data-framer-original-sizes]\")){let t=e.getAttribute(\"data-framer-original-sizes\");t===\"\"?e.removeAttribute(\"sizes\"):e.setAttribute(\"sizes\",t),e.removeAttribute(\"data-framer-original-sizes\")}}function a(){window.__framer_onRewriteBreakpoints=i}return a})()() \\n !function(){function c(t,r){let e=r.indexOf(\"#\"),n=e===-1?r:r.substring(0,e),o=e===-1?\"\":r.substring(e),a=n.indexOf(\"?\");if(a===-1)return n+t+o;let d=new URLSearchParams(t),h=n.substring(a+1),s=new URLSearchParams(h);for(let[i,m]of d)s.has(i)||s.append(i,m);return n.substring(0,a+1)+s.toString()+o}var l=\\'div#main a[href^=\"#\"],div#main a[href^=\"/\"],div#main a[href^=\".\"]\\',u=\"div#main a[data-framer-preserve-params]\",f,g=(f=document.currentScript)==null?void 0:f.hasAttribute(\"data-preserve-internal-params\");if(window.location.search&&!/bot|-google|google-|yandex|ia_archiver|crawl|spider/iu.test(navigator.userAgent)){let t=document.querySelectorAll(g?`${l},${u}`:u);for(let r of t){let e=c(window.location.search,r.href);r.setAttribute(\"href\",e)}}\\r\\n}() \\n \\n $ $ $ \\n \\n \\n $ Create a free website with Framer, the website builder loved by startups, designers and agencies. \\n \\n /$ \\n \\n \\n /$ /$ /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  Start of bodyEnd  \\n  End of bodyEnd'),\n",
       " Document(metadata={'Header 1': '#TITLE#', 'source': 'https://fearless-writers-028990.framer.app/project/improper-face-detection-at-frontend', 'section_header': 'Start of bodyStart', 'section_type': 'remote_html'}, page_content='Start of bodyStart  \\n  End of bodyStart  \\n \\n $ \\n \\n \\n \\n $ \\n \\n Ritam Upadhyay \\n Data Scientist \\n /$ \\n \\n \\n $ \\n $ \\n Home \\n /$ \\n \\n /$ $ \\n $ \\n About \\n /$ \\n \\n /$ $ \\n $ \\n Projects \\n /$ \\n \\n /$ $ \\n $ \\n Stacks \\n /$ \\n \\n /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Search \\n \\n \\n \\n \\n $ /$ \\n \\n \\n $ \\n $ /$ \\n \\n /$ $ \\n $ /$ \\n \\n /$ $ \\n $ /$ \\n \\n /$ $ \\n $ /$ \\n \\n /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n $ /$ \\n \\n Menu \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Available for work'),\n",
       " Document(metadata={'Header 1': 'Improper Face Detection at Frontend - Finding faces that do not meet requirements', 'source': 'https://fearless-writers-028990.framer.app/project/improper-face-detection-at-frontend', 'section_header': 'Improper Face Detection at Frontend - Finding faces that do not meet requirements', 'section_type': 'remote_html'}, page_content='Improper Face Detection at Frontend - Finding faces that do not meet requirements \\n \\n \\n Date \\n September 2023 \\n \\n \\n Service \\n \\n \\n \\n Employer \\n Paytm Money'),\n",
       " Document(metadata={'Header 2': 'Project Overview', 'source': 'https://fearless-writers-028990.framer.app/project/improper-face-detection-at-frontend', 'section_header': 'Project Overview', 'section_type': 'remote_html'}, page_content='Project Overview \\n The users had to upload a selfie to open a trading account with Paytm Money. That photo had to match their ID uploaded earlier. The main job was to prompt users that the selfie clicked did not match the requirements and that  the user should upload a new image to reduce his onboarding time.'),\n",
       " Document(metadata={'Header 2': 'Key Highlights', 'source': 'https://fearless-writers-028990.framer.app/project/improper-face-detection-at-frontend', 'section_header': 'Key Highlights', 'section_type': 'remote_html'}, page_content='Key Highlights \\n \\n To alert the users who upload improper photos of their faces during the onboarding process with mentioned guidelines regarding the quality of the photo such as brightness, contrast, face too close to boundaries, face is spoofed, user not wearing a proper shirt, etc. \\n Used ML models and image processing techniques for filters, reducing the photo rejection at the backend by 5%. \\n Used Python and FastAPI to create RESTful API for checks.'),\n",
       " Document(metadata={'Header 2': 'More Project', 'source': 'https://fearless-writers-028990.framer.app/project/improper-face-detection-at-frontend', 'section_header': 'Improper Face Detection At Frontend', 'section_label': 'Improper Face Detection At Frontend', 'section_type': 'remote_html'}, page_content='More Project \\n \\n $ $ \\n $ \\n \\n \\n RAG-PDF QA Method \\n Implementation of RAG method to perform question answering on PDF documents. HuggingFace embeddings in addition to the Groq API were used for efficient and accurate information retrieval. Session management was done to ensure embeddings are computed once per document upload to optimize performance by avoiding redundant computations.  \\n \\n \\n \\n \\n January 2025 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n RAG-PDF QA Method \\n Implementation of RAG method to perform question answering on PDF documents. HuggingFace embeddings in addition to the Groq API were used for efficient and accurate information retrieval. Session management was done to ensure embeddings are computed once per document upload to optimize performance by avoiding redundant computations.  \\n \\n \\n \\n \\n January 2025 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n RAG-PDF QA Method \\n Implementation of RAG method to perform question answering on PDF documents. HuggingFace embeddings in addition to the Groq API were used for efficient and accurate information retrieval. Session management was done to ensure embeddings are computed once per document upload to optimize performance by avoiding redundant computations.  \\n \\n \\n \\n \\n January 2025 \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n LLM Based Q/A on Web Context  \\n Development of Web App that uses open source LLMs to answer user queries based on its own knowledge as well as any reference provided in the form of external webpages. \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n LLM Based Q/A on Web Context  \\n Development of Web App that uses open source LLMs to answer user queries based on its own knowledge as well as any reference provided in the form of external webpages. \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n LLM Based Q/A on Web Context  \\n Development of Web App that uses open source LLMs to answer user queries based on its own knowledge as well as any reference provided in the form of external webpages. \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n Next Word \\n \"Forecasting the Future: Utilizing LSTM for Next-Word Prediction in Natural Language Processing.\"  \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Next Word \\n \"Forecasting the Future: Utilizing LSTM for Next-Word Prediction in Natural Language Processing.\"  \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Next Word \\n \"Forecasting the Future: Utilizing LSTM for Next-Word Prediction in Natural Language Processing.\"  \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n /$ /$ \\n \\n \\n \\n \\n \\n \\n \\n Index \\n \\n $ Main Home /$ \\n $ About /$ \\n \\n \\n \\n Resources \\n \\n $ Project /$ \\n $ Stack /$ \\n \\n \\n \\n Contact \\n \\n $ rupadh17@asu.edu /$ \\n $ Linkedin /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Index \\n \\n $ Main Home /$ \\n $ About /$ \\n \\n \\n \\n Resources \\n \\n $ Project /$ \\n $ Stack /$ \\n \\n \\n \\n Contact \\n \\n $ rupadh17@asu.edu /$ \\n $ Linkedin /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Index \\n \\n $ Main Home /$ \\n $ About /$ \\n \\n \\n \\n Resources \\n \\n $ Project /$ \\n $ Stack /$ \\n \\n \\n \\n Contact \\n \\n $ rupadh17@asu.edu /$ \\n $ Linkedin /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n /$ \\n \\n (()=>{function u(){function n(t,e,i){let r=document.createElement(\"a\");r.href=t,r.target=i,r.rel=e,document.body.appendChild(r),r.click(),r.remove()}function o(t){if(this.dataset.hydrated){this.removeEventListener(\"click\",o);return}t.preventDefault(),t.stopPropagation();let e=this.getAttribute(\"href\");if(!e)return;if(/Mac|iPod|iPhone|iPad/u.test(navigator.userAgent)?t.metaKey:t.ctrlKey)return n(e,\"\",\"_blank\");let r=this.getAttribute(\"rel\")??\"\",c=this.getAttribute(\"target\")??\"\";n(e,r,c)}function a(t){if(this.dataset.hydrated){this.removeEventListener(\"auxclick\",o);return}t.preventDefault(),t.stopPropagation();let e=this.getAttribute(\"href\");e&&n(e,\"\",\"_blank\")}function s(t){if(this.dataset.hydrated){this.removeEventListener(\"keydown\",s);return}if(t.key!==\"Enter\")return;t.preventDefault(),t.stopPropagation();let e=this.getAttribute(\"href\");if(!e)return;let i=this.getAttribute(\"rel\")??\"\",r=this.getAttribute(\"target\")??\"\";n(e,i,r)}document.querySelectorAll(\"[data-nested-link]\").forEach(t=>{t instanceof HTMLElement&&(t.addEventListener(\"click\",o),t.addEventListener(\"auxclick\",a),t.addEventListener(\"keydown\",s))})}return u})()() (()=>{function i(){for(let e of document.querySelectorAll(\"[data-framer-original-sizes]\")){let t=e.getAttribute(\"data-framer-original-sizes\");t===\"\"?e.removeAttribute(\"sizes\"):e.setAttribute(\"sizes\",t),e.removeAttribute(\"data-framer-original-sizes\")}}function a(){window.__framer_onRewriteBreakpoints=i}return a})()() \\n !function(){function c(t,r){let e=r.indexOf(\"#\"),n=e===-1?r:r.substring(0,e),o=e===-1?\"\":r.substring(e),a=n.indexOf(\"?\");if(a===-1)return n+t+o;let d=new URLSearchParams(t),h=n.substring(a+1),s=new URLSearchParams(h);for(let[i,m]of d)s.has(i)||s.append(i,m);return n.substring(0,a+1)+s.toString()+o}var l=\\'div#main a[href^=\"#\"],div#main a[href^=\"/\"],div#main a[href^=\".\"]\\',u=\"div#main a[data-framer-preserve-params]\",f,g=(f=document.currentScript)==null?void 0:f.hasAttribute(\"data-preserve-internal-params\");if(window.location.search&&!/bot|-google|google-|yandex|ia_archiver|crawl|spider/iu.test(navigator.userAgent)){let t=document.querySelectorAll(g?`${l},${u}`:u);for(let r of t){let e=c(window.location.search,r.href);r.setAttribute(\"href\",e)}}\\r\\n}() \\n \\n $ $ $ \\n \\n \\n $ Create a free website with Framer, the website builder loved by startups, designers and agencies. \\n \\n /$ \\n \\n \\n /$ /$ /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  Start of bodyEnd  \\n  End of bodyEnd'),\n",
       " Document(metadata={'Header 1': '#TITLE#', 'source': 'https://fearless-writers-028990.framer.app/project/low-cost-vision-based-gripper', 'section_header': 'Start of bodyStart', 'section_type': 'remote_html'}, page_content='Start of bodyStart  \\n  End of bodyStart  \\n \\n $ \\n \\n \\n \\n $ \\n \\n Ritam Upadhyay \\n Data Scientist \\n /$ \\n \\n \\n $ \\n $ \\n Home \\n /$ \\n \\n /$ $ \\n $ \\n About \\n /$ \\n \\n /$ $ \\n $ \\n Projects \\n /$ \\n \\n /$ $ \\n $ \\n Stacks \\n /$ \\n \\n /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Search \\n \\n \\n \\n \\n $ /$ \\n \\n \\n $ \\n $ /$ \\n \\n /$ $ \\n $ /$ \\n \\n /$ $ \\n $ /$ \\n \\n /$ $ \\n $ /$ \\n \\n /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n $ /$ \\n \\n Menu \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Available for work'),\n",
       " Document(metadata={'Header 1': 'Low Cost Vision-Based Gripper - Camera based gripper to locate, localize and grasp object', 'source': 'https://fearless-writers-028990.framer.app/project/low-cost-vision-based-gripper', 'section_header': 'Low Cost Vision-Based Gripper - Camera based gripper to locate, localize and grasp object', 'section_type': 'remote_html'}, page_content='Low Cost Vision-Based Gripper - Camera based gripper to locate, localize and grasp object \\n \\n \\n Date \\n July 2021 \\n \\n \\n Service \\n \\n \\n \\n Employer \\n Centre of Excellence in Advanced Manufacturing Technology, IIT Kharagpur'),\n",
       " Document(metadata={'Header 2': 'Project Overview', 'source': 'https://fearless-writers-028990.framer.app/project/low-cost-vision-based-gripper', 'section_header': 'Project Overview', 'section_type': 'remote_html'}, page_content='Project Overview \\n \\n This project focused on automating object detection and grasping for a KUKA robot in an assembly line. The objective was to identify whether an object was cylindrical or cuboid to select the appropriate gripper type (three-fingered for cylindrical objects, parallel plate for cuboids). YOLOv4 was used to identify the cuboid and cylindrical objects. \\n Watch the video : $ https://www.youtube.com/watch?v=g1MSxzrRrLM /$'),\n",
       " Document(metadata={'Header 2': 'Key Highlights', 'source': 'https://fearless-writers-028990.framer.app/project/low-cost-vision-based-gripper', 'section_header': 'Key Highlights', 'section_type': 'remote_html'}, page_content='Key Highlights \\n \\n To pick up an object, it is essential to find whether the object is cylindrical or cuboid so that the appropriate gripper is used for grasping. The three-fingered gripper is generally used for cylindrical surfaces whereas parallel plate grippers are used for cuboids. \\n YOLOv4 was used for object detection and to differentiate between cylindrical and cuboid. \\n In addition, the vertices of a cuboid were found using segmentation of the object and it was used to estimate pose of the cuboid. OpenCV was used to perform segmentation and automate the KUKA robot. \\n The camera mounted on top of the KUKA robot, aided in calculating the position and the pose for perfect grasp. \\n Used Python to connect the KUKA robot controllers through the cloud to feed the 6D coordinates and automate the process in an assembly line.'),\n",
       " Document(metadata={'Header 2': 'More Project', 'source': 'https://fearless-writers-028990.framer.app/project/low-cost-vision-based-gripper', 'section_header': 'Low Cost Vision Based Gripper', 'section_label': 'Low Cost Vision Based Gripper', 'section_type': 'remote_html'}, page_content='More Project \\n \\n $ $ \\n $ \\n \\n \\n RAG-PDF QA Method \\n Implementation of RAG method to perform question answering on PDF documents. HuggingFace embeddings in addition to the Groq API were used for efficient and accurate information retrieval. Session management was done to ensure embeddings are computed once per document upload to optimize performance by avoiding redundant computations.  \\n \\n \\n \\n \\n January 2025 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n RAG-PDF QA Method \\n Implementation of RAG method to perform question answering on PDF documents. HuggingFace embeddings in addition to the Groq API were used for efficient and accurate information retrieval. Session management was done to ensure embeddings are computed once per document upload to optimize performance by avoiding redundant computations.  \\n \\n \\n \\n \\n January 2025 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n RAG-PDF QA Method \\n Implementation of RAG method to perform question answering on PDF documents. HuggingFace embeddings in addition to the Groq API were used for efficient and accurate information retrieval. Session management was done to ensure embeddings are computed once per document upload to optimize performance by avoiding redundant computations.  \\n \\n \\n \\n \\n January 2025 \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n LLM Based Q/A on Web Context  \\n Development of Web App that uses open source LLMs to answer user queries based on its own knowledge as well as any reference provided in the form of external webpages. \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n LLM Based Q/A on Web Context  \\n Development of Web App that uses open source LLMs to answer user queries based on its own knowledge as well as any reference provided in the form of external webpages. \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n LLM Based Q/A on Web Context  \\n Development of Web App that uses open source LLMs to answer user queries based on its own knowledge as well as any reference provided in the form of external webpages. \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n Next Word \\n \"Forecasting the Future: Utilizing LSTM for Next-Word Prediction in Natural Language Processing.\"  \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Next Word \\n \"Forecasting the Future: Utilizing LSTM for Next-Word Prediction in Natural Language Processing.\"  \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Next Word \\n \"Forecasting the Future: Utilizing LSTM for Next-Word Prediction in Natural Language Processing.\"  \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n /$ /$ \\n \\n \\n \\n \\n \\n \\n \\n Index \\n \\n $ Main Home /$ \\n $ About /$ \\n \\n \\n \\n Resources \\n \\n $ Project /$ \\n $ Stack /$ \\n \\n \\n \\n Contact \\n \\n $ rupadh17@asu.edu /$ \\n $ Linkedin /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Index \\n \\n $ Main Home /$ \\n $ About /$ \\n \\n \\n \\n Resources \\n \\n $ Project /$ \\n $ Stack /$ \\n \\n \\n \\n Contact \\n \\n $ rupadh17@asu.edu /$ \\n $ Linkedin /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Index \\n \\n $ Main Home /$ \\n $ About /$ \\n \\n \\n \\n Resources \\n \\n $ Project /$ \\n $ Stack /$ \\n \\n \\n \\n Contact \\n \\n $ rupadh17@asu.edu /$ \\n $ Linkedin /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n /$ \\n \\n (()=>{function u(){function n(t,e,i){let r=document.createElement(\"a\");r.href=t,r.target=i,r.rel=e,document.body.appendChild(r),r.click(),r.remove()}function o(t){if(this.dataset.hydrated){this.removeEventListener(\"click\",o);return}t.preventDefault(),t.stopPropagation();let e=this.getAttribute(\"href\");if(!e)return;if(/Mac|iPod|iPhone|iPad/u.test(navigator.userAgent)?t.metaKey:t.ctrlKey)return n(e,\"\",\"_blank\");let r=this.getAttribute(\"rel\")??\"\",c=this.getAttribute(\"target\")??\"\";n(e,r,c)}function a(t){if(this.dataset.hydrated){this.removeEventListener(\"auxclick\",o);return}t.preventDefault(),t.stopPropagation();let e=this.getAttribute(\"href\");e&&n(e,\"\",\"_blank\")}function s(t){if(this.dataset.hydrated){this.removeEventListener(\"keydown\",s);return}if(t.key!==\"Enter\")return;t.preventDefault(),t.stopPropagation();let e=this.getAttribute(\"href\");if(!e)return;let i=this.getAttribute(\"rel\")??\"\",r=this.getAttribute(\"target\")??\"\";n(e,i,r)}document.querySelectorAll(\"[data-nested-link]\").forEach(t=>{t instanceof HTMLElement&&(t.addEventListener(\"click\",o),t.addEventListener(\"auxclick\",a),t.addEventListener(\"keydown\",s))})}return u})()() (()=>{function i(){for(let e of document.querySelectorAll(\"[data-framer-original-sizes]\")){let t=e.getAttribute(\"data-framer-original-sizes\");t===\"\"?e.removeAttribute(\"sizes\"):e.setAttribute(\"sizes\",t),e.removeAttribute(\"data-framer-original-sizes\")}}function a(){window.__framer_onRewriteBreakpoints=i}return a})()() \\n !function(){function c(t,r){let e=r.indexOf(\"#\"),n=e===-1?r:r.substring(0,e),o=e===-1?\"\":r.substring(e),a=n.indexOf(\"?\");if(a===-1)return n+t+o;let d=new URLSearchParams(t),h=n.substring(a+1),s=new URLSearchParams(h);for(let[i,m]of d)s.has(i)||s.append(i,m);return n.substring(0,a+1)+s.toString()+o}var l=\\'div#main a[href^=\"#\"],div#main a[href^=\"/\"],div#main a[href^=\".\"]\\',u=\"div#main a[data-framer-preserve-params]\",f,g=(f=document.currentScript)==null?void 0:f.hasAttribute(\"data-preserve-internal-params\");if(window.location.search&&!/bot|-google|google-|yandex|ia_archiver|crawl|spider/iu.test(navigator.userAgent)){let t=document.querySelectorAll(g?`${l},${u}`:u);for(let r of t){let e=c(window.location.search,r.href);r.setAttribute(\"href\",e)}}\\r\\n}() \\n \\n $ $ $ \\n \\n \\n $ Create a free website with Framer, the website builder loved by startups, designers and agencies. \\n \\n /$ \\n \\n \\n /$ /$ /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  Start of bodyEnd  \\n  End of bodyEnd'),\n",
       " Document(metadata={'Header 1': '#TITLE#', 'source': 'https://fearless-writers-028990.framer.app/stack', 'section_header': 'Start of bodyStart', 'section_type': 'remote_html'}, page_content='Start of bodyStart  \\n  End of bodyStart  \\n \\n $ \\n \\n \\n \\n $ \\n \\n Ritam Upadhyay \\n Data Scientist \\n /$ \\n \\n \\n $ \\n $ \\n Home \\n /$ \\n \\n /$ $ \\n $ \\n About \\n /$ \\n \\n /$ $ \\n $ \\n Projects \\n /$ \\n \\n /$ $ \\n $ \\n Stacks \\n /$ \\n \\n /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Search \\n \\n \\n \\n \\n $ /$ \\n \\n Menu \\n \\n \\n \\n \\n \\n \\n \\n \\n $ /$ \\n \\n \\n $ \\n $ /$ \\n \\n /$ $ \\n $ /$ \\n \\n /$ $ \\n $ /$ \\n \\n /$ $ \\n $ /$ \\n \\n /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Available for work \\n \\n \\n \\n $ \\n $ \\n Back to homepage \\n \\n \\n /$ \\n \\n \\n $ \\n Back to homepage \\n \\n \\n /$ \\n \\n \\n $ \\n Back to homepage \\n \\n \\n /$ \\n \\n /$'),\n",
       " Document(metadata={'Header 2': 'Stack', 'source': 'https://fearless-writers-028990.framer.app/stack', 'section_header': 'Stack', 'section_type': 'remote_html'}, page_content='Stack \\n Tech-Stack I am familiar with \\n \\n \\n $ $ \\n $ \\n \\n \\n Python \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Python \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Python \\n \\n \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n SQL \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n SQL \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n SQL \\n \\n \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n Streamlit \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Streamlit \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Streamlit \\n \\n \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n Tensorflow \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Tensorflow \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Tensorflow \\n \\n \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n PowerBI \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n PowerBI \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n PowerBI \\n \\n \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n Matplotlib \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Matplotlib \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Matplotlib \\n \\n \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n Tableau \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Tableau \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Tableau \\n \\n \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n Microsoft Office \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Microsoft Office \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Microsoft Office \\n \\n \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n Keras \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Keras \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Keras \\n \\n \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n LangChain \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n LangChain \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n LangChain \\n \\n \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n Jupyter Notebook \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Jupyter Notebook \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Jupyter Notebook \\n \\n \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n Git/Gitlab \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Git/Gitlab \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Git/Gitlab \\n \\n \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n PySpark \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n PySpark \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n PySpark \\n \\n \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n Google Suite \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Google Suite \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Google Suite \\n \\n \\n \\n /$ \\n \\n /$ /$ \\n \\n \\n \\n \\n \\n \\n \\n Index \\n \\n $ Main Home /$ \\n $ About /$ \\n \\n \\n \\n Resources \\n \\n $ Project /$ \\n $ Stack /$ \\n \\n \\n \\n Contact \\n \\n $ rupadh17@asu.edu /$ \\n $ Linkedin /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Index \\n \\n $ Main Home /$ \\n $ About /$ \\n \\n \\n \\n Resources \\n \\n $ Project /$ \\n $ Stack /$ \\n \\n \\n \\n Contact \\n \\n $ rupadh17@asu.edu /$ \\n $ Linkedin /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Index \\n \\n $ Main Home /$ \\n $ About /$ \\n \\n \\n \\n Resources \\n \\n $ Project /$ \\n $ Stack /$ \\n \\n \\n \\n Contact \\n \\n $ rupadh17@asu.edu /$ \\n $ Linkedin /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n /$ \\n \\n (()=>{function u(){function n(t,e,i){let r=document.createElement(\"a\");r.href=t,r.target=i,r.rel=e,document.body.appendChild(r),r.click(),r.remove()}function o(t){if(this.dataset.hydrated){this.removeEventListener(\"click\",o);return}t.preventDefault(),t.stopPropagation();let e=this.getAttribute(\"href\");if(!e)return;if(/Mac|iPod|iPhone|iPad/u.test(navigator.userAgent)?t.metaKey:t.ctrlKey)return n(e,\"\",\"_blank\");let r=this.getAttribute(\"rel\")??\"\",c=this.getAttribute(\"target\")??\"\";n(e,r,c)}function a(t){if(this.dataset.hydrated){this.removeEventListener(\"auxclick\",o);return}t.preventDefault(),t.stopPropagation();let e=this.getAttribute(\"href\");e&&n(e,\"\",\"_blank\")}function s(t){if(this.dataset.hydrated){this.removeEventListener(\"keydown\",s);return}if(t.key!==\"Enter\")return;t.preventDefault(),t.stopPropagation();let e=this.getAttribute(\"href\");if(!e)return;let i=this.getAttribute(\"rel\")??\"\",r=this.getAttribute(\"target\")??\"\";n(e,i,r)}document.querySelectorAll(\"[data-nested-link]\").forEach(t=>{t instanceof HTMLElement&&(t.addEventListener(\"click\",o),t.addEventListener(\"auxclick\",a),t.addEventListener(\"keydown\",s))})}return u})()() (()=>{function i(){for(let e of document.querySelectorAll(\"[data-framer-original-sizes]\")){let t=e.getAttribute(\"data-framer-original-sizes\");t===\"\"?e.removeAttribute(\"sizes\"):e.setAttribute(\"sizes\",t),e.removeAttribute(\"data-framer-original-sizes\")}}function a(){window.__framer_onRewriteBreakpoints=i}return a})()() \\n !function(){function c(t,r){let e=r.indexOf(\"#\"),n=e===-1?r:r.substring(0,e),o=e===-1?\"\":r.substring(e),a=n.indexOf(\"?\");if(a===-1)return n+t+o;let d=new URLSearchParams(t),h=n.substring(a+1),s=new URLSearchParams(h);for(let[i,m]of d)s.has(i)||s.append(i,m);return n.substring(0,a+1)+s.toString()+o}var l=\\'div#main a[href^=\"#\"],div#main a[href^=\"/\"],div#main a[href^=\".\"]\\',u=\"div#main a[data-framer-preserve-params]\",f,g=(f=document.currentScript)==null?void 0:f.hasAttribute(\"data-preserve-internal-params\");if(window.location.search&&!/bot|-google|google-|yandex|ia_archiver|crawl|spider/iu.test(navigator.userAgent)){let t=document.querySelectorAll(g?`${l},${u}`:u);for(let r of t){let e=c(window.location.search,r.href);r.setAttribute(\"href\",e)}}\\r\\n}() \\n \\n $ $ $ \\n \\n \\n $ Create a free website with Framer, the website builder loved by startups, designers and agencies. \\n \\n /$ \\n \\n \\n /$ /$ /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  Start of bodyEnd  \\n  End of bodyEnd'),\n",
       " Document(metadata={'Header 1': '#TITLE#', 'source': 'https://fearless-writers-028990.framer.app/project/llm-based-q-a-on-web-context', 'section_header': 'Start of bodyStart', 'section_type': 'remote_html'}, page_content='Start of bodyStart  \\n  End of bodyStart  \\n \\n $ \\n \\n \\n \\n $ \\n \\n Ritam Upadhyay \\n Data Scientist \\n /$ \\n \\n \\n $ \\n $ \\n Home \\n /$ \\n \\n /$ $ \\n $ \\n About \\n /$ \\n \\n /$ $ \\n $ \\n Projects \\n /$ \\n \\n /$ $ \\n $ \\n Stacks \\n /$ \\n \\n /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Search \\n \\n \\n \\n \\n $ /$ \\n \\n \\n $ \\n $ /$ \\n \\n /$ $ \\n $ /$ \\n \\n /$ $ \\n $ /$ \\n \\n /$ $ \\n $ /$ \\n \\n /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n $ /$ \\n \\n Menu \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Available for work'),\n",
       " Document(metadata={'Header 1': 'LLM Based Q/A on Web Context  - LLM Q/A based on OLLAMA', 'source': 'https://fearless-writers-028990.framer.app/project/llm-based-q-a-on-web-context', 'section_header': 'LLM Based Q/A on Web Context  - LLM Q/A based on OLLAMA', 'section_type': 'remote_html'}, page_content='LLM Based Q/A on Web Context  - LLM Q/A based on OLLAMA \\n \\n \\n Date \\n December 2024 \\n \\n \\n Service \\n \\n \\n \\n Employer'),\n",
       " Document(metadata={'Header 2': 'Project Overview', 'source': 'https://fearless-writers-028990.framer.app/project/llm-based-q-a-on-web-context', 'section_header': 'Project Overview', 'section_type': 'remote_html'}, page_content='Project Overview \\n \\n This project aimed to build an interface so that user could provide an URL and ask specific questions to the LLM based on that webpage where the URL leads. The LLM also indicated the paragraphs and texts that led to the answer that it provided. This reduced the chances of hallucinations by the model. The user could also see the answer the model gave based on its own knowledge and could choose the one that suited him the most. \\n $ https://github.com/ritam3/LLM-Based-Q-A-based-on-Web-Context /$'),\n",
       " Document(metadata={'Header 2': 'Key Highlights', 'source': 'https://fearless-writers-028990.framer.app/project/llm-based-q-a-on-web-context', 'section_header': 'Key Highlights', 'section_type': 'remote_html'}, page_content='Key Highlights \\n \\n Used Langchain to work with LLMs and loading web based data, splitting text and converting them into documents. \\n FAISS to store the encodings. \\n Worked with OLLAMA to include models like LLAMA 3.2 and Gemma:2b. \\n Used Streamlit to build the User Interface. \\n User was given the ability to ask a question and the chosen LLM model returned the answer based on web context and its own knowledge.'),\n",
       " Document(metadata={'Header 2': 'More Project', 'source': 'https://fearless-writers-028990.framer.app/project/llm-based-q-a-on-web-context', 'section_header': 'Llm Based Q A On Web Context', 'section_label': 'Llm Based Q A On Web Context', 'section_type': 'remote_html'}, page_content='More Project \\n \\n $ $ \\n $ \\n \\n \\n RAG-PDF QA Method \\n Implementation of RAG method to perform question answering on PDF documents. HuggingFace embeddings in addition to the Groq API were used for efficient and accurate information retrieval. Session management was done to ensure embeddings are computed once per document upload to optimize performance by avoiding redundant computations.  \\n \\n \\n \\n \\n January 2025 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n RAG-PDF QA Method \\n Implementation of RAG method to perform question answering on PDF documents. HuggingFace embeddings in addition to the Groq API were used for efficient and accurate information retrieval. Session management was done to ensure embeddings are computed once per document upload to optimize performance by avoiding redundant computations.  \\n \\n \\n \\n \\n January 2025 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n RAG-PDF QA Method \\n Implementation of RAG method to perform question answering on PDF documents. HuggingFace embeddings in addition to the Groq API were used for efficient and accurate information retrieval. Session management was done to ensure embeddings are computed once per document upload to optimize performance by avoiding redundant computations.  \\n \\n \\n \\n \\n January 2025 \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n Next Word \\n \"Forecasting the Future: Utilizing LSTM for Next-Word Prediction in Natural Language Processing.\"  \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Next Word \\n \"Forecasting the Future: Utilizing LSTM for Next-Word Prediction in Natural Language Processing.\"  \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Next Word \\n \"Forecasting the Future: Utilizing LSTM for Next-Word Prediction in Natural Language Processing.\"  \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n Churn Prediction and Customer Cohort \\n Development of a Machine Learning Model that takes user\\'s activity data as input and predicts if the user is going to churn \\n \\n \\n \\n \\n October 2023 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Churn Prediction and Customer Cohort \\n Development of a Machine Learning Model that takes user\\'s activity data as input and predicts if the user is going to churn \\n \\n \\n \\n \\n October 2023 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Churn Prediction and Customer Cohort \\n Development of a Machine Learning Model that takes user\\'s activity data as input and predicts if the user is going to churn \\n \\n \\n \\n \\n October 2023 \\n \\n /$ \\n \\n /$ /$ \\n \\n \\n \\n \\n \\n \\n \\n Index \\n \\n $ Main Home /$ \\n $ About /$ \\n \\n \\n \\n Resources \\n \\n $ Project /$ \\n $ Stack /$ \\n \\n \\n \\n Contact \\n \\n $ rupadh17@asu.edu /$ \\n $ Linkedin /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Index \\n \\n $ Main Home /$ \\n $ About /$ \\n \\n \\n \\n Resources \\n \\n $ Project /$ \\n $ Stack /$ \\n \\n \\n \\n Contact \\n \\n $ rupadh17@asu.edu /$ \\n $ Linkedin /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Index \\n \\n $ Main Home /$ \\n $ About /$ \\n \\n \\n \\n Resources \\n \\n $ Project /$ \\n $ Stack /$ \\n \\n \\n \\n Contact \\n \\n $ rupadh17@asu.edu /$ \\n $ Linkedin /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n /$ \\n \\n (()=>{function u(){function n(t,e,i){let r=document.createElement(\"a\");r.href=t,r.target=i,r.rel=e,document.body.appendChild(r),r.click(),r.remove()}function o(t){if(this.dataset.hydrated){this.removeEventListener(\"click\",o);return}t.preventDefault(),t.stopPropagation();let e=this.getAttribute(\"href\");if(!e)return;if(/Mac|iPod|iPhone|iPad/u.test(navigator.userAgent)?t.metaKey:t.ctrlKey)return n(e,\"\",\"_blank\");let r=this.getAttribute(\"rel\")??\"\",c=this.getAttribute(\"target\")??\"\";n(e,r,c)}function a(t){if(this.dataset.hydrated){this.removeEventListener(\"auxclick\",o);return}t.preventDefault(),t.stopPropagation();let e=this.getAttribute(\"href\");e&&n(e,\"\",\"_blank\")}function s(t){if(this.dataset.hydrated){this.removeEventListener(\"keydown\",s);return}if(t.key!==\"Enter\")return;t.preventDefault(),t.stopPropagation();let e=this.getAttribute(\"href\");if(!e)return;let i=this.getAttribute(\"rel\")??\"\",r=this.getAttribute(\"target\")??\"\";n(e,i,r)}document.querySelectorAll(\"[data-nested-link]\").forEach(t=>{t instanceof HTMLElement&&(t.addEventListener(\"click\",o),t.addEventListener(\"auxclick\",a),t.addEventListener(\"keydown\",s))})}return u})()() (()=>{function i(){for(let e of document.querySelectorAll(\"[data-framer-original-sizes]\")){let t=e.getAttribute(\"data-framer-original-sizes\");t===\"\"?e.removeAttribute(\"sizes\"):e.setAttribute(\"sizes\",t),e.removeAttribute(\"data-framer-original-sizes\")}}function a(){window.__framer_onRewriteBreakpoints=i}return a})()() \\n !function(){function c(t,r){let e=r.indexOf(\"#\"),n=e===-1?r:r.substring(0,e),o=e===-1?\"\":r.substring(e),a=n.indexOf(\"?\");if(a===-1)return n+t+o;let d=new URLSearchParams(t),h=n.substring(a+1),s=new URLSearchParams(h);for(let[i,m]of d)s.has(i)||s.append(i,m);return n.substring(0,a+1)+s.toString()+o}var l=\\'div#main a[href^=\"#\"],div#main a[href^=\"/\"],div#main a[href^=\".\"]\\',u=\"div#main a[data-framer-preserve-params]\",f,g=(f=document.currentScript)==null?void 0:f.hasAttribute(\"data-preserve-internal-params\");if(window.location.search&&!/bot|-google|google-|yandex|ia_archiver|crawl|spider/iu.test(navigator.userAgent)){let t=document.querySelectorAll(g?`${l},${u}`:u);for(let r of t){let e=c(window.location.search,r.href);r.setAttribute(\"href\",e)}}\\r\\n}() \\n \\n $ $ $ \\n \\n \\n $ Create a free website with Framer, the website builder loved by startups, designers and agencies. \\n \\n /$ \\n \\n \\n /$ /$ /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  Start of bodyEnd  \\n  End of bodyEnd'),\n",
       " Document(metadata={'Header 1': '#TITLE#', 'source': 'https://fearless-writers-028990.framer.app/old-home', 'section_header': 'Start of bodyStart', 'section_type': 'remote_html'}, page_content='Start of bodyStart  \\n  End of bodyStart  \\n \\n $ \\n \\n \\n \\n $ \\n \\n Ritam Upadhyay \\n Data Scientist \\n /$ \\n \\n \\n $ \\n $ \\n Home \\n /$ \\n \\n /$ $ \\n $ \\n About \\n /$ \\n \\n /$ $ \\n $ \\n Projects \\n /$ \\n \\n /$ $ \\n $ \\n Stacks \\n /$ \\n \\n /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Search \\n \\n \\n \\n \\n $ /$ \\n \\n \\n $ \\n $ /$ \\n \\n /$ $ \\n $ /$ \\n \\n /$ $ \\n $ /$ \\n \\n /$ $ \\n $ /$ \\n \\n /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n $ /$ \\n \\n Menu \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Available for work'),\n",
       " Document(metadata={'Header 1': \"Let's Transform Ideas into Reality\", 'source': 'https://fearless-writers-028990.framer.app/old-home', 'section_header': \"Let's Transform Ideas into Reality\", 'section_type': 'remote_html'}, page_content=\"Let's Transform Ideas into Reality \\n \\n $ \\n \\n Ritam Upadhyay \\n \\n Data Scientist \\n Problem Solver \\n Data Scientist \\n Problem Solver \\n Data Scientist \\n Problem Solver \\n Data Scientist \\n Problem Solver \\n \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n Ritam Upadhyay \\n \\n Data Scientist \\n Problem Solver \\n Data Scientist \\n Problem Solver \\n Data Scientist \\n Problem Solver \\n Data Scientist \\n Problem Solver \\n \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n Ritam Upadhyay \\n \\n Data Scientist \\n Problem Solver \\n Data Scientist \\n Problem Solver \\n Data Scientist \\n Problem Solver \\n Data Scientist \\n Problem Solver \\n \\n \\n \\n \\n /$ \\n \\n Welcome to my portfolio! I am Ritam Upadhyay, passionate in the field of Data Science and Artificial Intelligence. I have 2 years of work experience in the field of Data Science at Paytm and I am currently pursuing my Master's in Data Science from Arizona State University. \\n \\n \\n $ \\n $ More about me \\n /$ \\n \\n \\n $ More about me \\n /$ \\n \\n \\n $ More about me \\n /$ \\n \\n /$ $ \\n $ Contact /$ \\n \\n \\n $ Contact /$ \\n \\n \\n $ Contact /$ \\n \\n /$\"),\n",
       " Document(metadata={'Header 2': 'My Latest Works', 'source': 'https://fearless-writers-028990.framer.app/old-home', 'section_header': 'My Latest Works', 'section_type': 'remote_html'}, page_content='My Latest Works \\n I present my top-tier projects, meticulously crafted with unwavering passion, simplicity, boundless creativity, and unparalleled attention to detail. \\n \\n \\n $ $ \\n $ \\n \\n \\n RAG-PDF QA Method \\n Implementation of RAG method to perform question answering on PDF documents. HuggingFace embeddings in addition to the Groq API were used for efficient and accurate information retrieval. Session management was done to ensure embeddings are computed once per document upload to optimize performance by avoiding redundant computations.  \\n \\n \\n \\n \\n January 2025 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n RAG-PDF QA Method \\n Implementation of RAG method to perform question answering on PDF documents. HuggingFace embeddings in addition to the Groq API were used for efficient and accurate information retrieval. Session management was done to ensure embeddings are computed once per document upload to optimize performance by avoiding redundant computations.  \\n \\n \\n \\n \\n January 2025 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n RAG-PDF QA Method \\n Implementation of RAG method to perform question answering on PDF documents. HuggingFace embeddings in addition to the Groq API were used for efficient and accurate information retrieval. Session management was done to ensure embeddings are computed once per document upload to optimize performance by avoiding redundant computations.  \\n \\n \\n \\n \\n January 2025 \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n LLM Based Q/A on Web Context  \\n Development of Web App that uses open source LLMs to answer user queries based on its own knowledge as well as any reference provided in the form of external webpages. \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n LLM Based Q/A on Web Context  \\n Development of Web App that uses open source LLMs to answer user queries based on its own knowledge as well as any reference provided in the form of external webpages. \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n LLM Based Q/A on Web Context  \\n Development of Web App that uses open source LLMs to answer user queries based on its own knowledge as well as any reference provided in the form of external webpages. \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n Next Word \\n \"Forecasting the Future: Utilizing LSTM for Next-Word Prediction in Natural Language Processing.\"  \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Next Word \\n \"Forecasting the Future: Utilizing LSTM for Next-Word Prediction in Natural Language Processing.\"  \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Next Word \\n \"Forecasting the Future: Utilizing LSTM for Next-Word Prediction in Natural Language Processing.\"  \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n /$ /$ \\n \\n $ \\n $ All project \\n /$ \\n \\n \\n $ All project \\n /$ \\n \\n \\n $ All project \\n /$ \\n \\n /$'),\n",
       " Document(metadata={'Header 2': 'Stack', 'source': 'https://fearless-writers-028990.framer.app/old-home', 'section_header': 'Stack', 'section_type': 'remote_html'}, page_content='Stack \\n Tech Stack I am familiar with \\n \\n \\n $ $ \\n $ \\n \\n \\n Python \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Python \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Python \\n \\n \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n SQL \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n SQL \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n SQL \\n \\n \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n Streamlit \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Streamlit \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Streamlit \\n \\n \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n Tensorflow \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Tensorflow \\n \\n \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Tensorflow \\n \\n \\n \\n /$ \\n \\n /$ /$ \\n \\n $ \\n $ More \\n /$ \\n \\n \\n $ More \\n /$ \\n \\n \\n $ More \\n /$ \\n \\n /$ \\n \\n \\n \\n \\n \\n \\n \\n Index \\n \\n $ Main Home /$ \\n $ About /$ \\n \\n \\n \\n Resources \\n \\n $ Project /$ \\n $ Stack /$ \\n \\n \\n \\n Contact \\n \\n $ rupadh17@asu.edu /$ \\n $ Linkedin /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Index \\n \\n $ Main Home /$ \\n $ About /$ \\n \\n \\n \\n Resources \\n \\n $ Project /$ \\n $ Stack /$ \\n \\n \\n \\n Contact \\n \\n $ rupadh17@asu.edu /$ \\n $ Linkedin /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Index \\n \\n $ Main Home /$ \\n $ About /$ \\n \\n \\n \\n Resources \\n \\n $ Project /$ \\n $ Stack /$ \\n \\n \\n \\n Contact \\n \\n $ rupadh17@asu.edu /$ \\n $ Linkedin /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n /$ \\n \\n (()=>{function u(){function n(t,e,i){let r=document.createElement(\"a\");r.href=t,r.target=i,r.rel=e,document.body.appendChild(r),r.click(),r.remove()}function o(t){if(this.dataset.hydrated){this.removeEventListener(\"click\",o);return}t.preventDefault(),t.stopPropagation();let e=this.getAttribute(\"href\");if(!e)return;if(/Mac|iPod|iPhone|iPad/u.test(navigator.userAgent)?t.metaKey:t.ctrlKey)return n(e,\"\",\"_blank\");let r=this.getAttribute(\"rel\")??\"\",c=this.getAttribute(\"target\")??\"\";n(e,r,c)}function a(t){if(this.dataset.hydrated){this.removeEventListener(\"auxclick\",o);return}t.preventDefault(),t.stopPropagation();let e=this.getAttribute(\"href\");e&&n(e,\"\",\"_blank\")}function s(t){if(this.dataset.hydrated){this.removeEventListener(\"keydown\",s);return}if(t.key!==\"Enter\")return;t.preventDefault(),t.stopPropagation();let e=this.getAttribute(\"href\");if(!e)return;let i=this.getAttribute(\"rel\")??\"\",r=this.getAttribute(\"target\")??\"\";n(e,i,r)}document.querySelectorAll(\"[data-nested-link]\").forEach(t=>{t instanceof HTMLElement&&(t.addEventListener(\"click\",o),t.addEventListener(\"auxclick\",a),t.addEventListener(\"keydown\",s))})}return u})()() (()=>{function i(){for(let e of document.querySelectorAll(\"[data-framer-original-sizes]\")){let t=e.getAttribute(\"data-framer-original-sizes\");t===\"\"?e.removeAttribute(\"sizes\"):e.setAttribute(\"sizes\",t),e.removeAttribute(\"data-framer-original-sizes\")}}function a(){window.__framer_onRewriteBreakpoints=i}return a})()() \\n !function(){function c(t,r){let e=r.indexOf(\"#\"),n=e===-1?r:r.substring(0,e),o=e===-1?\"\":r.substring(e),a=n.indexOf(\"?\");if(a===-1)return n+t+o;let d=new URLSearchParams(t),h=n.substring(a+1),s=new URLSearchParams(h);for(let[i,m]of d)s.has(i)||s.append(i,m);return n.substring(0,a+1)+s.toString()+o}var l=\\'div#main a[href^=\"#\"],div#main a[href^=\"/\"],div#main a[href^=\".\"]\\',u=\"div#main a[data-framer-preserve-params]\",f,g=(f=document.currentScript)==null?void 0:f.hasAttribute(\"data-preserve-internal-params\");if(window.location.search&&!/bot|-google|google-|yandex|ia_archiver|crawl|spider/iu.test(navigator.userAgent)){let t=document.querySelectorAll(g?`${l},${u}`:u);for(let r of t){let e=c(window.location.search,r.href);r.setAttribute(\"href\",e)}}\\r\\n}() \\n \\n $ $ $ \\n \\n \\n $ Create a free website with Framer, the website builder loved by startups, designers and agencies. \\n \\n /$ \\n \\n \\n /$ /$ /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  Start of bodyEnd  \\n  End of bodyEnd'),\n",
       " Document(metadata={'Header 1': '#TITLE#', 'source': 'https://fearless-writers-028990.framer.app/project/churn-prediction-and-customer-cohort', 'section_header': 'Start of bodyStart', 'section_type': 'remote_html'}, page_content='Start of bodyStart  \\n  End of bodyStart  \\n \\n $ \\n \\n \\n \\n $ \\n \\n Ritam Upadhyay \\n Data Scientist \\n /$ \\n \\n \\n $ \\n $ \\n Home \\n /$ \\n \\n /$ $ \\n $ \\n About \\n /$ \\n \\n /$ $ \\n $ \\n Projects \\n /$ \\n \\n /$ $ \\n $ \\n Stacks \\n /$ \\n \\n /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Search \\n \\n \\n \\n \\n $ /$ \\n \\n \\n $ \\n $ /$ \\n \\n /$ $ \\n $ /$ \\n \\n /$ $ \\n $ /$ \\n \\n /$ $ \\n $ /$ \\n \\n /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n $ /$ \\n \\n Menu \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Available for work'),\n",
       " Document(metadata={'Header 1': 'Churn Prediction and Customer Cohort -', 'source': 'https://fearless-writers-028990.framer.app/project/churn-prediction-and-customer-cohort', 'section_header': 'Churn Prediction and Customer Cohort -', 'section_type': 'remote_html'}, page_content='Churn Prediction and Customer Cohort -  \\n \\n \\n Date \\n October 2023 \\n \\n \\n Service \\n \\n \\n \\n Employer \\n Paytm Money'),\n",
       " Document(metadata={'Header 2': 'Project Overview', 'source': 'https://fearless-writers-028990.framer.app/project/churn-prediction-and-customer-cohort', 'section_header': 'Project Overview', 'section_type': 'remote_html'}, page_content='Project Overview \\n This project pinpointed those users on the Paytm Money platform that were active the previous month but were predicted to churn in the upcoming month. Insights helped marketing and growth teams proactively target these users and implement retention strategies.'),\n",
       " Document(metadata={'Header 2': 'Key Highlights', 'source': 'https://fearless-writers-028990.framer.app/project/churn-prediction-and-customer-cohort', 'section_header': 'Key Highlights', 'section_type': 'remote_html'}, page_content='Key Highlights \\n \\n To identify the users who traded in the platform the previous month but are likely to churn for the current month to assist the marketing and growth team targets the users and plans. \\n Used Python to implement clustering techniques like KNN and DB-SCAN to create cohorts of users that helped to segment the user based on trading habits. \\n ML methods like Random Forests, Gradient Boosting, and Deep Neural Networks were used to predict and identify the churned users with a Recall of 88% and Precision of 70%.'),\n",
       " Document(metadata={'Header 2': 'More Project', 'source': 'https://fearless-writers-028990.framer.app/project/churn-prediction-and-customer-cohort', 'section_header': 'Churn Prediction And Customer Cohort', 'section_label': 'Churn Prediction And Customer Cohort', 'section_type': 'remote_html'}, page_content='More Project \\n \\n $ $ \\n $ \\n \\n \\n RAG-PDF QA Method \\n Implementation of RAG method to perform question answering on PDF documents. HuggingFace embeddings in addition to the Groq API were used for efficient and accurate information retrieval. Session management was done to ensure embeddings are computed once per document upload to optimize performance by avoiding redundant computations.  \\n \\n \\n \\n \\n January 2025 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n RAG-PDF QA Method \\n Implementation of RAG method to perform question answering on PDF documents. HuggingFace embeddings in addition to the Groq API were used for efficient and accurate information retrieval. Session management was done to ensure embeddings are computed once per document upload to optimize performance by avoiding redundant computations.  \\n \\n \\n \\n \\n January 2025 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n RAG-PDF QA Method \\n Implementation of RAG method to perform question answering on PDF documents. HuggingFace embeddings in addition to the Groq API were used for efficient and accurate information retrieval. Session management was done to ensure embeddings are computed once per document upload to optimize performance by avoiding redundant computations.  \\n \\n \\n \\n \\n January 2025 \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n LLM Based Q/A on Web Context  \\n Development of Web App that uses open source LLMs to answer user queries based on its own knowledge as well as any reference provided in the form of external webpages. \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n LLM Based Q/A on Web Context  \\n Development of Web App that uses open source LLMs to answer user queries based on its own knowledge as well as any reference provided in the form of external webpages. \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n LLM Based Q/A on Web Context  \\n Development of Web App that uses open source LLMs to answer user queries based on its own knowledge as well as any reference provided in the form of external webpages. \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n Next Word \\n \"Forecasting the Future: Utilizing LSTM for Next-Word Prediction in Natural Language Processing.\"  \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Next Word \\n \"Forecasting the Future: Utilizing LSTM for Next-Word Prediction in Natural Language Processing.\"  \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Next Word \\n \"Forecasting the Future: Utilizing LSTM for Next-Word Prediction in Natural Language Processing.\"  \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n /$ /$ \\n \\n \\n \\n \\n \\n \\n \\n Index \\n \\n $ Main Home /$ \\n $ About /$ \\n \\n \\n \\n Resources \\n \\n $ Project /$ \\n $ Stack /$ \\n \\n \\n \\n Contact \\n \\n $ rupadh17@asu.edu /$ \\n $ Linkedin /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Index \\n \\n $ Main Home /$ \\n $ About /$ \\n \\n \\n \\n Resources \\n \\n $ Project /$ \\n $ Stack /$ \\n \\n \\n \\n Contact \\n \\n $ rupadh17@asu.edu /$ \\n $ Linkedin /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Index \\n \\n $ Main Home /$ \\n $ About /$ \\n \\n \\n \\n Resources \\n \\n $ Project /$ \\n $ Stack /$ \\n \\n \\n \\n Contact \\n \\n $ rupadh17@asu.edu /$ \\n $ Linkedin /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n /$ \\n \\n (()=>{function u(){function n(t,e,i){let r=document.createElement(\"a\");r.href=t,r.target=i,r.rel=e,document.body.appendChild(r),r.click(),r.remove()}function o(t){if(this.dataset.hydrated){this.removeEventListener(\"click\",o);return}t.preventDefault(),t.stopPropagation();let e=this.getAttribute(\"href\");if(!e)return;if(/Mac|iPod|iPhone|iPad/u.test(navigator.userAgent)?t.metaKey:t.ctrlKey)return n(e,\"\",\"_blank\");let r=this.getAttribute(\"rel\")??\"\",c=this.getAttribute(\"target\")??\"\";n(e,r,c)}function a(t){if(this.dataset.hydrated){this.removeEventListener(\"auxclick\",o);return}t.preventDefault(),t.stopPropagation();let e=this.getAttribute(\"href\");e&&n(e,\"\",\"_blank\")}function s(t){if(this.dataset.hydrated){this.removeEventListener(\"keydown\",s);return}if(t.key!==\"Enter\")return;t.preventDefault(),t.stopPropagation();let e=this.getAttribute(\"href\");if(!e)return;let i=this.getAttribute(\"rel\")??\"\",r=this.getAttribute(\"target\")??\"\";n(e,i,r)}document.querySelectorAll(\"[data-nested-link]\").forEach(t=>{t instanceof HTMLElement&&(t.addEventListener(\"click\",o),t.addEventListener(\"auxclick\",a),t.addEventListener(\"keydown\",s))})}return u})()() (()=>{function i(){for(let e of document.querySelectorAll(\"[data-framer-original-sizes]\")){let t=e.getAttribute(\"data-framer-original-sizes\");t===\"\"?e.removeAttribute(\"sizes\"):e.setAttribute(\"sizes\",t),e.removeAttribute(\"data-framer-original-sizes\")}}function a(){window.__framer_onRewriteBreakpoints=i}return a})()() \\n !function(){function c(t,r){let e=r.indexOf(\"#\"),n=e===-1?r:r.substring(0,e),o=e===-1?\"\":r.substring(e),a=n.indexOf(\"?\");if(a===-1)return n+t+o;let d=new URLSearchParams(t),h=n.substring(a+1),s=new URLSearchParams(h);for(let[i,m]of d)s.has(i)||s.append(i,m);return n.substring(0,a+1)+s.toString()+o}var l=\\'div#main a[href^=\"#\"],div#main a[href^=\"/\"],div#main a[href^=\".\"]\\',u=\"div#main a[data-framer-preserve-params]\",f,g=(f=document.currentScript)==null?void 0:f.hasAttribute(\"data-preserve-internal-params\");if(window.location.search&&!/bot|-google|google-|yandex|ia_archiver|crawl|spider/iu.test(navigator.userAgent)){let t=document.querySelectorAll(g?`${l},${u}`:u);for(let r of t){let e=c(window.location.search,r.href);r.setAttribute(\"href\",e)}}\\r\\n}() \\n \\n $ $ $ \\n \\n \\n $ Create a free website with Framer, the website builder loved by startups, designers and agencies. \\n \\n /$ \\n \\n \\n /$ /$ /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  Start of bodyEnd  \\n  End of bodyEnd'),\n",
       " Document(metadata={'Header 1': '#TITLE#', 'source': 'https://fearless-writers-028990.framer.app/project/next-word', 'section_header': 'Start of bodyStart', 'section_type': 'remote_html'}, page_content='Start of bodyStart  \\n  End of bodyStart  \\n \\n $ \\n \\n \\n \\n $ \\n \\n Ritam Upadhyay \\n Data Scientist \\n /$ \\n \\n \\n $ \\n $ \\n Home \\n /$ \\n \\n /$ $ \\n $ \\n About \\n /$ \\n \\n /$ $ \\n $ \\n Projects \\n /$ \\n \\n /$ $ \\n $ \\n Stacks \\n /$ \\n \\n /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Search \\n \\n \\n \\n \\n $ /$ \\n \\n \\n $ \\n $ /$ \\n \\n /$ $ \\n $ /$ \\n \\n /$ $ \\n $ /$ \\n \\n /$ $ \\n $ /$ \\n \\n /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n $ /$ \\n \\n Menu \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Available for work'),\n",
       " Document(metadata={'Header 1': 'Next Word - Next Word Prediction', 'source': 'https://fearless-writers-028990.framer.app/project/next-word', 'section_header': 'Next Word - Next Word Prediction', 'section_type': 'remote_html'}, page_content='Next Word - Next Word Prediction \\n \\n \\n Date \\n December 2024 \\n \\n \\n Service \\n \\n \\n \\n Employer'),\n",
       " Document(metadata={'Header 2': 'Project Overview', 'source': 'https://fearless-writers-028990.framer.app/project/next-word', 'section_header': 'Project Overview', 'section_type': 'remote_html'}, page_content='Project Overview \\n \\n The prediction of the next word, using LSTM, is to guess the next word in a sentence. LSTM is a particular kind of neural network that remembers word context. This makes it very good for applications like text suggestions, chatbots, or writing assistants. The model predicts the next word by analyzing the words provided so far; this helps make typing faster and interactions smoother. \\n $ https://github.com/ritam3/Next_Word /$'),\n",
       " Document(metadata={'Header 2': 'Key Highlights', 'source': 'https://fearless-writers-028990.framer.app/project/next-word', 'section_header': 'Key Highlights', 'section_type': 'remote_html'}, page_content='Key Highlights \\n \\n Used the \"Adventures of Sherlock Holmes\" dataset to train the LSTM model. \\n The data was cleaned and tokenized and the final data was generated using the n-gram method. \\n The model was built using TensorFlow. \\n User Interface was built using Streamlit.'),\n",
       " Document(metadata={'Header 2': 'More Project', 'source': 'https://fearless-writers-028990.framer.app/project/next-word', 'section_header': 'Next Word', 'section_label': 'Next Word', 'section_type': 'remote_html'}, page_content='More Project \\n \\n $ $ \\n $ \\n \\n \\n RAG-PDF QA Method \\n Implementation of RAG method to perform question answering on PDF documents. HuggingFace embeddings in addition to the Groq API were used for efficient and accurate information retrieval. Session management was done to ensure embeddings are computed once per document upload to optimize performance by avoiding redundant computations.  \\n \\n \\n \\n \\n January 2025 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n RAG-PDF QA Method \\n Implementation of RAG method to perform question answering on PDF documents. HuggingFace embeddings in addition to the Groq API were used for efficient and accurate information retrieval. Session management was done to ensure embeddings are computed once per document upload to optimize performance by avoiding redundant computations.  \\n \\n \\n \\n \\n January 2025 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n RAG-PDF QA Method \\n Implementation of RAG method to perform question answering on PDF documents. HuggingFace embeddings in addition to the Groq API were used for efficient and accurate information retrieval. Session management was done to ensure embeddings are computed once per document upload to optimize performance by avoiding redundant computations.  \\n \\n \\n \\n \\n January 2025 \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n LLM Based Q/A on Web Context  \\n Development of Web App that uses open source LLMs to answer user queries based on its own knowledge as well as any reference provided in the form of external webpages. \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n LLM Based Q/A on Web Context  \\n Development of Web App that uses open source LLMs to answer user queries based on its own knowledge as well as any reference provided in the form of external webpages. \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n LLM Based Q/A on Web Context  \\n Development of Web App that uses open source LLMs to answer user queries based on its own knowledge as well as any reference provided in the form of external webpages. \\n \\n \\n \\n \\n December 2024 \\n \\n /$ \\n \\n /$ $ \\n $ \\n \\n \\n Churn Prediction and Customer Cohort \\n Development of a Machine Learning Model that takes user\\'s activity data as input and predicts if the user is going to churn \\n \\n \\n \\n \\n October 2023 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Churn Prediction and Customer Cohort \\n Development of a Machine Learning Model that takes user\\'s activity data as input and predicts if the user is going to churn \\n \\n \\n \\n \\n October 2023 \\n \\n /$ \\n \\n \\n $ \\n \\n \\n Churn Prediction and Customer Cohort \\n Development of a Machine Learning Model that takes user\\'s activity data as input and predicts if the user is going to churn \\n \\n \\n \\n \\n October 2023 \\n \\n /$ \\n \\n /$ /$ \\n \\n \\n \\n \\n \\n \\n \\n Index \\n \\n $ Main Home /$ \\n $ About /$ \\n \\n \\n \\n Resources \\n \\n $ Project /$ \\n $ Stack /$ \\n \\n \\n \\n Contact \\n \\n $ rupadh17@asu.edu /$ \\n $ Linkedin /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Index \\n \\n $ Main Home /$ \\n $ About /$ \\n \\n \\n \\n Resources \\n \\n $ Project /$ \\n $ Stack /$ \\n \\n \\n \\n Contact \\n \\n $ rupadh17@asu.edu /$ \\n $ Linkedin /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Index \\n \\n $ Main Home /$ \\n $ About /$ \\n \\n \\n \\n Resources \\n \\n $ Project /$ \\n $ Stack /$ \\n \\n \\n \\n Contact \\n \\n $ rupadh17@asu.edu /$ \\n $ Linkedin /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n /$ \\n \\n (()=>{function u(){function n(t,e,i){let r=document.createElement(\"a\");r.href=t,r.target=i,r.rel=e,document.body.appendChild(r),r.click(),r.remove()}function o(t){if(this.dataset.hydrated){this.removeEventListener(\"click\",o);return}t.preventDefault(),t.stopPropagation();let e=this.getAttribute(\"href\");if(!e)return;if(/Mac|iPod|iPhone|iPad/u.test(navigator.userAgent)?t.metaKey:t.ctrlKey)return n(e,\"\",\"_blank\");let r=this.getAttribute(\"rel\")??\"\",c=this.getAttribute(\"target\")??\"\";n(e,r,c)}function a(t){if(this.dataset.hydrated){this.removeEventListener(\"auxclick\",o);return}t.preventDefault(),t.stopPropagation();let e=this.getAttribute(\"href\");e&&n(e,\"\",\"_blank\")}function s(t){if(this.dataset.hydrated){this.removeEventListener(\"keydown\",s);return}if(t.key!==\"Enter\")return;t.preventDefault(),t.stopPropagation();let e=this.getAttribute(\"href\");if(!e)return;let i=this.getAttribute(\"rel\")??\"\",r=this.getAttribute(\"target\")??\"\";n(e,i,r)}document.querySelectorAll(\"[data-nested-link]\").forEach(t=>{t instanceof HTMLElement&&(t.addEventListener(\"click\",o),t.addEventListener(\"auxclick\",a),t.addEventListener(\"keydown\",s))})}return u})()() (()=>{function i(){for(let e of document.querySelectorAll(\"[data-framer-original-sizes]\")){let t=e.getAttribute(\"data-framer-original-sizes\");t===\"\"?e.removeAttribute(\"sizes\"):e.setAttribute(\"sizes\",t),e.removeAttribute(\"data-framer-original-sizes\")}}function a(){window.__framer_onRewriteBreakpoints=i}return a})()() \\n !function(){function c(t,r){let e=r.indexOf(\"#\"),n=e===-1?r:r.substring(0,e),o=e===-1?\"\":r.substring(e),a=n.indexOf(\"?\");if(a===-1)return n+t+o;let d=new URLSearchParams(t),h=n.substring(a+1),s=new URLSearchParams(h);for(let[i,m]of d)s.has(i)||s.append(i,m);return n.substring(0,a+1)+s.toString()+o}var l=\\'div#main a[href^=\"#\"],div#main a[href^=\"/\"],div#main a[href^=\".\"]\\',u=\"div#main a[data-framer-preserve-params]\",f,g=(f=document.currentScript)==null?void 0:f.hasAttribute(\"data-preserve-internal-params\");if(window.location.search&&!/bot|-google|google-|yandex|ia_archiver|crawl|spider/iu.test(navigator.userAgent)){let t=document.querySelectorAll(g?`${l},${u}`:u);for(let r of t){let e=c(window.location.search,r.href);r.setAttribute(\"href\",e)}}\\r\\n}() \\n \\n $ $ $ \\n \\n \\n $ Create a free website with Framer, the website builder loved by startups, designers and agencies. \\n \\n /$ \\n \\n \\n /$ /$ /$ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  Start of bodyEnd  \\n  End of bodyEnd')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
